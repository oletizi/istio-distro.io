[{"url":"https://istio.tetratelabs.io/community/event/getistio-inaugural/","title":"What is GetIstio - Inaugural Community Meetup","description":"Episode 00: What is GetIstio - Inaugural Community Meetup","content":" Feb 18, 2021 at AM 9:00 PST  Check your timezone   Recording: https://youtu.be/EGzvJK0pzx8  The inaugural meetup will be a special edition with 90 minutes to introduce the community, unpack the GetIstio and GetEnvoy projects, and hear what\u0026rsquo;s new in the Istio 1.9 release. We look forward to diving into the topics, demonstrating some new Istio capabilities and answering all of your questions during the Q\u0026amp;A session.\n Welcome and Introduction  GetIstio community Community goals and objectives Meeting cadence Current stakeholders Future topics Community resources   Introduction to the GetIstio project with Zack Butcher Review of the GetEnvoy project with Christoph Pakulski Istio 1.9 Release Update  Demonstration of new features related to VMs, observability, and more   Q\u0026amp;A Closing with community resource links  "},{"url":"https://istio.tetratelabs.io/istio-in-practice/prerequisites/","title":"Prerequisites","description":"","content":"To go through the Istio in practice tutorials we will need a running instance of a Kubernetes cluster and Istio.\n1. Kubernetes Cluster All cloud providers have managed Kubernete scluster offering we can use to install Istio service mesh.\nWe can also run a Kubernetes cluster locally on your computer using one of the following platforms:\n Minikube Docker Desktop kind MicroK8s  When using a local Kubernetes cluster, make sure your computer meets the minimum requirements for Istio installation (e.g. 16384 MB RAM and 4 CPUs). Also, ensure the Kubernetes cluster version is v1.19.0 or higher.\nInstall Kubernetes CLI If you need to install the Kubernetes CLI, follow these instructions.\nWe can run kubectl version to check if the CLI is installed. You should see the output similar to this one:\n$ kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;19\u0026#34;, GitVersion:\u0026#34;v1.19.2\u0026#34;, GitCommit:\u0026#34;f5743093fd1c663cb0cbc89748f730662345d44d\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-09-16T21:51:49Z\u0026#34;, GoVersion:\u0026#34;go1.15.2\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;19\u0026#34;, GitVersion:\u0026#34;v1.19.0\u0026#34;, GitCommit:\u0026#34;e19964183377d0ec2052d1f1fa930c4d7575bd50\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-08-26T14:23:04Z\u0026#34;, GoVersion:\u0026#34;go1.15\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 2. Install Istio with Tetrate Istio Distro Tetrate Istio Distro is the easiest way to get started with Istio. After you\u0026rsquo;ve set up your Kubernetes cluster, you can download Tetrate Istio Distro:\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash Finally, to install the demo profile of Istio, use the following command:\ngetmesh istioctl install --set profile=demo 3. Label the namespace for Istio sidecar injection We need to label the namespace where we want Istio to inject the sidecar proxies to Kubernetes deployments automatically.\nTo label the namepace, we can use the kubectl label command and label the namespace (default in our case) with a label called istio-injection=enabled:\nkubectl label namespace default istio-injection=enabled 4. Install Hello world application (OPTIONAL) As a sample to deploy on your cluster, you can use the Hello World Web application. You can pull the image from gcr.io/tetratelabs/hello-world:1.0.0, and use the commands below to create a Kubernetes deployment and Service.\nkubectl create deploy helloworld --image=gcr.io/tetratelabs/hello-world:1.0.0 --port=3000 Copy the below YAML to helloworld-svc.yaml and deploy it using kubectl apply -f helloworld-svc.yaml.\napiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - name: http port: 80 targetPort: 3000 selector: app: helloworld To access the service from an external IP, we also need a Gateway resource:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: public-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;*\u0026#39; Save the above YAML to gateway.yaml and deploy it using kubectl apply -f gateway.yaml.\nWe can now access the deployed Hello World web application through the external IP address. You can get the IP address using this command:\nkubectl get svc istio-ingressgateway -n istio-system -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; "},{"url":"https://istio.tetratelabs.io/community/event/istiocon-2021/","title":"IstioCon 2021","description":"IstioCon 2021 is the inaugural community conference for the industry&#39;s most popular service mesh.","content":"IstioCon 2021 is a community-led event, showcasing the lessons learned from running Istio in production, hands-on experiences from the Istio community, and featuring maintainers from across the Istio ecosystem. The conference offers a mix of keynotes, technical talks, lightning talks, workshops, and roadmap sessions. Fun and games are also included with two social hours to take the load off and mesh with the Istio community, vendors, and maintainers!\nWe are excited to launch IstioCon as a new way for Istio community members to gather, connect, and share information. In its inaugural year, IstioCon will be a virtual event, connecting our community across the globe.\nIstioCon is a community-led conference, showcasing the lessons learned from running Istio in production, hands-on experiences from the Istio community, and featuring maintainers from across the Istio ecosystem. Offering a mix of keynotes, technical talks, lightning talks, workshops, roadmap sessions, as well as fun and games and social hours, we invite you to take the load off and mesh with the Istio community, vendors, and maintainers. We’d love for you to join us and get involved!\nRegister now\n"},{"url":"https://istio.tetratelabs.io/istio-in-practice/zero-downtime-releases/","title":"How to do Zero-Downtime Releases","description":"","content":"The purpose of the zero-downtime release is to release a new version of the application without affecting its users. If you have a website running, this means that you can release a new version without taking the website down. It means that you can make continuous requests to that application while releasing a new application, and the application users will never get that dreaded 504 Service Unavailable response.\nYou might wonder why we would use Istio to do rolling updates if the Kubernetes option is much simpler. Yes, you can get zero-downtime deployments whether you use Istio or Kubernetes. However, with Istio, you get more features and control over traffic routing. You can use weight-based routing, mirroring the traffic to a different version, or you can route the traffic based on the request properties (URI, scheme, method, etc.).\nPrerequisites You can follow the prerequisites for instructions on how to install and setup Istio.\nKubernetes Deployments need to be versioned Each deployment of the service needs to be versioned - you need a label called version: v1 (or release: prod or anything similar to that), as well as name the deployment, so it\u0026rsquo;s clear which version it represents (e.g. helloworld-v1). Usually, you\u0026rsquo;d have at minimum these two labels set on each deployment:\nlabels: app: helloworld version: v1 You could also include many other labels if it makes sense, but you should have a label that identifies your component and its version.\nKubernetes Service needs to be generic There\u0026rsquo;s no need to put a version label in the Kubernetes Service selector. The label with the app/component name is enough. Also, keep the following in mind:\n  Start with a destination rule that contains versions you are currently running, and make sure you keep it in sync. There\u0026rsquo;s no need to end up with a destination rule with many unused or obsolete subsets.\n  If you are using matching and conditions, always define a \u0026ldquo;fallback\u0026rdquo; route in the VirtualService resource. If you don\u0026rsquo;t, any requests not matching the conditions will end up in digital heaven and won\u0026rsquo;t get served.\n  Let\u0026rsquo;s take the following example:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service.default.svc.cluster.local http: - match: - headers: my-header: regex: \u0026#39;.*debug.*\u0026#39; route: - destination: host: my-service.default.svc.cluster.local port: number: 3000 subset: debug The above VirtualService is missing a \u0026ldquo;fallback\u0026rdquo; route. In case the request doesn\u0026rsquo;t match (i.e., missing my-header: debug, for example), Istio won\u0026rsquo;t know where to route the traffic to. To fix this, always define a route that applies if none of the matches evaluates to true. Here\u0026rsquo;s the same VirtualService, with a fallback route to the subset called prod.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service.default.svc.cluster.local http: - match: - headers: my-header: regex: \u0026#39;.*debug.*\u0026#39; route: - destination: host: my-service.default.svc.cluster.local port: number: 3000 subset: debug - route: - destination: host: my-service.default.svc.cluster.local port: number: 3000 subset: prod With these guidelines in mind, here\u0026rsquo;s a rough process of doing a zero-downtime deployment using Istio. We are starting with Kubernetes deployment called helloworld-v1, a destination rule with one subset (v1) and a VirtualService resource that routes all traffic to the v1 subset. Here\u0026rsquo;s how the DestinationRule resource looks like:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: helloworld spec: host: helloworld.default.svc.cluster.local subsets: - name: v1 labels: version: v1 And the corresponding virtual service:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld spec: hosts: - helloworld http: - route: - destination: host: helloworld port: number: 3000 subset: v1 weight: 100 Once you deployed these two resources, all traffic is routed to the v1 subset.\nRolling out the second version Before you deploy the second version, the first thing you need to do is to modify the DestinationRule and add a subset that represents the second version.\n  Deploy the modified destination rule that adds the new subset:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: helloworld spec: host: helloworld.default.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2   Create/deploy the helloworld-v2 Kubernetes deployment.\n  Update the virtual service and re-deploy it. In the virtual service, you can configure a percentage of the traffic to the subset v1 and a percentage of the traffic to the new subset v2.\n  There are multiple ways you can do this - you can gradually route more traffic to v2 (e.g., in 10% increments, for example), or you can do a straight 50/50 split between versions, or even route 100% of the traffic to the new v2 subset.\nFinally, once you routed all traffic to the newest/latest subset, you can follow the steps in this order to remove the previous v1 deployment and subset:\n Remove the v1 subset from the VirtualService and re-deploy it. This will cause all traffic to go to v2 subset. Remove the v1 subset from the DestinationRule and re-deploy it. Finally, you can now safely delete the v1 Kubernetes deployment, as no traffic is being sent to it anymore.  If you got to this part, all traffic is now flowing to the v2 subset, and you don\u0026rsquo;t have any v1 artifacts running anymore.\n"},{"url":"https://istio.tetratelabs.io/community/event/getenvoy-deep-dive/","title":"Effortless Envoy and WASM with GetEnvoy","description":"Episode 01: Effortless Envoy and WASM with GetEnvoy","content":" Mar 4, 2021 at AM 9:00 PST  Check your timezone   Recording: https://youtu.be/mauuUWrxpdk  Episode 01 of the community launches into the broader Istio ecosystem with a look into Envoy and how, like GetIstio, the GetEnvoy project helps ensure success on any cloud or platform. Christoph Pakulski will be showing off the power of WASM and how Istio and Envoy are better together.\n Welcome and Introduction  Community news Future topics Community resources   GetEnvoy  Envoy overview GetEnvoy CLI GetEnvoy demo GetEnvoy roadmap   Q\u0026amp;A Closing with resource links  Join here "},{"url":"https://istio.tetratelabs.io/istio-in-practice/traffic-mirroring/","title":"How to use Traffic Mirroring","description":"","content":"In addition to more \u0026ldquo;traditional\u0026rdquo; traffic routing between different application versions, which can be based on various incoming request properties, such as portions of the URL, header values, request method, etc., Istio also supports traffic mirroring.\nTraffic mirroring can be used in cases when you don\u0026rsquo;t want to release a new version of your application and expose users to it. However, you\u0026rsquo;d still like to deploy it and observe how it works, gather telemetry data, and compare the existing application\u0026rsquo;s performance and functionality with the new application.\nYou might ask — what is the difference between deploying and releasing something? When we talk about deploying a service to production, we are merely moving the executable code (binaries, containers, whatever form needed for the code to execute) to live in the production environment. However, we are not sending any production traffic to it. The application is there, but it\u0026rsquo;s not affecting any existing applications and services running next to it.\nReleasing an application involves taking the deployed instance and start routing production traffic to it. At this point, the code we moved to production is running, and it will probably impact other applications and end-users.\nRouting traffic between two versions, doing blue-green releases is helpful and useful, but there are risks involved. For example, what if the new application breaks or malfunctions? Even if the new application is receiving only 1% of the production traffic, it can still negatively impact many users.\nWhat is Traffic Mirroring? The idea behind traffic mirroring is to minimize the risk of exposing users to a potentially broken or buggy application. Instead of deploying, releasing, and routing traffic to the new application, we deploy the new application and mirror the production traffic being sent to the released version of the application.\nYou can then observe the application receiving mirrored traffic for errors without impacting any production traffic. In addition to running various tests on the deployed version of the application, we can now also use actual production traffic and increase the testing coverage. This gives us more confidence and minimizing the risk of releasing something that doesn\u0026rsquo;t work correctly.\nNote that the requests that are sent to the mirrored instance are \u0026ldquo;fire and forget\u0026rdquo;, and any responses the mirror instance sends back are ignored.\nHow to enable traffic mirroring with Istio? Here\u0026rsquo;s a quick YAML snippet that shows how to enable traffic mirroring with Istio.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-app spec: hosts: - my-app http: - route: - destination: host: my-app.default.svc.cluster.local port: number: 3000 subset: v1 weight: 100 mirror: host: my-app.default.svc.cluster.local port: number: 3000 subset: test-v1 The above VirtualService routes 100% of the traffic to the v1 subset while also mirroring the same traffic to the test-v1 subset. The same request that was sent to the v1 subset is copied and fired off to the test-v1 subset.\nThe quickest way to see this in action is to watch the logs from the test-v1 application while sending some requests to the application\u0026rsquo;s v1 version.\nThe response you will get back when you call the application will be coming from the v1 subset. However, you\u0026rsquo;ll also see the request mirrored to the test-v1 subset:\nkubectl logs my-app-test-v1–78fc64b995-krzf7 -c svc -f \u0026gt; my-app@test-1.0.0 start /app \u0026gt; node server.js Listening on port 3000 GET /hello 200 9.303 ms — 59 GET /hello 200 0.811 ms — 59 GET /hello 200 0.254 ms — 59 GET /hello 200 3.563 ms — 59 "},{"url":"https://istio.tetratelabs.io/community/event/getistio-deep-dive/","title":"Istio the Easy Way with GetIstio","description":"Episode 02: Istio the Easy Way with GetIstio","content":" Mar 18, 2021 at AM 9:00 PDT  Check your timezone   Recording: https://youtu.be/Q5MQjwUpay0  Episode 02 of the community meetup dives into getmesh gen-ca to help integrate Istio with intermediate certificate authorities (CA). GetIstio will do more than CA integration by managing the installation, operations, and upgrade of Istio across clouds. Join us as we dive into Istio the Easy Way!\n Welcome and Introduction  Community news Future topics Community resources   GetIstio  GetIstio CLI GetIstio demo GetIstio roadmap   getmesh gen-ca  Generate intermediate CA from different managed services   Q\u0026amp;A Closing with resource links  Join here "},{"url":"https://istio.tetratelabs.io/istio-in-practice/sticky-sessions/","title":"How to use Sticky Sessions","description":"","content":"What are sticky sessions? The idea behind sticky sessions is to route the requests for a particular session to the same endpoint that served the first request. With a sticky session, you can associate a service instance with the caller based on HTTP headers or cookies. You might want to use sticky sessions if your service is doing an expensive operation on the first request but cache the value for all subsequent calls. That way, if the same user makes the request, the costly operation will not be performed, and value from the cache will be used.\nPrerequisites You can follow the prerequisites for instructions on how to install and setup Istio.\nHow to use sticky sessions with Istio? To demonstrate the functionality of sticky sessions, we will use a sample service called sticky-svc. When called, this service checks for the presence of the x-user header. If the header is present, it tries to look up the header value in the internal cache. On any first request with a new x-user, the value won\u0026rsquo;t exist in the cache, so the service will sleep for 5 seconds (simulating an expensive operation), and after that, it will cache the value. Any subsequent requests with the same x-user header value will return right away. Here\u0026rsquo;s the snippet of this simple logic from the service source code:\nvar ( cache = make(map[string]bool) ) func process(userHeaderValue string) { if cache[userHeaderValue] { return } cache[userHeaderValue] = true time.Sleep(5 * time.Second) } To see the sticky sessions in action, we will need to deploy multiple replicas of this service. That way, when we enable sticky sessions, the requests with the same x-user header value will always be directed to the pod that initially served the request for the same x-user value. The first request we make will still take 5 seconds. However, any subsequent requests will be instantaneous.\nLet\u0026rsquo;s go ahead create the Kubernetes deployment and service first.\napiVersion: apps/v1 kind: Deployment metadata: name: sticky-svc namespace: default labels: app: sticky-svc version: v1 spec: replicas: 5 selector: matchLabels: app: sticky-svc version: v1 template: metadata: labels: app: sticky-svc version: v1 spec: containers: - image: gcr.io/tetratelabs/sticky-svc:1.0.0 imagePullPolicy: Always name: svc ports: - containerPort: 8080 --- kind: Service apiVersion: v1 metadata: name: sticky-svc namespace: default labels: app: sticky-svc spec: selector: app: sticky-svc ports: - port: 8080 name: http Save the above YAML to sticky-deployment.yaml and run kubectl apply -f sticky-deployment.yaml to create the Deployment and Service.\nTo access the service from an external IP, we also need a Gateway resource:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;*\u0026#39; Save the above YAML to gateway.yaml and deploy it using kubectl apply -f gateway.yaml.\nNext, we can deploy the VirtualService and attach it to the Gateway.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: sticky-svc namespace: default spec: hosts: - \u0026#39;*\u0026#39; gateways: - gateway http: - route: - destination: host: sticky-svc.default.svc.cluster.local port: number: 8080 Save the above YAML to sticky-vs.yaml and create it using kubectl apply -f sticky-vs.yaml\nLet\u0026rsquo;s make sure everything works fine without configuring sticky sessions by invoking the /ping endpoint a couple of times with the x-user header value set:\n$ curl -H \u0026#34;x-user: ricky\u0026#34; http://localhost/ping Call was processed by host sticky-svc-689b4b7876-cv5t9 for user ricky and it took 5.0002721s The first request (as expected) will take 5 seconds. If you make a couple of more requests, you will see that some of them will also take 5 seconds, and some of them (being directed to one of the previous pods) will take significantly less, perhaps 500 microseconds.\nWith the creation of a sticky session, we want to achieve that all subsequent requests finish within a matter of microseconds, instead of taking 5 seconds. The sticky session settings can be configured in a destination rule for the service.\nAt a high level, there are two options to pick the load balancer settings. The first option is called simple, and we can only pick one of the load balancing algorithms shown in the table below.\n   Name Description     ROUND_ROBIN Round Robin load balancing algorithm (default)   LEAST_CONN This algorithm selects two random healthy hosts and picks the one with fewer active requests   RANDOM Randomly selects a host   PASSTHROUGH Forwards the connection to the requested IP address without any load balancing    For example, this snippet would set the load balancing algorithm to LEAST_CONN:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: sticky-svc namespace: default spec: host: sticky-service.default.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_CONN The second option for setting the load balancer settings is using the field called consistentHash. This option allows us to provide session affinity based on the HTTP headers (httpHeaderName), cookies (httpCookie), or other properties (source IP, for example, using useSourceIp: true setting).\nLet\u0026rsquo;s define a consistent hash algorithm in the destination rule using the x-user header name and deploy it:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: sticky-svc namespace: default spec: host: sticky-svc.default.svc.cluster.local trafficPolicy: loadBalancer: consistentHash: httpHeaderName: x-user Save the above YAML to sticky-dr-hash.yaml and deploy it using kubectl apply -f sticky-dr-hash.yaml.\nBefore we test it out, let\u0026rsquo;s restart all Pods so we get a clean slate and clear the in-memory cache. First, we scaled down the deployment to 0 replicas, and then we scale it back up to 5 replicas:\nkubectl scale deploy sticky-svc --replicas=0 kubectl scale deploy sticky-svc --replicas=5 Once all replicas are running, try and make the first request to the endpoint:\n$ curl -H \u0026#34;x-user: ricky\u0026#34; http://localhost/ping Call was processed by host sticky-svc-689b4b7876-cq8hs for user ricky and it took 5.0003232s As expected, the first request takes 5 seconds. However, any subsequent requests will go to the same instance and will take considerably less:\n$ curl -H \u0026#34;x-user: ricky\u0026#34; http://localhost/ping Call was process by host sticky-svc-689b4b7876-cq8hs for user ricky and it took 47.4µs $ curl -H \u0026#34;x-user: ricky\u0026#34; http://localhost/ping Call was process by host sticky-svc-689b4b7876-cq8hs for user ricky and it took 53.7µs $ curl -H \u0026#34;x-user: ricky\u0026#34; http://localhost/ping Call was process by host sticky-svc-689b4b7876-cq8hs for user ricky and it took 46.1µs $ curl -H \u0026#34;x-user: ricky\u0026#34; http://localhost/ping Call was process by host sticky-svc-689b4b7876-cq8hs for user ricky and it took 76.5µs This is a sticky session in action! If we send a request with a different user, it will initially take 5 seconds, but then it will go to the same pod again.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-003/","title":"SSL Certificates in Istio Ingress Gateway","description":"Episode 03: SSL Certificates in Istio Ingress Gateway","content":" May 27, 2021 at AM 11:00 PDT  Check your timezone   Recording: https://youtu.be/nYJJ57WCkxE  In episode 3, we will talk about how to set up SSL certificates in your Istio ingress gateway. We’ll show you how to use self-signed certificates (and why you should never do this) and how to automate certificate management.\nDemo script for all scenarios is here.\n"},{"url":"https://istio.tetratelabs.io/istio-in-practice/setting-up-ssl-certs/","title":"How to set up SSL Certificates","description":"","content":"SSL certificates are a must these days. They help protect the data that\u0026rsquo;s sent between the server and the client by encrypting it, which gives your website more credibility. This section will explore a couple of different ways to obtain SSL certificates and configure the Istio Gateway to use them.\nWe will learn how to manually create a self-signed certificate, followed by obtaining a real SSL certificate and setting that up as well. As you will see, setting all this up is not too complicated.\nPrerequisites For you to follow along, you will need an actual, cloud-hosted Kubernetes cluster. A cloud-hosted cluster is required because we will need an external IP address to hook up a domain name to. Of course, you will also need a domain name.\nYou can follow the prerequisites for instructions on how to install and setup Istio.\nDeploying a sample application To ensure stuff works as it should, we will start by deploying a simple Hello World web application. If you have your own application/service you want to use, feel free to use that. Otherwise, you can follow along and use the gcr.io/tetratelabs/hello-world:1.0.0 image.\nkubectl create deploy helloworld --image=gcr.io/tetratelabs/hello-world:1.0.0 --port=3000 Next, let\u0026rsquo;s create a Kubernetes Service for it.\napiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - name: http port: 80 targetPort: 3000 selector: app: helloworld Copy the above YAML to helloworld-svc.yaml and deploy it using kubectl apply -f helloworld-svc.yaml.\n Note we\u0026rsquo;re not using kubectl expose command because we need to name the ports in Kubernetes services (e.g. http), and we can\u0026rsquo;t do that through the expose command.\n To access the service from an external IP, we also need a Gateway resource:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: public-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;*\u0026#39; Save the above YAML to gateway.yaml and deploy it using kubectl apply -f gateway.yaml.\n Note: the hosts field has a value of *. We will change the value of this field after we create the SSL certificate for the domain name. The value will be the actual domain name.\n Finally, we also need a VirtualService that routes the traffic to the helloworld Kubernetes Service:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld spec: hosts: - \u0026#39;*\u0026#39; gateways: - public-gateway http: - route: - destination: host: helloworld.default.svc.cluster.local port: number: 80 Save the above YAML to helloworld-vs.yaml and deploy it using kubectl apply -f helloworld-vs.yaml.\nWith all these resources deployed, you can now get the external IP of the Istio\u0026rsquo;s ingress gateway:\nkubectl get svc -l istio=ingressgateway -n istio-system If you open the IP that shows up in the EXTERNAL-IP column, you will see something similar to the figure below.\nWe got a response back from the application, but we also got the Not Secure message from the browser, which tells the user that the connection is not secure and doesn\u0026rsquo;t instill a lot of confidence.\nSelf-signed certs and manual setup Let\u0026rsquo;s start with the simplest scenario where we manually obtain the certificate. First thing - pick a domain you want to use - note that to test this, you don\u0026rsquo;t have to own an actual domain name because we will use a self-signed certificate.\n A self-signed certificate is not signed by a certificate authority (CA). You\u0026rsquo;d use these certificates for development and testing. However, they don\u0026rsquo;t provide all of the security features that certificates signed by a CA provide.\n I will use mysuperdomain.com for my domain name.\nexport DOMAIN_NAME=mysuperdomain.com As a first step, we are going to create the root certificate ($DOMAIN_NAME.crt) and the private key used for signing the certificate ($DOMAIN_NAME.key):\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj \u0026#39;/O=$DOMAIN_NAME Inc./CN=$DOMAIN_NAME\u0026#39; -keyout $DOMAIN_NAME.key -out $DOMAIN_NAME.crt The above command creates a .crt and a .key file.\nNext, we need to create the private key and a signing request:\nopenssl req -out helloworld.$DOMAIN_NAME.csr -newkey rsa:2048 -nodes -keyout helloworld.$DOMAIN_NAME.key -subj \u0026#34;/CN=helloworld.$DOMAIN_NAME/O=hello world from $DOMAIN_NAME\u0026#34; Finally, we can create the certificate:\nopenssl x509 -req -days 365 -CA $DOMAIN_NAME.crt -CAkey $DOMAIN_NAME.key -set_serial 0 -in helloworld.$DOMAIN_NAME.csr -out helloworld.$DOMAIN_NAME.crt Now that you have the certificate and the key, you can create the Kubernetes Secret to store the certificate and the key.\nThe secret with certificates must be called istio-ingressgateway-certs, and we have to deploy it to the istio-system namespace. That way, the Istio ingress gateway will load the secret automatically.\nkubectl create secret tls istio-ingressgateway-certs -n istio-system --key helloworld.$DOMAIN_NAME.key --cert helloworld.$DOMAIN_NAME.crt With the secret in place, let\u0026rsquo;s update the Gateway resource and tell it to use this certificate and private key:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: public-gateway spec: selector: istio: ingressgateway servers: - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE # These are coming from the istio-ingressgateway-certs secret serverCertificate: /etc/istio/ingressgateway-certs/tls.crt privateKey: /etc/istio/ingressgateway-certs/tls.key hosts: - helloworld.$DOMAIN_NAME EOF Similarly, we need to update the hosts field in the VirtualService with the domain name we set to the $DOMAIN_NAME environment variable:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld spec: hosts: - helloworld.$DOMAIN_NAME gateways: - public-gateway http: - route: - destination: host: helloworld.default.svc.cluster.local port: number: 80 EOF The simplest way to test that this works is to use cURL and the --resolve flag.\nThe resolve flag has a format of [DOMAIN]:[PORT]:[IP] and it routes all requests that match the [DOMAIN]:[PORT] portion to the specified IP address. This way, we don\u0026rsquo;t need to go to DNS/domain registrars' website and make changes to test this, and we can use a domain that might not even exist.\nThe IP address in our case is the external IP address of the ingress gateway. Let\u0026rsquo;s save the gateways' IP address to EXTERNAL_IP environment variable:\nexport EXTERNAL_IP=$(kubectl get svc istio-ingressgateway -n istio-system -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) Finally, let\u0026rsquo;s use this cURL command to test that the SSL certs get verified and used:\ncurl -v --resolve helloworld.$DOMAIN_NAME:443:$EXTERNAL_IP --cacert $DOMAIN_NAME.crt https://helloworld.$DOMAIN_NAME We are telling cURL to resolve any requests to helloworld.mysuperdomain.com:443 to the external IP address of the ingress gateway with the above command. Additionally, we are providing the name of the CA certificate we created earlier.\nFrom the output, you will be able to see the details of the server certificate and a line that says the certificate was verified as well as the actual response from the helloworld Pod:\n... * Server certificate: * subject: CN=helloworld.mysuperdomain.com; O=hello world from mysuperdomain.com * start date: Feb 2 23:32:11 2021 GMT * expire date: Feb 2 23:32:11 2022 GMT * common name: helloworld.mysuperdomain.com (matched) * issuer: O=mysuperdomain.com Inc.; CN=mysuperdomain.com * SSL certificate verify ok. ... Hello World Real-signed certs and manual setup The self-signed cert route from the previous section is useful to kick the tires and test things out. We will need certificates signed by an actual certificate authority (CA) that your clients can trust.\nThere are a couple of ways you can get SSL certificates. The most popular one is Let\u0026rsquo;s Encrypt. We will be using SSL For Free, which uses Let\u0026rsquo;s Encrypt to issue the certificates. If you want to spend money, you can also purchase SSL certificates from your domain registrar or at DigiCert.\nIn this section, we will use a real domain name and actual SSL certificates - this means that if you want to follow along, make sure you have your domain ready to go.\nOnce you registered a domain, open SSL for Free to get the SSL certificates. Note that you will have to register for a free account to create the SSL certificate.\n Enter your domain name, e.g. mydomain.com in the text field. Click the Create Free SSL Certificate button. From the dashboard, click the New Certificate button.   Enter the domain name and click the Next Step button. Select the 90-Day certificate option and click Next Step. Confirm the auto-generate CSR option and click Next Step. On the last page, select the Free option and click Next Step.  Once the certificate is created, we will need to verify the domain name and prove we own the domain name we created the SSL certificate for.\nThere are three options to verify the domain: email verification (ensure you have emails set up on the domain), DNS (CNAME) verification, and HTTP file upload.\nYou can use any one of these options. I\u0026rsquo;ll be using the CNAME verification. CNAME verification involves logging in to your domain registrars and setting a CNAME record with a specific value.\nSet the A name record While we are logged in to the domain registrar\u0026rsquo;s website, let\u0026rsquo;s also create an A record for the domain that will point to your cluster\u0026rsquo;s external IP address.\nSince we requested a certificate for mydomain.com and www.mydomain.com, the A record should point from mydomain.com to the IP address.\nVerifying the domain After we\u0026rsquo;ve set the A record and CNAME, we can click the Verify Domain button. Note that it might take a while for the values to propagate and verify the domain.\nOnce the domain is verified, the certificate will be issued, and you\u0026rsquo;ll be able to download a ZIP package with generated files.\nThe package will contain the following files:\n ca_bundle.crt certificate.crt private.key  Re-create the secret Let\u0026rsquo;s delete the existing ingressgateway-certs secret, and create a new one with real certificates:\nkubectl delete secret istio-ingressgateway-certs -n istio-system We can re-create the Secret with the real SSL certificate and key we got from the downloaded package:\nkubectl create secret tls istio-ingressgateway-certs -n istio-system --key private.key --cert certificate.crt We also need to update the Gateway and the VirtualService to modify the hostnames.\nLet\u0026rsquo;s update the Gateway first (make sure you update the mysuperdomain.com to your actual domain name):\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: public-gateway spec: selector: istio: ingressgateway servers: - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE # These are coming from the istio-ingressgateway-certs secret serverCertificate: /etc/istio/ingressgateway-certs/tls.crt privateKey: /etc/istio/ingressgateway-certs/tls.key hosts: - mysuperdomain.com EOF Similarly, make a change to the VirtualService:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld spec: hosts: - mysuperdomain.com gateways: - public-gateway http: - route: - destination: host: helloworld.default.svc.cluster.local port: number: 80 EOF With both of these resources updated, you can open your browser of choice and navigate to the domain. You should see Hello World! response and the padlock before the domain name shows that the website is secure. If you click on the padlock and check the certificate, you will see your domain name in the certificate, as well as the root authority (Let\u0026rsquo;s Encrypt) and the expiration date.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-004/","title":"Deploying multiple Istio ingress gateways","description":"Episode 04: Deploying multiple Istio ingress gateways","content":" Jun 10, 2021 at AM 11:00 PDT  Check your timezone  In episode 4, we will talk about how to deploy multiple Istio ingress gateways. Running more than one Istio ingress gateway allows for more complex scenarios. For example, running a separate internal gateway and a public gateway, or running separate gateways based on the hosts.\nJoin here "},{"url":"https://istio.tetratelabs.io/istio-in-practice/multiple-ingress-gateways/","title":"How to Deploy Multiple Istio Ingress Gateways","description":"","content":"When installing Istio, you have an option to pick the installation configuration profile to use. There are six installation profiles in the latest Istio release: default, demo, minimal, remote, empty, and preview.\nEach of the profiles contains a different combination of components. For example, if you pick the minimal profile, you will only install istiod. No egress or ingress gateways will be installed. On the other hand, if we use the demo profile, Istio installs both ingress and egress gateway, in addition to istiod.\nYou can read more about the configuration profiles and check components that are part of the profiles on Istio\u0026rsquo;s docs page.\nUsing Tetrate Istio Distro you can pass in the installation configuration profile name to install Istio. For example, to install the demo profile, you can run this command:\ngetmesh istioctl install --set profile=demo You can additionally customize your Istio installation, regardless of the profile, by passing additional --set \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; key/value pairs to the command.\nWhy multiple gateways? Now before you go and create multiple ingress gateways (and multiple load balancers with your cloud provider), make sure you need it. Load balancers cost money, and it\u0026rsquo;s yet another thing you need to manage. A single load balancer can work well with a lot of scenarios, however there are cases where you might have one private or internal load balancer and a second public one.\nThe scenario with a single load balancer would look like the figure below.\nWe have a single ingress gateway - a Kubernetes service with LoadBalancer type and a Pod running Envoy. The fact that the service is of LoadBalancer type causes creating an actual load balancer instance and gives us an external IP address.\nWith the Istio Gateway resource, the host key in the configuration and attaching a Gateway to a VirtualService, we can expose multiple different services from the cluster on different domain names or sub-domains.\nNow consider a different scenario where you want two separate load balancer instances running - shown in the figure below.\nIn this scenario, we have two different external IPs that point to two different ingress gateways that run inside the same Kubernetes cluster. Let\u0026rsquo;s look at how to achieve this.\nConfiguring Gateways To get started, we need to look at the Istio configuration for a single ingress gateway that gets deployed when you use the default (or demo/preview profile). We can use the profile dump command to get the configuration:\ngetmesh istioctl profile dump --config-path components.ingressGateways \u0026gt; ingress-gateway.yaml  If you see a message saying proto: tag has too few fields: \u0026quot;-\u0026quot;, you can safely ignore it. This is a known issue that\u0026rsquo;s being currently worked on.\n Here\u0026rsquo;s how the contents of the ingress-gateway.yaml file look like:\n- enabled: true k8s: env: - name: ISTIO_META_ROUTER_MODE value: standard hpaSpec: maxReplicas: 5 metrics: - resource: name: cpu targetAverageUtilization: 80 type: Resource minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: istio-ingressgateway resources: limits: cpu: 2000m memory: 1024Mi requests: cpu: 100m memory: 128Mi service: ports: - name: status-port port: 15021 protocol: TCP targetPort: 15021 - name: http2 port: 80 protocol: TCP targetPort: 8080 - name: https port: 443 protocol: TCP targetPort: 8443 - name: tcp-istiod port: 15012 protocol: TCP targetPort: 15012 - name: tls port: 15443 protocol: TCP targetPort: 15443 strategy: rollingUpdate: maxSurge: 100% maxUnavailable: 25% name: istio-ingressgateway The settings defined above are for the default Istio ingress gateway. The YAML includes the HorizontalPodAutoscaler configuration (hpaSpec), resource limits and requests (resources), service ports (ports), deployment strategy (strategy), and environment variables (env).\nWhen installing Istio, we can define one or more Gateways directly in the IstioOperator resource. Here\u0026rsquo;s an example of an Istio operator that deploys a single (default) ingress gateway:\napiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: ingressGateways: - name: istio-ingressgateway enabled: true To deploy a second ingress gateway, we can add an entry under ingressGateways field. For example, let\u0026rsquo;s add a second gateway called istio-ingressgateway-staging in the namespace staging:\napiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: ingressGateways: - name: istio-ingressgateway enabled: true - name: istio-ingressgateway-staging namespace: staging enabled: true Before we go and deploy this, we also need to modify the labels this new Gateway will use. Remember, if we don\u0026rsquo;t specify anything, Istio uses the default Gateway configuration, and we will end up with two Gateways with the same labels, albeit in a different namespace.\nThe IstioOperator allows us to add new or modify existing labels by merely setting the label field. Here\u0026rsquo;s how the updated IstioOperator looks like:\napiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: ingressGateways: - name: istio-ingressgateway enabled: true - name: istio-ingressgateway-staging namespace: staging enabled: true label: istio: istio-ingressgateway-staging This YAML ensures that the label istio: istio-ingressgateway-staging is applied to all resource Istio creates for the ingress gateway. Before we install the operator, we need to create the staging namespace first:\nkubectl create ns staging Now we\u0026rsquo;re ready to install Istio. Save the above YAML to istio-2-gw.yaml and use getmesh to install it:\n$ getmesh istioctl install -f istio-2-gw.yaml This will install the Istio default profile with [\u0026#34;Istio core\u0026#34; \u0026#34;Istiod\u0026#34; \u0026#34;Ingress gateways\u0026#34;] components into the cluster. Proceed? (y/N) y ✔ Istio core installed ✔ Istiod installed ✔ Ingress gateways installed ✔ Installation complete When the installation completes, you can list the Pods and Services in the staging namespace:\n$ kubectl get po,svc -n staging NAME READY STATUS RESTARTS AGE pod/istio-ingressgateway-staging-8b59464d7-fvlhx 1/1 Running 0 5m30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/istio-ingressgateway-staging LoadBalancer 10.96.13.200 XX.XXX.XXX.XX 15021:31259/TCP,80:31104/TCP,443:31853/TCP,15443:31053/TCP 5m29s You\u0026rsquo;ll notice a running istio-ingressgateway-staging Pod and a istio-ingressgateway-staging service of the type LoadBalancer and with an external IP that\u0026rsquo;s different from the default ingress gateway that\u0026rsquo;s running in the istio-system namespace.\nTesting multiple Istio Gateways Time to test the gateways! Make sure you have labeled the default namespace with istio-injection=enabled (see Prerequisites) and then use the snippet below to create a Service, Deployment, Gateway, and a VirtualService.\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: nginx namespace: default labels: app: nginx spec: ports: - port: 80 name: http selector: app: nginx --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: default labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: gateway namespace: default spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;*\u0026#39; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx-1 namespace: default spec: hosts: - \u0026#34;*\u0026#34; gateways: - gateway http: - route: - destination: host: nginx port: number: 80 EOF Wait for the Pod to start, and open the first ingress gateway IP address in your browser. You can use this command to get the IP address:\nkubectl get svc istio-ingressgateway -n istio-system -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; You should get back the default \u0026ldquo;Welcome to nginx!\u0026rdquo; page. Let\u0026rsquo;s see what happens if we try to open the external IP of the second ingress gateway we deployed. You can use a similar command as above to get the IP address by updating the service name and namespace:\nkubectl get svc istio-ingressgateway-staging -n staging -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39; You won\u0026rsquo;t be able to connect to the staging ingress gateway, and this is expected. We haven\u0026rsquo;t deployed any Gateway resources that would configure the ingress. Let\u0026rsquo;s update the label value to istio-ingressgateway-staging and re-deploy the Gateway resource:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: gateway namespace: default spec: selector: istio: istio-ingressgateway-staging servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;*\u0026#39; EOF This time, you should access the Nginx home page through the staging gateway, while the original gateway won\u0026rsquo;t point to anything.\nAt this point, you could create a separate Gateway resource to control both ingress gateways independently.\n"},{"url":"https://istio.tetratelabs.io/istio-in-practice/aws-cloudmap-integration/","title":"Integrate AWS Cloud Map with Istio","description":"","content":"This tutorial describes how to integrate AWS Cloud Map with your Istio service mesh.\nOverview AWS Cloud Map is a managed service registry provided by Amazon Web Services(AWS). If your applications in Istio mesh require accessing to an external service registered in Cloud Map, then you might want to utilize the endpoint information in Cloud Map. By creating ServiceEntry resources holding the endpoint information in Cloud Map, we can finely control and observe egresses to the service. However, Istio does not provide the functionality to automatically sync ServiceEntries with the corresponding records in Cloud Map. That\u0026rsquo;s where Istio Cloud Map Operator comes into play.\nIstio Cloud Map Operator is designed for syncing Cloud Map data into Istio by pushing ServiceEntry to the Kube API server. It periodically checks the Cloud Map resources in AWS, and if there\u0026rsquo;s any update in the information, then it creates/updates a ServiceEntry resource in the k8s cluster.\nPrerequisites Before proceeding, make sure you have a Kubernetes cluster with Istio installed.\nYou can follow the prerequisites for instructions on how to install and setup Istio.\nDeploying Istio Cloud Map Operator To get started we need to download and deploy Istio Cloud Map Operator:\n  Download the manifests located here.\n  Create an AWS IAM identity with read access to AWS Cloud Map for the operator.\n  Modify the YAML file aws-config.yaml as follows:\n Set the access key used by the operator by updating the values in the secret.  apiVersion: v1 kind: Secret metadata: name: aws-creds type: Opaque data: access-key-id: \u0026lt;base64-encoded-IAM-access-key-id\u0026gt; # EDIT ME secret-access-key: \u0026lt;base64-encoded-IAM-secret-access-key\u0026gt; # EDIT ME Set the target AWS region.  apiVersion: v1 kind: ConfigMap metadata: name: aws-config data: aws-region: us-west-2 # EDIT ME   Apply the modified manifest using Kubernetes CLI.\n  Verify the deployment Assuming that you have the following data in you AWS Cloud Map,\n# list the service in Cloud Map $ aws servicediscovery list-services | jq \u0026#39;.Services[] | \u0026#34;Name: \\(.Name), Id: \\(.Id)\u0026#34;\u0026#39; \u0026#34;Name: getmesh-external-service, Id: srv-ou6hvfmjpls2lev6\u0026#34; # check namespace of your service $ aws servicediscovery get-namespace --id $(aws servicediscovery get-service --id srv-ou6hvfmjpls2lev6 | jq -r \u0026#39;.Service.NamespaceId\u0026#39;) | jq \u0026#39;.Namespace.Name\u0026#39; \u0026#34;my-namespace\u0026#34; # list endpoints $ aws servicediscovery list-instances --service-id srv-ou6hvfmjpls2lev6 | jq \u0026#39;.Instances[] | .Attributes\u0026#39; { \u0026#34;AWS_INSTANCE_IPV4\u0026#34;: \u0026#34;52.192.72.89\u0026#34; } Then, you can verify the Kubernetes deployment by checking that a ServiceEntry is created, and it contains exactly the same endpoint information as in the AWS Cloud Map.\nYou can run the following command to get the YAML representation of the resource:\nkubectl get serviceentries.networking.istio.io cloudmap-getmesh-external-service.my-namespace -o yaml Here\u0026rsquo;s how the output from the command above should look like:\napiVersion: networking.istio.io/v1beta1 kind: ServiceEntry metadata: spec: addresses: - 52.192.72.89 endpoints: - address: 52.192.72.89 ports: http: 80 https: 443 hosts: - getmesh-external-service.my-namespace  ports: - name: http number: 80 protocol: HTTP - name: https number: 443 protocol: HTTPS resolution: STATIC Note that the host name getmesh-external-service.my-namespace is in the follow format: ${Cloud Map's service name}.${service namespace}.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-006/","title":"Istio Big Talk Episode 4","description":"Istio Big Talk Episode 4","content":" Jun 23, 2021 at AM 8:00 CST  Check your timezone  Topic: How to used Istio in a large scale production environment\nGuest: Peng Chen(陈鹏) from Baidu. He is a R\u0026amp;D engineer, is now working in the Cloud Native team of Baidu Infrastructure Department. He has led and participated in the large-scale implementation of Service Mesh in many core businesses within Baidu, and has in-depth research and practical experience in Cloud Native, Service Mesh, Isito.\nJoin here "},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-005/","title":"Security in Istio","description":"Episode 05: Security in Istio","content":" Jun 24, 2021 at AM 11:00 PDT  Check your timezone  In episode 5, we will talk about Security in Istio. You’ll learn about the difference between authentication and authorization. We’ll show you how to enable mutual TLS with peer authentication, use request authentication with Auth0, and how to control access to your workloads with the authorization policies.\nJoin here "},{"url":"https://istio.tetratelabs.io/istio-in-practice/install-skywalking/","title":"Integration with Apache SkyWalking","description":"","content":"This is a quick start for Istio integration with Apache Skywalking in your environment .\nFor the detailed steps please refer to the blog article on Apache Skywalking website.\nTo highlight the essential integration steps:\n Install getmesh per documentation Deploy Istio using getmesh command and enable Access Log Service (ALS) using the following command:  getmesh istioctl install --set profile=demo \\  --set meshConfig.enableEnvoyAccessLogService=true \\  --set meshConfig.defaultConfig.envoyAccessLogService.address=skywalking-oap.istio-system:11800  Label the application namespace with  kubectl label namespace \u0026lt;namespace\u0026gt; istio-injection=enabled  Deploy Apache SkyWalking and the Application per the blog post Monitor your application via SkyWalking WebUI  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/install-istio/prepare-eks-cluster/","title":"Prepare AWS EKS cluster","description":"AWS EKS Cluster is prerequisite for Istio deployment","content":"To install Istio in EKS, you will need to set up the Kubernetes context before.\nIf you create your cluster from the command line, Kubernetes context will be automatically set. Download and configure ekscli and aws cli. Now create your cluster. For example:\neksctl create cluster --nodes 3 This will deploy a EKS cluster with three nodes on it in your default region.\nIf you have created your cluster using AWS web UI, you are going to need to set the context manually. Ensure your aws cli is properly configured. Then, run the following command:\naws eks --region \u0026lt;region-code\u0026gt; update-kubeconfig --name \u0026lt;cluster_name\u0026gt; Where  is the region where you have deployed your cluster and \u0026lt;cluster_name\u0026gt; is the name of your cluster.\nVerify your cluster if properly configure in your system:\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 3d18h When your Kubernetes context is set you are ready to use EKS add-on or install Istio using CLI.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-007/","title":"Istio Big Talk Episode 5","description":"Istio Big Talk Episode 5","content":" Jun 30, 2021 at AM 8:00 CST  Check your timezone  Tetrate is hosting Istio Big Talk, discussing Istio Best Practices on Tencent Cloud with Jimmy Song and guest Hua Zhong on Bilibili.\nJoin here "},{"url":"https://istio.tetratelabs.io/introduction/","title":"Introduction","description":"Introduction to Tetrate Istio Distribution","content":"Tetrate Istio Distro is an open source project from Tetrate that provides vetted builds of Istio tested against all major cloud platforms. TID provides extended Istio version support beyond upstream Istio (release date plus 14 months). It also includes the GetMesh lifecycle and change management CLI.\nThe TID Istio distributions are hardened and performant, and are full distributions of the upstream Istio project.\nWhy use Tetrate Istio Distro? Users of Tetrate Istio Distro benefit from:\n GetMesh: an easy way to install, manage, and update Istio; Tetrate Istio distributions: whenever you need an Istio distribution that is tested for use in AWS, Azure, GCP or vanilla Kubernetes; Support from the Tetrate Community: our community slack channels are managed by Tetrate experts who actively participate in the Istio and Envoy projects  What Istio distributions are available? The Istio distributions managed by TID include:\n Community Istio distributions - Istio binaries taken directly from the community Istio project Tetrate Istio distributions - based on the community project, the Tetrate binaries benefit from longer support windows and additional release testing. Tetrate Istio distributions (FIPS) - on request, Tetrate can also provide Istio distributions which contain FIPS-compliant cryptographic modules. TIS subscribers obtain FIPS-validated modules and the corresponding FIPS validation certificate.  What Environments are Supported? Tetrate Istio Distro can install and manage Istio on a range of Kubernetes platforms. Tetrate tests Istio builds on vanilla Kubernetes, Amazon EKS, Azure AKS and Google GKE:\n Installation: Istio installation and updates are performed using GetMesh or EKS Addons (Amazon EKS only) Management: Istio management is performed using GetMesh  Components of Tetrate Istio Distro GetMesh GetMesh simplifies installation, management, and upgrades of Istio to help you get started quickly. It guides you towards certified, compatible Istio software for your clusters. You get:\n certainty from knowing you\u0026rsquo;re installing certified versions of Istio, tested for compatibility with your production clusters flexibility to switch easily between different istioctl versions, allowing you to manage multiple different Istio clusters confidence in the correctness of your configuration, provided by Istio validation libraries from multiple sources rapid security by integrating quickly with external certificate authorities from cloud providers and cert-manager  \u0026hellip; plus multiple additional integration points with cloud providers. The getmesh CLI tool runs on Linux and MacOS.\nTetrate Istio distributions The Istio release schedule can be very aggressive for enterprise lifecycle and change management practices. Releases are issued approximately quarterly. Each release is typically supported for 7 months, after which it no longer receive any security updates.\nTetrate supports and maintains each Istio release for up to 14 months, providing technical support and security updates. This reduces the burden on Enterprises to frequently upgrade and retest their Istio infrastructure.\n   Community Istio Lifecycle Tetrate Istio Lifecycle     Support provided until 6 weeks after the following N+2 minor release, after which all security updates cease. Support provided for the 4 most recent Istio releases. All supported releases benefit from base software security updates; all Istio security updates applied to matching Istio releases.   Typical support window: up-to 7 months Typical support window: up-to 14 months    The Tetrate-provided Istio distributions are rigorously tested against multiple different Kubernetes distributions to ensure continual functional integrity and smooth upgrade experiences.\nFor a complete list of the currently-supported Istio versions and the associated Kubernetes versions, refer to the notes on the Downloads documentation.\nFIPS-Compliant and FIPS-Validated Istio distributions Istio performs mTLS and other cryptographic operations to encrypt and decrypt data. Some environments require enhanced security assurance, and may require the use of FIPS-validated cryptographic software.\nOn request, Tetrate can provide a FIPS-compliant implementation of each supported Tetrate Istio distribution. These implementations contain a cryptographic module that complies with the requirements of the FIPS-140-2 standard.\nAdditionally, where an organization is required to demonstrate their use of FIPS-validated software, Tetrate Istio Subscription (TIS) customers benefit from a FIPS-validated module and validation certificate.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-008/","title":"Istio Weekly Episode 6: Envoy Fundamentals","description":"Istio Weekly Episode 6: Envoy Fundamentals","content":" Jul 8, 2021 at AM 11:00 PDT  Check your timezone  Envoy is one part of the engine that keeps Istio running. How does Envoy work? What are listeners, clusters, and endpoints?\nJoin us on July 8th for another Istio weekly episode where we\u0026rsquo;ll introduce the fundamentals of Envoy proxy.\nJoin here "},{"url":"https://istio.tetratelabs.io/quickstart/","title":"TID Quickstart","description":"Tetrate Istio Distro quickstart","content":"Download The command below obtains a shell script that downloads and installs GetMesh CLI binary that corresponds to the OS distribution detected by the script (currently MacOS and Linux are supported). Additionally the most recent supported version of Istio is downloaded. Also script adds GetMesh location to PATH variable (re-login is required to get PATH populated)\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash Output example:\n$ curl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash tetratelabs/getmesh info checking GitHub for latest tag tetratelabs/getmesh info found version: 1.1.1 for v1.1.1/linux/amd64 tetratelabs/getmesh info installed /home/mathetake/./bin/getmesh Install Istio As explained earlier - GetMesh by default communicates to the cluster defined by your Kubernetes configuration. Please make sure you’re connected to the correct cluster before proceeding with the installation steps.\nThe most command example is to install the demo profile of Istio - that can be done with following command:\ngetmesh istioctl install --set profile=demo Output:\n$ getmesh istioctl install --set profile=demo This will install the Istio demo profile with [\u0026#34;Istio core\u0026#34; \u0026#34;Istiod\u0026#34; \u0026#34;Ingress gateways\u0026#34; \u0026#34;Egress gateways\u0026#34;] components into the cluster. Proceed? (y/N) Y ✔ Istio core installed ✔ Istiod installed ✔ Ingress gateways installed ✔ Egress gateways installed ✔ Installation complete Verify it Worked! After the previous step is completed, you can use GetMesh to verify that the installation has succeeeded.\nCheck the Installed GetMesh and Istio Versions\nRun getmesh version to check the versions of GetMesh and the target Istio install:\nOutput:\n$ getmesh version getmesh version: 1.1.4 active istioctl: 1.15.3-tetrate-v0 client version: 1.15.3-tetrate-v0 control plane version: 1.15.3-tetrate-v0 data plane version: 1.15.3-tetrate-v0 (2 proxies) Check the Istio Configuration\nRun getmesh config-validate to confirm that GetMesh can access and validate the configuration in the Istio cluster. With a fresh install of Kubernetes and Istio, the output will look similar to the below:\n$ getmesh config-validate Running the config validator. This may take some time... NAMESPACE NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE default default Namespace IST0102 Info The namespace is not enabled for Istio injection. Run \u0026#39;kubectl label namespace default istio-injection=enabled\u0026#39; to enable it, or \u0026#39;kubectl label namespace default istio-injection=disabled\u0026#39; to explicitly mark it as not needing injection. The error codes of the found issues are prefixed by \u0026#39;IST\u0026#39; or \u0026#39;KIA\u0026#39;. For the detailed explanation, please refer to - https://istio.io/latest/docs/reference/config/analysis/ for \u0026#39;IST\u0026#39; error codes - https://kiali.io/documentation/latest/validations/ for \u0026#39;KIA\u0026#39; error codes Next Steps Read on for next steps with Tetrate Istio Distro and GetMesh.\nFor community support, join Tetrate\u0026rsquo;s #tid-and-getmesh Slack Channel.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-009/","title":"Episode 07: Developing Envoy Wasm Extensions","description":"Episode 07: Developing Envoy Wasm Extensions","content":" Jul 22, 2021 at AM 11:00 PST  Check your timezone  WebAssembly (Wasm) and the Envoy Proxy Wasm SDK are a way to extend the Envoy proxy functionality. We\u0026rsquo;ll show you how to use the Proxy Wasm Go SDK and the func-e CLI to develop and test your Wasm extensions.\nJoin us on July 22th for another Istio weekly episode where we\u0026rsquo;ll talk about how to start developing Envoy Wasm extensions.\nJoin here "},{"url":"https://istio.tetratelabs.io/mgmt-tasks/","title":"Common Management Tasks","description":"Using GetMesh to perform common management tasks","content":"GetMesh simplifies the management of Istio clusters. It is used by platform operators to quickly and reliably maintain an installation, apply upgrades and perform quick troubleshooting operations.\nIn this section, you\u0026rsquo;ll discover how to:\n Apply Upgrades: Inspect a running Istio installation on a Kubernetes cluster to determine if there are applicable upgrades. You can then acquire the updated Istio binaries and apply the upgrade using GetMesh; Manage Multiple Clusters and Versions: Use a single GetMesh instance to manage multiple clusters, and to switch quickly and safely between different istioctl clients. GetMesh will always check version compatibility to avoid user errors; Validate Istio Configurations: Use getmesh config-validate to check both applied configuration and pending changes, using a broad set of validation checks  "},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-010/","title":"Istio in the enterprise: Solving security and scale challenges for microservices in Kubernetes","description":"Join experts from Mirantis and Tetrate as they guide you through some of the challenges of deploying","content":" Aug 4, 2021 at AM 1:00 EDT  Check your timezone  Join experts from Mirantis and Tetrate as they guide you through some of the challenges of deploying, managing and scaling microservices in enterprise. We’ll cover why and how Istio helps address these challenges from a customer perspective.\nAs container-based applications and Kubernetes are widely being adopted by enterprises, customers face multiple challenges on how to manage large number of microservices based applications in a multi-cluster environment. Istio Service Mesh which is independent of underlying services, is designed to manage communications between microservices, provides authentication, security, distributed tracing, traffic management resiliency.\nJoin here "},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-011/","title":"Episode 08: External CA with Istio","description":"Join us on August 5th for another Istio weekly episode where we will talk about how to configure external CA with Istio.","content":" Aug 5, 2021 at AM 11:00 PDT  Check your timezone  Istio CA generates a self-signed root certificate and key and uses them to sign all workload certificates. In this episode, we’ll show you how to configure Istio to use an external CA to distribute certificates.\nJoin us on August 5th for another Istio weekly episode where we\u0026rsquo;ll talk about how to configure external CA with Istio.\nJoin here "},{"url":"https://istio.tetratelabs.io/getmesh-cli/","title":"GetMesh Documentation","description":"Learn more about GetMesh, how to install and manage the CLI","content":"GetMesh is the easiest way to get started with Istio and to ensure you\u0026rsquo;re using trusted, supported versions of Istio. Installing GetMesh is as easy as issuing the following command:\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash Linux and MacOS hosts are supported for GetMesh.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-012/","title":"Episode 09: Customizing Istio metrics","description":"Join us on August 19th for another Istio weekly episode where we will talk about how to customize and create new service-level metrics.","content":" Aug 19, 2021 at AM 11:00 PDT  Check your timezone  Join us on August 19th for another Istio weekly episode where we\u0026rsquo;ll talk about how to customize and create new service-level metrics.\nJoin here "},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/install-istio/aks-tid-deployment/","title":"Install Istio on Azure AKS","description":"How to deploy Istio on AKS (step by step instruction)","content":"Azure Container Marketplace also the deployment of Tetrate Istio Distro (TID) on existing AKS clusters, or allows to create Azure AKS cluster with pre-installed TID. (TID is also listed in Azure Stack HCI Catalog as tested and validated Solution).\n Start by selecting the Tetrate Istio Distro offer on the Azure Marketplace.   Then choose a plan (the list is constantly changing as newer versions are released):   Select between existing cluster and a new cluster creation. Enter the basic info on the first page per figure below:   Select AKS cluster name and Kubernetes version, also the cluster attributes:   Let Azure to validate your settings and proceed with cluster creation:   After the cluster is created, you can begin provisioning your application and Istio will automatically onboard and connect your services:  When the cluster Deployment is completed - you can proceed with deploying applications in Istio-enabled AKS cluster.\n"},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/install-istio/eks-addon/","title":"Install Istio in EKS with add-on","description":"How to use EKS Istio add-on.","content":"AWS and Tetrate have brought ability to deploy Istio to EKS cluster with minimal number of steps. TID EKS add-on deployment can be done via AWS Web console or AWS CLI. Below are both approaches are described.\nInstalling TID addon via AWS Web Console  Proceed to EKS section of AWS Web Console and locate your cluster:  Select Add-ons tab and select Get more add-ons  Scroll down to AWS Marketplace add-ons section of add-ons  type Tetrate in the search bar select checkmark in the right top corner and click Next    on the next screen confirm the TID version and click Next  Review and add screen to make sure the selection is correct  You\u0026rsquo;re taken back to the cluster add-on tab and can see that the add-on is being created  After waiting for 90 seconds and UI refresh you can see that the add-on is deployed successfully   Installing TID addon via the command line   Check that add-on is available (the AWS Marketplace subscription is required before for TID addon to be deployed in AWS account)\naws eks describe-addon-versions --addon-name tetrate-io_istio-distro   Deploy TID add-on to the cluster in AWS EKS\naws eks create-addon --addon-name tetrate-io_istio-distro --cluster-name \u0026lt;CLUSTER_NAME\u0026gt;   The installation will take around 2 minutes. To get the current state use the following command.\naws eks describe-addon --addon-name tetrate-io_istio-distro --cluster-name \u0026lt;CLUSTER_NAME\u0026gt;   When the add-on is in Active state - you can proceed with deploying applications in Istio-enabled EKS cluster.\n"},{"url":"https://istio.tetratelabs.io/istio-in-practice/","title":"Istio in Practice","description":"After you got Istio running, sharpen your Istio skill by following these tutorials.","content":"This section contains practical tutorials and walkthroughs for Istio.\n   Title Description     Prerequisites List of all prerequisites and instructions on how to install and configure them   How to do Zero-Downtime Releases Learn how to release new versions without any downtime using Istio   How to use Traffic Mirroring Learn about traffic mirroring and how to use it with Istio   How to use Sticky Sessions Learn about sticky sessions and how to use them with Istio   How to set up SSL Certificates Learn how to set up Istio ingress gateway with a real SSL certificate   How to Deploy Multiple Istio Ingress Gateways Learn how to deploy multiple Istio ingress gateways   Integrate AWS Cloud Map with Istio Learn how to integrate cloud resource discovery service (AWS Cloud Map)   How to Install Apache SkyWalking Learn how to install Apache SkyWalking and integrate it with Istio to gain observability    "},{"url":"https://istio.tetratelabs.io/istio-ca-certs-integrations/","title":"Istio CA Certs Integration","description":"Integration with GCP CAS","content":"Istio provides different mechanisms to sign workload certificates for the purpose of mutual TLS (mTLS). Here are some of the options:\n Istio Certificate Authority (CA) uses a self-signed root certificate Istio CA uses an administrator-specified certificate and key with an administrator-specified root certificate Custom CA issues keys and certificate files mounted into the sidecars Experimental Custom CA integration uses Kubernetes CSR API (Kubernetes 1.18+) External CA uses Istio CA gRPC API (either through the Istiod Registration Authority (RA) model or directly authenticating workloads and validating Subject Altenrative Name (SAN))  Tetrate Istio Distro integrates with the Private CA from AWS Certificate Manager, the GCP Certificate Authority Service (CAS), and cert-manager to sign the workload certificates.\n"},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/install-istio/deploy-gateway/","title":"Deploy Istio Gateway in EKS","description":"How to use EKS Istio add-on.","content":"Istio Gateway should be deployed separately from Istio deployment. The reason for it - the gateway architecture requires additional planning work. Below you can find an example demonstrates how Istio Gateway can be deployed after the architecture work is completed. The gateway will allow traffic reach out to the applications running inside of the EKS Cluster.\nTo deploy gateway in EKS cluster the following objects are required - Envoy Gateway Deployment, Pod and Kubernetes Service will be hosted in the EKS cluster. The creation of Kubernetes service (type LoadBalancer) will trigger creation of AWS Classic LoadBalancer (The example below is taken from Istio web-site (with slight changes)):\nNamespace definition will look like this:\napiVersion: v1 kind: Namespace metadata: name: tid-ingress-ns Service object will trigger creation of AWS LoadBalancer and will allow calls to reach to gateway pod.\napiVersion: v1 kind: Service metadata: name: tid-ingressgateway namespace: tid-ingress-ns spec: type: LoadBalancer selector: istio: tid-ingress-gw ports: - port: 80 name: http - port: 443 name: https The Gateway Deployment will be placed in the namespace that we just created.\napiVersion: apps/v1 kind: Deployment metadata: name: tid-ingressgateway-gw namespace: tid-ingress-ns spec: selector: matchLabels: istio: tid-ingress-gw template: metadata: annotations: # Select the gateway injection template (rather than the default sidecar template) inject.istio.io/templates: gateway labels: # Set a unique label for the gateway. This is required to ensure Gateways can select this workload istio: tid-ingress-gw # Enable gateway injection. If connecting to a revisioned control plane, replace with \u0026#34;istio.io/rev: revision-name\u0026#34; sidecar.istio.io/inject: \u0026#34;true\u0026#34; spec: containers: - name: istio-proxy image: auto # The image will automatically update each time the pod starts. Role and Role bindings are also required per below. This allows the gateway to access kubernetes secrets. Kubernetes secrets are used to store TLS certificates.\n# Set up roles to allow reading credentials for TLS apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: tid-ingressgateway-sds namespace: tid-ingress-ns rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: tid-ingressgateway-sds namespace: tid-ingress-ns roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: tid-ingressgateway-sds subjects: - kind: ServiceAccount name: default To confirm that the steps worked as expected you can query tid-ingress-ns with the following command - kubectl get pods -n tid-ingress-ns\nThe result should show Ingress GW Pod running: "},{"url":"https://istio.tetratelabs.io/istio-cheatsheet/","title":"Istio Hints and Tips","description":"Important commands that you&#39;ll need to manage your Istio.","content":"Set up  Always do pre and post checks after Istio installation. This is enabled by default when installing Istio through Tetrate Istio Distro. As a best practice, install distroless images. Make sure your settings are in line with Istio Operator most recent API. Advanced customization. Verify specifics for your platform set up. Different cloud providers have different configuration requirements to enable Istio CNI. To have CNI enabled, AKS clusters would have to be created with the --network-plugin azure flag added. Click here and here for further details. Starting with Istio 1.8, Istio by default would merge application metrics into Istio metrics by scraping prometheus.io annotations. This may not be suitable where application metrics data are considered sensitive. This default merge can be disabled at the mesh level by passing --set meshConfig.enablePrometheusMerge=false during installation. Or this feature can be disabled per workload by adding a prometheus.istio.io/merge-metrics: \u0026quot;false\u0026quot; annotation on a pod. Click here for further details. To enable automatic sidecar injection at namespace level, set kubectl label \u0026lt;namespace\u0026gt; default istio-injection=enabled  Istio official FAQ Applications  Istio makes some assumptions about Application Service association, Pod labels, etc., that are to be satisfied for the mesh to function properly. Details can be found here Resource Annotations - While pod annotations provide a useful means to alter Istio sidecar behavior, allowing application teams to use pod annotations has to be on an exceptional basis as it opens the avenue to bypass mesh level settings and pose a security risk. HTTP and TCP based readiness probes should be used sparingly as they require allowing non-mTLS traffic from the kubelet into the pod. Instead use the command-based probes that execute a Command inside the pod instead of sending a request to the pod. Each headless service in the cluster must use a unique port. The behavior of the system is undefined if two headless services share the same port. Use ServiceEntries instead of ExternalName services The useful learning would be to know and understand Protocols selection logic in Istio.  Common Problems  Init Containers with Istio CNI\nAs Istio CNI sets up traffic redirection even before the application Pod starts up, it could potentially result in incompatibility with the application init containers. Please refer here for further details and workaround. Empty reply from Istio Ingress Gateway\nTypically Istio ingress gateway pods are reached through either cloud provider load balancers or other GTM/LTMs. If TLS is terminated at the load balancer and re-established again to Istio ingress gateway, and if the downstream SNI header field is not set, the Istio Ingress gateway wouldn’t be able terminate TLS based on the SNI header. This would result in an empty response from the Istio gateway. To prevent this, either the provider load balancer has to set the SNI header, or the Istio ingress gateway should serve all hosts with wildcard dns cert. If the latter, further routing to application services can happen through the HTTP ‘Authority’ header. Forbidden response from Istio Ingress Gateway\nFor workloads without authorization policies applied, Istio doesn’t enforce access control allowing all requests. When an authorization policy is applicable to a workload, then access to the workload is denied by default. This is true for gateway workloads as well. When a policy at the namespace level is applied to control access to cluster’s internal traffic, the policy is also applied to the Ingress gateways in that namespace that in turn can block external traffic to the gateway. This is one of the common pitfalls. In such cases add additional authorization to ingress workload to allow external traffic. For details on how to get traffic into Kubernetes and Istio and IP based white/black listing, refer to the documentation EKS VM Integration\nEKS Clusters’ Service Loadbalancers typically use Ingress.Hostname as opposed to the default Ingress.IP. As of Istio 1.16, while running the istioctl experimental workload entry configure command for VM integration into the mesh, on EKS clusters it is needed to pass the --ingressIP flag for the eastwestgateway. Otherwise, the gateway address would be empty and the VM wouldn’t be able to connect to the control plane. In general, applications relying on the Ingress.IP would have to change to use Ingress.Hostname when using EKS or distros having similar behavior.  Istio Networking   Traffic Management Best Practices\n  Service Entry\n The addition of service entries must be closely monitored to ensure that arbitrary external accesses are not provided to user namespaces. Hostname  There can be only one service entry for a given hostname. Multiple service entries for the same hostname will result in undefined behavior. No two service entries must overlap on the hostname. For example, two service entries with hosts .example.com \u0026amp; .com will cause undefined behavior at runtime. Only fully qualified domain names must be used for hostnames. Short names, without a ‘.’ should not be used.   Address  The address field in a service entry is equivalent to the ClusterIP field of a Kubernetes service. Service entries without an Address field are equivalent to headless services. Hence, all restrictions for headless services apply here with the exception of service entries using HTTP or HTTPS/TLS protocols. Multiple address-less service entries on the same port are allowed if and only if all the service entries use the same protocol. In this scenario, the protocol can be one of HTTP or TLS. In the case of HTTP, the destination service is distinguished using the HTTP Host header. In the case of TLS, the destination service is distinguished using the SNI value in the TLS connection. Refer to how Istio smart DNS proxy solves the problem when the address field is empty. Avoid using the NONE resolution mode unless the Address field has a CIDR block. A service entry without an Address field with NONE resolution allows traffic to any IP on the ports specified in the service entry - creating a potential security issue.      Virtual Services\n There should be only one virtual service for a given hostname. Multiple virtual services will result in undesirable behavior. Avoid using short names to refer to hosts in the virtual service as the interpretation is subject to runtime context, creating undesirable ambiguity. For HTTP rules, prefer exact or prefix URL matches over regular expressions. Regular expression-based matches are slow and have unintended side effects unless crafted properly. Virtual services cannot be inherited. Hence, the settings for a virtual service for host .example.com are independent of the settings for a VirtualService for host .com. If multiple virtual services with different wildcard hosts match a given hostname, only the settings from the most specific virtual service will apply at runtime.    Destination Rules\n When creating subsets for traffic shifting, leave a few seconds of delay between creating a destination rule with subsets and creating a virtual service that refers to those subsets. The delay ensures that the Envoy upstream configuration for the subset is in place when Pilot sends the routing configuration that refers to those subsets. It is possible to specify a single destination rule across a range of hosts using wildcard destination rules. For example, a global destination rule in the istio-config root namespace configures all services matching .local hostname with mutual TLS. When a destination rule is created for a more specific hostname (e.g.,.ns1.svc.cluster.local or svc1.ns1.svc.cluster.local), the most specific destination rule is chosen. Settings from other wildcard destination rules are not inherited. Hence, it is important that any user-authored destination rule carries all the required settings set in the global destination rule, such as mutual TLS, outlier detection values if any, etc.    Scope of Virtual Services \u0026amp; Destination\n Unless configuration access is tightly controlled, it is still possible for a client to override the server specified destination rule and virtual service by authoring rules in the client namespace with an exportTo value of ‘.’. Destination rules and virtual service for a service (in Kubernetes or Service Entry) impact all sidecars talking to the service. Settings like outlier detection, retry policies, etc., are executed at the client side sidecars. While it may be tempting to let each consumer namespace author its own virtual service and destination rule for another namespace’s service, failure to restrict the visibility of such custom configurations could result in undefined behavior.    Sidecar\n  Unless configuration access is tightly controlled, it is possible for a namespace owner to override the global specified Sidecar with a namespace-wide Sidecar that results in the same behavior as a system with no Sidecar by declaring dependency on / in the egress.hosts section or even by importing services and configs from multiple namespaces with potentially conflicting configurations.\n  It is recommended to use the Sidecar API in Istio to restrict the dependency of each workload on other workloads in the system. It is sufficient to have a single Sidecar resource for the entire namespace without a workload selector to configure namespace-wide defaults. Each Sidecar resource specifies the hosts that are needed by workloads in the namespace. These hosts correspond to the hostnames in Kubernetes services, Istio service entries, and Istio VirtualServices. Based on the imported service hostnames, the appropriate Destination Rules are also automatically imported from the service’s namespace. - Sidecars declare dependency on services from other namespaces in the egress.hosts section. Declaring a dependency on a hostname (with or without wildcards) will cause Pilot to try to search all namespaces where the said host is present and import from all matching namespaces. For example,\negress: - hosts: - “_/_” OR\negress: - hosts: - foo.example.com # import from any namespace that exports this host - .fun.com # import all hosts from any namespace that match this pattern The caveat here is that if there are conflicting service entries with the same host in multiple namespaces, or conflicting virtual services with the same host (with exportTo: * ) in multiple namespaces, the choice of a specific resource is based on the order of creation of these resources. This behavior can cause unintended consequences in production as it\u0026rsquo;s often not obvious to the developer.\n    Sidecars do not support inheritance. If a namespace declares a Sidecar resource, the namespace local Sidecar takes precedence over the global default Sidecar.\n  When multiple Sidecars have overlapping workload selectors, the choice of Sidecar resource for a pod is arbitrary. Hence, care must be taken to ensure that when authoring Sidecars with workload selectors, each Sidecar targets a distinct set of pods in its namespace.\n  Istio Security  Security Best Practices Kubernetes CSR API requires Kubernetes version 1.18+. Some Kubernetes distros support Kubernetes signer legacy-unknown even in earlier versions of Kubernetes with all the X509 Extensions honored. As this is not an universal truth, exercise caution on lower Kubernetes versions while choosing legacy-unknown signer.  Istio Observability Best Practice for Istio Obervability\nDebugging  To check if Istio cluster configurations and yet to be applied Istio configurations are valid, run the getmesh config-validate command. Many times, default envoy logs provide a great deal of information about the traffic. Use this link for the default envoy log format details. To get better insight into the mesh, istio-proxy and the control plane pods, Istio provides istioctl and UI based dashboard features. Details here  Useful Links  Tetrate Istio Distro CLI Kubectl Local K8s (kind, minikube, etc)  Integrations  Skywalking (log aggregation and visualization): analyze your platform with an open source tool that provides graphical analysis of multi-cluster and multi-cloud environment via log aggregation Zipkin (tracing): track and analyze path, performance, and latency of distributed transactions Prometheus (monitor): record metrics to track the health of Istio and applications in the mesh Grafana (visualization): connect to various data sources and visualize the data use graphs, tables, and heatmaps Kiali: perform basic troubleshooting with this service mesh visualization tool  Documentation  Tetrate Istio Distro func-e Learn Istio concepts interactively Envoy Docs Istio details:  Traffic management Security constructs Observability    "},{"url":"https://istio.tetratelabs.io/helm/","title":"Helm Chart Deployment","description":"Use Helm Charts to deploy Tetrate Istio Distro","content":"Helm Charts While there are multiple benefits of using getmesh CLI tool, there are use-cases that require a complete automation and repeatable approach. To address this need, Tetrate offers a de-facto standard approach with Helm charts. This page illustrates typical steps to deploy Tetrate Istio Distro via Helm.\nAdd Tetrate Helm Repo Tetrate hosts publicly available Helm Repo that can be accesses via standard Helm commands:\nhelm repo add tid https://tetratelabs.github.io/istio-helm/ helm repo update tid Output:\n$ helm repo add tid https://tetratelabs.github.io/istio-helm/ \u0026quot;tid\u0026quot; has been added to your repositories $ helm repo update tid Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;tid\u0026quot; chart repository Update Complete. ⎈Happy Helming!⎈ Set variables To ensure consistency the variables can be set per below:\n  Name matches the repo naming pattern:\nexport name=\u0026#34;tid/tetrate-istio\u0026#34; FIPS Note if customer has purchased Tetrate FIPS Distro - than the name variable should be set to tid/tetrate-fips\nexport name=\u0026quot;tid/tetrate-istio\u0026quot;   querying the repo, allows to set the latest version as variable:\nexport latest_version=$(helm search repo ${name} -o json | jq -r .[].version) Make sure the variables are set:\necho $name $latest_version tid/tetrate-istio 1.16.1   Charts installation via Helm Adding repo and setting up the variables is all prerequisites. Executing the helm install command should be run per below example:\nhelm install tetrate-istio ${name} \\  --set global.hub=containers.istio.tetratelabs.com \\  --set global.tag=${latest_version}-tetrate-v0 \\  --create-namespace --namespace istio-system FIPS Note if customer has purchased Tetrate FIPS Distro - than the tag should include -tetratefips- instead of -tetrate-\nhelm install tetrate-istio ${name} \\ --set global.hub=containers.istio.tetratelabs.com \\ --set global.tag=${latest_version}-tetratefips-v0 \\ --create-namespace --namespace istio-system Output:\n$ export name=\u0026quot;tid/tetrate-istio\u0026quot; $ export latest_version=$(helm search repo ${name} -o json | jq -r .[].version) $ helm install tetrate-istio ${name} \\ \u0026gt; --set global.hub=containers.istio.tetratelabs.com \\ \u0026gt; --set global.tag=${latest_version}-tetrate-v0 \\ \u0026gt; --create-namespace --namespace istio-system NAME: tetrate-istio LAST DEPLOYED: Thu Jan 26 14:29:54 2023 NAMESPACE: istio-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: \u0026quot;TID - Tetrate Distro of Istio 1.16.1-tetrate-v0 successfully installed!\u0026quot; To learn more about the release, try: $ helm status tetrate-istio $ helm get all tetrate-istio More details and feedback [Tetrate Istio Distro](https://istio.tetratelabs.io/) Gateway Note While a separate Envoy Gateway chart is available, it\u0026rsquo;s not necessary to deploy them. The configuration templates deployed with the default chart allow to configure proxyv2 container to act as standalone gateway pod.\nInstalling specific version Tetrate supports multiple versions of Istio. There are many reasons to deploy one of the previous versions of Istio. To achive specific version, follow the following steps\n  Set deployment name variable\nexport name=\u0026#34;tid/tetrate-istio\u0026#34;   List all available versions from Tetrate repo that is added via a procedure deployed above.\n$ helm search repo ${name} --versions NAME CHART VERSION APP VERSION DESCRIPTION tid/tetrate-istio 1.16.1 Tetrate Istio Distro Istiod is simple, safe ent... tid/tetrate-istio 1.16.0 Tetrate Istio Distro Istiod is simple, safe ent... tid/tetrate-istio 1.15.3 Tetrate Istio Distro Istiod is simple, safe ent... tid/tetrate-istio 1.15.1 Tetrate Istio Distro Istiod is simple, safe ent... tid/tetrate-istio 1.14.6 Tetrate Istio Distro Istiod is simple, safe ent... tid/tetrate-istio 1.14.5 Tetrate Istio Distro Istiod is simple, safe ent... tid/tetrate-istio 1.14.4 Tetrate Istio Distro Istiod is simple, safe ent... tid/tetrate-istio 1.14.3 Tetrate Istio Distro Istiod is simple, safe ent.. ...\u0026lt;truncated\u0026gt;   export the version environment based on CHART VERSION\n  export version=1.14.1  deploy desired version using the following command  helm install tetrate-istio ${name} \\  --set global.hub=containers.istio.tetratelabs.com \\  --set global.tag=${version}-tetrate-v0 \\  --create-namespace --namespace istio-system \\  --version $version --devel Verify the Tetrate Istio Distro deployment Use helm command anytime to get the output similar to install command:\nhelm status tetrate-istio -n istio-system Also confirming that all objects are created as expected, use kubectl the output should be something similar to below:\n$ kubectl get all -n istio-system NAME READY STATUS RESTARTS AGE pod/istio-cni-node-4vc9p 1/1 Running 0 21m pod/istio-cni-node-dhp6v 1/1 Running 0 21m pod/istiod-56b5bdd674-vssdl 1/1 Running 0 21m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/istiod ClusterIP 10.100.83.177 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 21m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/istio-cni-node 2 2 2 2 2 kubernetes.io/os=linux 21m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/istiod 1/1 1 1 21m NAME DESIRED CURRENT READY AGE replicaset.apps/istiod-56b5bdd674 1 1 1 21m Upgrade version with latest When you need to upgrade the version, the steps are similar to the install command.\n Follow the Set variables above to set the variables. issue the Helm upgrade command helm upgrade tetrate-istio ${name} \\  --set global.hub=containers.istio.tetratelabs.com \\  --set global.tag=${latest_version}-tetrate-v0 \\  --namespace istio-system   Output:\n$ export name=\u0026quot;tid/tetrate-istio\u0026quot; $ export latest_version=$(helm search repo ${name} -o json | jq -r .[].version) $ echo $name $latest_version tid/tetrate-istio 1.16.1 $ helm upgrade tetrate-istio ${name} \\ \u0026gt; --set global.hub=containers.istio.tetratelabs.com \\ \u0026gt; --set global.tag=${latest_version}-tetrate-v0 \\ \u0026gt; --namespace istio-system Release \u0026quot;tetrate-istio\u0026quot; has been upgraded. Happy Helming! NAME: tetrate-istio LAST DEPLOYED: Thu Jan 26 22:39:25 2023 NAMESPACE: istio-system STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: \u0026quot;TID - Tetrate Distro of Istio 1.16.1-tetrate-v0 successfully installed!\u0026quot; To learn more about the release, try: $ helm status tetrate-istio $ helm get all tetrate-istio More details and feedback [Tetrate Istio Distro](https://istio.tetratelabs.io/) Uninstalling Tetrate Istio Distro Before removing Tetrate Istio Distro - all the application and gateway need to be off-boarded.\nExport chart name into environmental variable and then use helm command:\n$ export chart=$(helm list --namespace istio-system -o json | jq -r .[].name) $ echo $chart tetrate-istio $ helm uninstall ${chart} --namespace istio-system release \u0026quot;tetrate-istio\u0026quot; uninstalled Next Steps The cluster is ready for your applications.\nRead on for next steps with Tetrate Istio Distro and GetMesh.\nFor community support, join Tetrate\u0026rsquo;s #tid-and-getmesh Slack Channel.\n"},{"url":"https://istio.tetratelabs.io/download/","title":"Downloads","description":"Downloads list","content":"Currently Tetrate Istio Distro runs on Linux and MacOS. To deploy Tetrate Istio Distro, all you need is this one simple command:\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash Please follow Tetrate Istio Distro Install and Update Page for detailed instruction on downloading and subsequent steps to have Tetrate Istio Distro up and running in your machine.\nSupported Istio Versions Tetrate Istio Distro tracks Istio upstream releases. As part of Tetrate Istio Distro build pipeline, a series of tests are run to ensure the Istio distro works well on the underlying Kubernetes platform.\nTetrate Istio Distro certified Istio distro has been tested against the following Kubernetes distros (exact support varies on version of Istio used):\n EKS - 1.22 - 1.16 GKE - 1.22 - 1.16 AKS - 1.22 - 1.16  While the core features of Istio have been tested and certified against different Kubernetes distros, users are advised to recognize that certain features of Istio stipulate a minimum Kubernetes version (for example K8S CSR API to sign istio workload needs Kubernetes 1.18+) and certain other features would need provider specific configurations to be set (for example enabling Istio CNI plugin)\nThere are additional download of Istio distributions and istioctl binaries for Linux, MacOS and Windows that you can find in this page. We recommend using GetMesh to obtain the required files.\nGetMesh ensures that all Istio releases are supported for at least 14 months from release date.\nBelow is the summary of Istio releases, security patches and minor updates. Please make sure that the version you're running include the security updates available. Also please plan upgrades when your version is planned to be out of support.  Currently supported releases are 1.17, 1.16, 1.15, 1.14, 1.13, 1.12, 1.11 and 1.10. \n        1.13 is End Of Life by 2023-04-11 please consider migrating to supported version soon\nIstio Binaries Download Linux Istio Distros Below you can find direct URL for Istio and istioctl distros:\nNeed a FIPS compliant distribution?\n Distribution type Istio version Release Notes Full Istio download URL Istioctl download URL   tetrate-v0   1.17.1   Release Notes 1.17.1   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.17.1   Release Notes 1.17.1   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     tetrate-v0   1.16.3   Release Notes 1.16.3   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.16.3   Release Notes 1.16.3   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     tetrate-v0   1.15.4   Release Notes 1.15.4   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.15.4   Release Notes 1.15.4   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     tetrate-v0   1.14.6   Release Notes 1.14.6   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.14.6   Release Notes 1.14.6   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     tetrate-v0   1.13.7   Release Notes 1.13.7   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.13.7   Release Notes 1.13.7   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     tetrate-v0   1.12.8   Release Notes 1.12.8   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.12.8   Release Notes 1.12.8   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     tetrate-v0   1.11.8   Release Notes 1.11.8   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.11.8   Release Notes 1.11.8   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     tetrate-v0   1.10.3   Release Notes 1.10.3   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7     istio-v0   1.10.3   Release Notes 1.10.3   Istio Distro\namd64 arm64 armv7   istioctl\namd64 arm64 armv7    MacOS Istio Distros Below you can find direct URL for Istio and istioctl distros:  Distribution type Istio version Release Notes Full Istio download URL Istioctl download URL   tetrate-v0   1.17.1   Release Notes 1.17.1   Istio Distro   istioctl     istio-v0   1.17.1   Release Notes 1.17.1   Istio Distro   istioctl     tetrate-v0   1.16.3   Release Notes 1.16.3   Istio Distro   istioctl     istio-v0   1.16.3   Release Notes 1.16.3   Istio Distro   istioctl     tetrate-v0   1.15.4   Release Notes 1.15.4   Istio Distro   istioctl     istio-v0   1.15.4   Release Notes 1.15.4   Istio Distro   istioctl     tetrate-v0   1.14.6   Release Notes 1.14.6   Istio Distro   istioctl     istio-v0   1.14.6   Release Notes 1.14.6   Istio Distro   istioctl     tetrate-v0   1.13.7   Release Notes 1.13.7   Istio Distro   istioctl     istio-v0   1.13.7   Release Notes 1.13.7   Istio Distro   istioctl     tetrate-v0   1.12.8   Release Notes 1.12.8   Istio Distro   istioctl     istio-v0   1.12.8   Release Notes 1.12.8   Istio Distro   istioctl     tetrate-v0   1.11.8   Release Notes 1.11.8   Istio Distro   istioctl     istio-v0   1.11.8   Release Notes 1.11.8   Istio Distro   istioctl     tetrate-v0   1.10.3   Release Notes 1.10.3   Istio Distro   istioctl     istio-v0   1.10.3   Release Notes 1.10.3   Istio Distro   istioctl   \nWindows Istio Distros Below you can find direct URL for Istio and istioctl distros:  Distribution type Istio version Release Notes Full Istio download URL Istioctl download URL   tetrate-v0   1.17.1   Release Notes 1.17.1   Istio Distro   istioctl     istio-v0   1.17.1   Release Notes 1.17.1   Istio Distro   istioctl     tetrate-v0   1.16.3   Release Notes 1.16.3   Istio Distro   istioctl     istio-v0   1.16.3   Release Notes 1.16.3   Istio Distro   istioctl     tetrate-v0   1.15.4   Release Notes 1.15.4   Istio Distro   istioctl     istio-v0   1.15.4   Release Notes 1.15.4   Istio Distro   istioctl     tetrate-v0   1.14.6   Release Notes 1.14.6   Istio Distro   istioctl     istio-v0   1.14.6   Release Notes 1.14.6   Istio Distro   istioctl     tetrate-v0   1.13.7   Release Notes 1.13.7   Istio Distro   istioctl     istio-v0   1.13.7   Release Notes 1.13.7   Istio Distro   istioctl     tetrate-v0   1.12.8   Release Notes 1.12.8   Istio Distro   istioctl     istio-v0   1.12.8   Release Notes 1.12.8   Istio Distro   istioctl     tetrate-v0   1.11.8   Release Notes 1.11.8   Istio Distro   istioctl     istio-v0   1.11.8   Release Notes 1.11.8   Istio Distro   istioctl     tetrate-v0   1.10.3   Release Notes 1.10.3   Istio Distro   istioctl     istio-v0   1.10.3   Release Notes 1.10.3   Istio Distro   istioctl   \n"},{"url":"https://istio.tetratelabs.io/faq/","title":"FAQs","description":"Tetrate Istio Distro frequently asked questions.","content":"General FAQ  What is Tetrate Istio Distro? Why should I use Tetrate Istio Distro? Is it free? What are the supported versions of Tetrate Istio Distro? What are the components of the Tetrate Istio Distro and how do they compare to Istio upstream? I am currently using the default distros of Istio. How do I switch to using Tetrate Istio Distro? I am new to Istio. Where should I start? I am a platform admin trying to streamline Istio binaries in my organization. How should I use Tetrate Istio Distro? How can I be alerted to vulnerabilities in my Istio deployments Can Tetrate Istio Distro help me upgrade Istio?  Project FAQ  Are you creating a fork of the Istio project? Do Tetrate Istio Distro builds have a performance impact? How often is Tetrate Istio Distro updated? How do I request features? Can Icontribute to the work done on Tetrate Istio Distro?  What is Tetrate Istio Distro? Tetrate Istio Distro is:\n  An open source project from Tetrate that aims to make adopting, managing, and updating Istio safe and easy.\n  Distros of upstream Istio that are tested and optimized for specific platforms by Tetrate. Tetrate Istio Distro provides vetted builds of Istio for different K8s environments and makes Istio lifecycle management simple and safe.\nWe continuously add support for new K8s flavors and new KMS backends. We support the latest three Istio versions per our support policy. Tetrate Istio Distro also offers FIPS compliant Istio builds for FedRAMP environments.\n  A CLI that facilitates acquiring, installing, and configuring multiple Tetrate Istio Distro distros for multiple environments.\n  A community of Istio contributors and users dedicated to helping each other make the practical use of Istio a joy.\n  Why should I use Tetrate Istio Distro? You should use Tetrate Istio Distro if need an Istio distro tested for use in AWS, Azure, or GCP and an easy way to install, manage, and update Istio in those environments.\nIs it free? Yes. Tetrate Istio Distro is an Apache 2 licenced open source project and all of the builds are available for free. We welcome community participation and contribution. Join our community for updates on new releases, notifications, and regular community events with Istio contributors and practitioners.\nWhat are the supported versions of Tetrate Istio Distro? See the download page for the current supported distributions.\nWhat are the components of Tetrate Istio Distro and how do they compare to Istio upstream? Tetrate Istio Distro is a distribution of upstream Istio which consists of a CLI, an agent and integration APIs. See the following for more details:\n Tetrate Istio Distro CLI Command Reference Cert Integration Security Patches.  I am currently using the default distros of Istio. How do I switch to using Tetrate Istio Distro? Tetrate Istio Distro offers distros of upstream Istio in multiple flavors that may be installed using your existing tooling. You may also use the getmesh CLI to view the available distros, acquire them, and easily manage installation, configuration, and upgrade.\nI am new to Istio. Where should I start?  Start by downloading the getmesh CLI. This will give you instant access to all of the Tetrate Istio Distro builds for the platforms you need. The documentation will introduce you to the fundamentals of installing and configuring Istio. To get up to speed on Istio quickly, we offer free training at Tetrate Academy To get insights from top Istio contributors and practitioners, join the Tetrate Istio Distro community and meet up at our regular Tetrate Istio Distro events.  I am a platform admin trying to streamline Istio binaries in my organization. How should I use Tetrate Istio Distro? There are multiple ways to leverage Tetrate Istio Distro. Please see the command reference and tutorials.\nHow can I be alerted to vulnerabilities in my Istio deployments? Join the Tetrate Istio Distro community for updates on CVEs and zero-day vulnerabilities.\nCan Tetrate Istio Distro help me upgrade Istio? Yes. Tetrate Istio Distro helps you check for available upgrades via the getmesh check-upgrade command.\nProject FAQ Are you creating a fork of the Istio project? No. We provide distributions of upstream tested for specific environments. Any enhancements we make to Istio are applied to upstream.\nDo Tetrate Istio Distro builds have a performance impact? No. As an upstream distribution, Tetrate Istio Distro has no performance impact on Istio.\nHow often is Tetrate Istio Distro updated? We aim to release TID versions within two weeks of an upstream release. This is because it takes time to create the extra build types (such as FIPS, arm64 etc) that TID provides and to complete testing. Where a release contains critical security fixes we apply our best effort to ship the release as soon as possible, starting with the most recent version of Istio.\nHow do I request features? Create a feature requests and vote for features on GitHub.\nCan I contribute to the work done on Tetrate Istio Distro? Yes. Tetrate Istio Distro is an Apache 2 licenced open source project. You can contribute to any component of Tetrate Istio Distro.\nWhat is the CMVP number for the FIPS verison of TID? The FIPS verison of TID embeds BoringCrypto, cerficiate number 3678.\nHow was the FIPS build of TID validated? The FIPS version of TID was tested by a third party NVLAP accredited lab and shown to faithfully implement its cryptographic and signing functions using the above library. A letter from the lab is available to customers with a valid support contract, please contact sales for more details.\n"},{"url":"https://istio.tetratelabs.io/community/","title":"Community","description":"Contributing to the community","content":"There are multiple ways you can contribute and get involved in the community:\n Join the #tid-and-getmesh channel on Tetrate Community Slack. Start contributing to the Tetrate Istio Distro project on Github. Join and participate at one of our events. Watch Tetrate Tech Talks videos. Get Istio certified at Tetrate Academy.  "},{"url":"https://istio.tetratelabs.io/fips-request/","title":"FIPS Download Request","description":"this is meta description","content":" blockquote{display:none !importatnt;}    hbspt.forms.create({ region: \"na1\", portalId: \"7637559\", formId: \"f43c27f5-3c6c-4c7f-a5a2-9122ae768b22\" });\t "},{"url":"https://istio.tetratelabs.io/","title":"Tetrate Istio Distro | Simple, safe enterprise-grade Istio","description":"","content":""},{"url":"https://istio.tetratelabs.io/istio-ca-certs-integrations/acmpca-integration/","title":"AWS Private CA Integration","description":"AWS Private CA Integration","content":"Instead of using a self-signed root certificate, here we get an intermediary Istio certificate authority (CA) from AWS ACM (Amazon Certificate Manager) Private CA service to sign the workload certificates.\nThis approach enables the same root of trust for the root CA\u0026rsquo;s workloads in ACM Private CA. As Istio signs the workload certs, the latency for getting workload certs issued is far less than directly getting the certs signed by ACM Private CA itself.\nThe getmesh gen-ca command furnishes the options to connect to ACM Private CA and get the intermediary CA cert signed. It uses the certificate details thus obtained to create the cacerts Kubernetes secret for Istio to use to sign workload certs. Istio, at startup, checks for the presence of the secret cacerts to decide if it needs to use this cert for signing workload certificates.\nPrerequisites To follow this tutorial, you will need an AWS account and a Kubernetes cluster with Istio installed as well as the following:\n A CA set up in AWS ACM Private CA and the Amazon Resource Name (ARN) of the CA AWS credentials with the AWSCertificateManagerPrivateCAFullAccess and AWSCertificateManagerFullAccess policy attached. Refer to the Configuring the AWS CLI documentation on how to set up the credentials.  You can follow the prerequisites for instructions on how to install and setup Istio.\n Click here, if you need to set up ACM Private CA Setting up ACM Private CA The first thing we need is to set up the ACM Private CA in AWS Console. Log in to your AWS account and follow the steps below to create an ACM Private CA instance.\n From the services dropdown, select Certificate Manager under Security, Identity, \u0026amp; Compliance. Click Get started button under Private certificate authority. Select the Root CA on the certificate authority (CA) type step, and click Next. Configure the CA name (you can use your values here):  For Organization (O), enter Istio. For Organization unit (OU), enter engineering. For Country name (C), select United States (US). For Locality name, enter Sunnyvale. For CA Common name (CN), enter getmesh.example.io. Click Next.   Configure the CA key size and algorithm:  Click Advanced to expand the options. Select RSA 2048. Click Next.   On the \u0026ldquo;Configure certificate revocation\u0026rdquo; step, click Next. On the \u0026ldquo;Add tags\u0026rdquo; step, click Next. On the \u0026ldquo;Configure CA permissions\u0026rdquo; step, click Next. On the \u0026ldquo;Review\u0026rdquo; step, select the confirmation check box, and click Confirm and create button to create ACM Private CA. Click the Create button to create the CAS.  Before ACM Private CA can start issuing certificates, you need to activate by installing a CA certificate.\n From the \u0026ldquo;Private CAs\u0026rdquo; page, click the Install a CA certificate to active your CA link. Change the validity to 365 days. Select SHA256WITHRSA from the Signature algorithm list. Click Next. Click the Confirm and install button to generate, and install the root CA certificate.  The figure below shows the Private CA page. Note that yours might look different if you configured your own CA subject name.\nConfigure AWS credentials Ensure you have AWS credentials set up with the AWSCertificateManagerPrivateCAFullAccess and AWSCertificateManagerFullAccess policy attached on a machine you\u0026rsquo;re accessing the Kubernetes cluster from. Alternatively, if you installed Tetrate Istio Distro on AWS Cloud Shell, the credentials are already set up.\n Creating ACM configuration We will use a YAML configuration to configure ACM Private CA. Use the YAML below as a template, and enter the ACM Private CA information from the AWS console:\nproviderName: \u0026#34;aws\u0026#34; providerConfig: aws: # This will hold the ARN value from the Details page of ACM Private CA  signingCAArn: \u0026#34; arn:aws:acm-pca:us-east-2:859085711342:certificate-authority/097162cc-6a9e-47ab-b5e0-fecf32556d6d\u0026#34; templateArn: \u0026#34;arn:aws:acm-pca:::template/SubordinateCACertificate_PathLen0/V1\u0026#34; signingAlgorithm: SHA256WITHRSA certificateParameters: secretOptions: istioCANamespace: \u0026#34;istio-system\u0026#34; # namespace where `cacerts` secrets live overrideExistingCACertsSecret: true # overwrites the existing `cacerts` secret and replaces it with this new one caOptions: validityDays: 365 # validity days before the CA expires keyLength: 2048 # length (bits) of Key to be created certSigningRequestParams: # x509.CertificateRequest; most fields omitted subject: commonname: \u0026#34;getmesh.example.io\u0026#34; country: - \u0026#34;US\u0026#34; locality: - \u0026#34;Sunnyvale\u0026#34; organization: - \u0026#34;Istio\u0026#34; organizationunit: - \u0026#34;engineering\u0026#34; emailaddresses: - \u0026#34;youremail@example.io\u0026#34; Save the above file to aws-acm-config.yaml and use gen-ca command to create the cacert:\ngetmesh gen-ca --config-file aws-acm-config.yaml The command output should look similar to this:\nKubernetes Secret YAML created successfully in /home/user/.getmesh/secret/getmesh-740905469.yaml Kubernetes Secret created successfully with name: cacerts, in namespace: istio-system Before continuing, make sure to delete the istiod Pod in the istio-system namespace to force it to use the created cacerts.\nTry it out If you\u0026rsquo;ve labeled the default namespace for automatic sidecar injection (see Prerequisites), we can then deploy a sample Hello world application:\nkubectl create deploy helloworld --image=gcr.io/tetratelabs/hello-world:1.0.0 Wait for the Pod to start and then get the certificate chain and CA root certificate proxies use for mTLS. We will save them in the proxy_secret file:\ngetmesh istioctl pc secret [pod-name] -o json \u0026gt; proxy_secret The CA root certificate is base64 encoded in the trustedCA field. For example:\n... { \u0026#34;name\u0026#34;: \u0026#34;ROOTCA\u0026#34;, \u0026#34;versionInfo\u0026#34;: \u0026#34;2021-02-08 22:42:32.301677034 +0000 UTC m=+0.917666618\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-02-08T22:42:32.310Z\u0026#34;, \u0026#34;secret\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ROOTCA\u0026#34;, \u0026#34;validationContext\u0026#34;: { \u0026#34;trustedCa\u0026#34;: { \u0026#34;inlineBytes\u0026#34;: \u0026#34;\u0026lt;base64 encoded value\u0026gt;\u0026#34;  } } } Store the decoded value to the encodedCA.crt file and then use openssl to decrypt the certificate into a more readable form:\nopenssl x509 -text -noout -in encodedCA.crt The output will include the common name, organization and other values we set in the CAS:\nCertificate: Data: Version: 3 (0x2) Serial Number: 94:d9:65:d0:b0:42:ac:31:70:f8:9f:84:02:6b:b1:d7:8d:2e:cb:c8 Signature Algorithm: sha256WithRSAEncryption Issuer: C = US, L = Sunnyvale, O = Istio, OU = engineering, CN = getmesh.example.io  Validity Not Before: Feb 8 22:23:59 2021 GMT Not After : Jan 27 22:23:59 2031 GMT Subject: C = US, L = Sunnyvale, O = Istio, OU = engineering, CN = getmesh.example.io  Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) ... "},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/install-istio/","title":"Install Istio","description":"How to install Istio with GetMesh CLI","content":"Tetrate Istio Distro by default communicates to the cluster defined by your Kubernetes configuration.\nBefore proceeding, please:\n make sure your current kubernetes context points to the correct cluster the required version and flavor of Istio version are set. The chosen version/flavor will be applied to the all following commands. Below is the example of the command sequence.   first list available versions using getmesh list command, output of the command will list all the versions available and highlights the current version with asterix (*)\ngetmesh list ISTIO VERSION FLAVOR FLAVOR VERSION K8S VERSIONS 1.16.0 tetrate 0 1.22,1.23,1.24,1.25 1.16.0 tetratefips 0 1.22,1.23,1.24,1.25 1.16.0 istio 0 1.22,1.23,1.24,1.25 1.15.3 tetrate 0 1.22,1.23,1.24,1.25 1.15.3 tetratefips 0 1.22,1.23,1.24,1.25 1.15.3 istio 0 1.22,1.23,1.24,1.25 1.15.1 tetrate 0 1.22,1.23,1.24,1.25 1.15.1 tetratefips 0 1.22,1.23,1.24,1.25 1.15.1 istio 0 1.22,1.23,1.24,1.25 *1.14.5 tetrate 0 1.21,1.22,1.23,1.24 1.14.5 tetratefips 0 1.21,1.22,1.23,1.24 1.14.5 istio 0 1.21,1.22,1.23,1.24 ...   Change getmesh to the desired version by issuing getmesh switch command. (e.g. to set the desired version to 1.15.3 and flavor to tetrate - the following command will be issued:\ngetmesh switch --version 1.15.3 --flavor=tetrate the confirmation will look like the following:\nistioctl switched to 1.15.3-tetrate-v0 now     To install the demo profile of Istio, That can be done using getmesh istioctl command\ngetmesh istioctl install --set profile=demo Output:\n$ getmesh istioctl install --set profile=demo This will install the Istio demo profile with [\u0026quot;Istio core\u0026quot; \u0026quot;Istiod\u0026quot; \u0026quot;Ingress gateways\u0026quot; \u0026quot;Egress gateways\u0026quot;] components into the cluster. Proceed? (y/N) Y ✔ Istio core installed ✔ Istiod installed ✔ Ingress gateways installed ✔ Egress gateways installed ✔ Installation complete By default, istio installation profiles points to normal istio images(distro based), In case users want to install istio with distroless images, this can be done by using the \u0026ndash;set hub and tag commands as below.\ngetmesh istioctl install --set profile=demo --set hub=containers.istio.tetratelabs.com --set tag=1.12.4-tetratefips-v0-distroless Output:\n$ getmesh istioctl install --set profile=demo --set hub=containers.istio.tetratelabs.com --set tag=1.12.4-tetratefips-v0-distroless This will install the Istio demo profile with [\u0026quot;Istio core\u0026quot; \u0026quot;Istiod\u0026quot; \u0026quot;Ingress gateways\u0026quot; \u0026quot;Egress gateways\u0026quot;] components into the cluster. Proceed? (y/N) Y ✔ Istio core installed ✔ Istiod installed ✔ Ingress gateways installed ✔ Egress gateways installed ✔ Installation complete Once this step is completed, the validation can be done by confirming the GetMesh and Istio versions installed, using the version command:\ngetmesh version Output:\n$ getmesh version getmesh version: 0.6.0 active istioctl: 1.8.2-tetrate-v0 client version: 1.8.2-tetrate-v0 control plane version: 1.8.2-tetrate-v0 data plane version: 1.8.2-tetrate-v0 (2 proxies) In addition to the version output, a user can validate the expected functionality by issuing the config-validate command (read more around config validation):\ngetmesh config-validate With fresh install of Kubernetes cluster and Istio, the output will look similar to the below:\n$ getmesh config-validate Running the config validator. This may take some time... NAMESPACE NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE default default Namespace IST0102 Info The namespace is not enabled or Istio injection. Run 'kubectl label namespace default istio-injection=enabled' to enable it, or 'kubectl label namespace default istio-injection=disabled' to explicitly mark it as not needing injection. The error codes of the found issues are prefixed by 'IST' or 'KIA'. For the detailed explanation, please refer to - https://istio.io/latest/docs/reference/config/analysis/ for 'IST' error codes - https://kiali.io/documentation/latest/validations/ for 'KIA' error codes "},{"url":"https://istio.tetratelabs.io/categories/","title":"Categories","description":"","content":""},{"url":"https://istio.tetratelabs.io/categories/event/","title":"event","description":"","content":""},{"url":"https://istio.tetratelabs.io/tags/istio/","title":"Istio","description":"","content":""},{"url":"https://istio.tetratelabs.io/tags/","title":"Tags","description":"","content":""},{"url":"https://istio.tetratelabs.io/blog/tuya-istio-practice/","title":"Tuya Smart’s Implementation of an Istio Enterprise-level Production","description":"Istio practice in Tuya.","content":"An Istio use case of Tuya Smart.\nBackground As the most active service mesh project at present, Istio provides numerous capabilities, including traffic management, security, and observability. Each capability is necessary for the management, operation, and maintenance of services. Istio\u0026rsquo;s extensive capabilities also bring certain challenges to the operation and maintenance of complex systems. In terms of its capabilities and future scalability, Istio reimagines service management, bringing about new opportunities along with challenges.\nTuya Smart currently uses Istio control plane version 1.5.0 for its front-end business, allowing access to the Istio control plane by over 700 services and 1,100 pod instances that are responsible for the traffic control and capacity support of the largest business cluster at the front-end of Tuya Smart.\nExisting challenges faced by Tuya Smart in the development process Tuya Smart’s Front-end Core Technology Team began to contact Kubernetes in 2018 and built a Kubernetes-based release platform to serve the front-end business team. However, as the business team grew larger, some issues began to emerge in the development and release process:\n Authentication issues in multi-branch parallel development Network issues caused by configuration complexity due to a multi-regional environment  Initially, we considered letting the business team make adjustments accordingly and deal with it internally. But as more of our teams experienced similar difficulties, we thought that gray release capabilities would be a better solution for these issues in the development and release process. In the daily pre-release environment, multiple branches release multiple grayscale versions, and distribute traffic to different versions according to different headers, ensuring that each feature branch has a separate instance for authentication, and does not affect each other. The online environment provides grayscale capabilities for regression testing and acts as the last line of defense for project quality.\nWe investigated a number of solutions and internally referred to the grayscale solutions of other teams. Due to the complexity of implementation and the diversity of capabilities, we began to deploy Istio in the production environment in early 2020. Although integration with Istio has increased the complexity of the system to a certain extent, it has also given us many unexpected benefits.\nIssues resolved by Istio Gray Release The gray release capability of the platform is built based on Istio\u0026rsquo;s native resources, VirtualService and DestinationRule.\n Each application released by the platform is marked with two labels: canarytag and stage. stage is used to identify normal releases and gray releases, and the canarytag is used to identify different grayscale versions. Every time a grayscale version is released, a corresponding grayscale ReplicaSet instance is created. The DestinationRule configuration corresponding to the release carries the label canarytag, stage defines normal releases and different versions of grayscale instances as different instance sets. A collection of different instances is distributed by VirtualService according to different headers.  The figure below shows the configuration information.\nTraffic observation and irregularity detection We built our monitoring platform based on the community’s native prometheus-operator. Each cluster deploys a separate prometheus-operator and divides it into three aspects according to the target objects to be collected: business, Kubernetes cluster infrastructure, and Istio data plane traffic. We deploy the corresponding business prometheus instance: Kubernetes cluster infrastructure for monitoring the prometheus instance and Istio traffic for monitoring the prometheus instance.\nWith Grafana, an overall data plane traffic monitoring system was built to observe traffic.\nBased on the current monitoring data, the corresponding alert rules are configured, and there are means to detect and discover irregularities and fluctuations in traffic, and deal with them in a timely manner. sum(envoy_cluster_upstream_cx_active{namespace!=\u0026ldquo;fe-pre\u0026rdquo;}) by (namespace, service) \u0026lt; 1 No available service alert\nsum by(namespace, service) (rate(envoy_cluster_upstream_rq_503[1m])) \u0026gt; 0 503 Irregularity alert sum by(namespace, service) (rate(envoy_cluster_upstream_rq{response_code_class!=\u0026#34;2xx\u0026#34;}[1m])) != 0 Business irregularity alert Current outcomes and existing problems All pod instances of the largest business cluster currently online have been connected to the Istio control plane. Istio controls the overall traffic and has the ability to observe traffic.\nThe number of grayscale releases accounts for more than 60% of the total number of releases. An increasing number of projects across the company’s two largest business lines have begun to utilize grayscale release capabilities. The grayscale capabilities and their level of stability have been recognized and endorsed by the businesses.\nHowever, with the increasing business volume and the increasing number of pod instances, some problems have also been encountered:\n The Istio version used by the team is 1.5.0. When the pilot pushes a large number of xds updates, this version will cause Envoy’s readiness probe check to fail, and again cause a large number of eds updates, causing cluster fluctuations. This problem has been resolved by the community and the team also plans to upgrade to version 1.7.7. After the pilot machine restarts irregularly, Envoy, which has been connected to the pilot instance, cannot perceive the irregularity of the server. It needs to wait for the tcp keepalive to time out and check for failure before it starts to reconnect to the normal Istiod. During this time, the cluster updates are not synchronized. By default, you must wait for 975 seconds. This problem can be solved by reconfiguring Envoy boot. Modify tcp_keepalive of the Envoy default boot configuration xds-grpc cluster upstream_connection_options to ensure reconnection within 1 minute.  \u0026#34;upstream_connection_options\u0026#34;: { \u0026#34;tcp_keepalive\u0026#34;: { \u0026#34;keepalive_time\u0026#34;: 30, \u0026#34;keepalive_probes\u0026#34;: 3, \u0026#34;keepalive_interval\u0026#34;: 5 } } Our future The Tuya Front-end Core Technology Team started using Istio in early 2020. We are still exploring Istio\u0026rsquo;s extensive capabilities and its powerful scalability. In the future, we will focus on exploring and testing on two different aspects:\n Refined management of traffic and service degradation and fusing based on the current capabilities of Istio. The current service management capabilities for front-end microservices are deficient and there is no effective means to ensure service stability when encountering irregularity cases. Addition of Istio-based fault injection capability to inject fault use cases into services, improving the fault tolerance and stability of the overall business system.  Author: Tuya Smart Front-end Core Technology Team (涂鸦智能前端基础技术团队)\n"},{"url":"https://istio.tetratelabs.io/blog/debugging-services-with-istio/","title":"How to debug services with Istio?","description":"In this blog, I will explain a couple of approaches you can use to debug services running inside your Kubernetes cluster, without deploying the version of the service you want to debug to the cluster.","content":"In this blog, we will explain a couple of approaches you can use to debug services running inside your Kubernetes cluster, without deploying the version of the service you want to debug to the cluster.\nUsing traffic mirroring One way to debug the services is to use traffic mirroring. The way traffic mirroring works is that it copies the real traffic and sents it to the mirrored version. Any responses from the mirrored service get dropped, so real traffic is not being affected at all.\nThe idea is that you have the source code to the service opened in your development IDE. You start the debugger and then, instead of manually interacting with the service or sending the requests to the service running locally, you make a request to the production service. The traffic from that service gets mirrored to the service running locally on your machine.\nSince you will be running the service under the debugger on your machine, we can use the service entry resource to point to the service running on your computer, then mirror the traffic to that. For this approach to work, you will have to expose the service running on your machine through a public IP that can be accessible from the cluster. We will then use that IP as an endpoint in the Service entry resource. The diagram below shows for this idea.\nThe traffic or request comes in to through the Hello web and gets mirrored to the external service, defined by the service entry. That service entry is using the static resolution to route to the IP address that\u0026rsquo;s exposing the locally running Greeter service to the public.\nLet\u0026rsquo;s deploy Hello web service:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: helloweb labels: app: helloweb version: v1 spec: replicas: 3 selector: matchLabels: app: helloweb version: v1 template: metadata: labels: app: helloweb version: v1 spec: containers: - name: web image: learnistio/hello-web:1.0.0 imagePullPolicy: Always ports: - containerPort: 3000 env: - name: GREETER_SERVICE_URL value: \u0026#39;http://greeter-service.default.svc.cluster.local:3000\u0026#39; --- kind: Service apiVersion: v1 metadata: name: helloweb labels: app: helloweb spec: selector: app: helloweb ports: - port: 3000 name: http EOF Let\u0026rsquo;s deploy the Greeter service:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: greeter-service-v1 labels: app: greeter-service version: v1 spec: replicas: 3 selector: matchLabels: app: greeter-service version: v1 template: metadata: labels: app: greeter-service version: v1 spec: containers: - name: svc image: learnistio/greeter-service:1.0.0 imagePullPolicy: Always ports: - containerPort: 3000 --- kind: Service apiVersion: v1 metadata: name: greeter-service labels: app: greeter-service spec: selector: app: greeter-service ports: - port: 3000 name: http EOF Let\u0026rsquo;s deploy the Virtual service for the Hello web:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloweb spec: hosts: - \u0026#39;*\u0026#39; gateways: - gateway http: - route: - destination: host: helloweb.default.svc.cluster.local port: number: 3000 EOF This gives us a working Hello web and with Greeter service v1. As a next step, let\u0026rsquo;s deploy a virtual service that routes 100% of the traffic to the v1 version of the greeter service and mirrors the traffic to a service we are calling greeter-service.ext - we will define this later.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service port: number: 3000 mirror: host: greeter-service.ext mirrorPercent: 100 EOF To mirror traffic to the service running on the local machine, we will need to create a ServiceEntry for the greeter-service.ext and use a static IP address to reach it. To get the static IP, we need to launch the debug version of the service locally and then expose it through the public IP.\nOpen the source to the greeter service in your favorite code editor (I am using Visual Studio Code), and start the service with debugging. If you are using Visual Studio Code, you can set the breakpoints, press the F5 button, pick Node.JS as an option and the editor will automatically run the service under the debugger. Your Visual Studio Code instance should look similar to the one in the figure below.\nIf you open your browser at http://localhost:3000/hello or /version the debugger will break at the set breakpoint. This is what we want to happen, but the traffic hits the production service running in the cluster.\nAs a next step, we need to expose the localhost:3000 and use IP address where the service is exposed in the ServiceEntry resource. The ServiceEntry resource would look like this (remember to replace the IP_ADDRESS with an actual IP address where your service is exposed on):\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: greeter-service.ext spec: hosts: - greeter-service.ext ports: - number: 80 protocol: HTTP name: http resolution: STATIC location: MESH_EXTERNAL endpoints: - address: [IP_ADDRESS] ports: http: 80 EOF With these resources deployed, you can send a request to the Hello web (e.g. http://localhost), and you will see the breakpoint will get hit in your editor.\nUsing Kubernetes service and Istio The previous approach can be awkward, mainly because you need a public IP address to be able to use the service entry to point to it. The second approach I am going to explain uses ngrok and a combination traffic rules to redirect traffic meant for debugging to the version of the service running locally on your machine. The figure below explains this approach in more detail.\nAt first glance, the approach looks similar to the previous one. The difference is that in this case, we won\u0026rsquo;t be mirroring the production traffic, which gives us more control over debugging. Instead, we will use a specific header (x-user) and utilize the redirect functionality in the virtual service to direct traffic to the debug version of the service. The service running locally will be exposed through the public URL using ngrok.\nIf you have the Hello web, corresponding virtual service and a greeter service v1 as well as the gateway deployed, you can continue. Otherwise, look at the previous section for commands to deploy those resources.\nStart the local greeter service and run it under the debugger, then launch ngrok with the following command:\nngrok http 3000 Running this command exposes your localhost:3000 on a publicly available address. When I ran the command, the external URL I got was http://97260680.ngrok.io. Try opening the ngrok URL in the browser, appending either /version or /hello to the URL to double-check everything still works - the breakpoint you have set should get hit. Make sure this works before continuing.\nLet\u0026rsquo;s create the virtual service and create the match condition and redirect:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - match: - headers: x-user: exact: debug redirect: authority: 97260680.ngrok.io - route: - destination: host: greeter-service port: number: 3000 EOF The above virtual service tries to match the x-user header to debug value and if there\u0026rsquo;s a match, it does a redirect to the URL specified in the authority field - this is the URL where our service running locally is exposed on. If you deploy this, you can send the following request using curl to see the breakpoint hit in your service:\n$ curl -v -H \u0026#34;x-user: debug\u0026#34; http://localhost Similarly, visiting the URL in from a browser and setting a header will trigger a breakpoint as well. If you remove the x-user header or set it to a different value, the breakpoint won\u0026rsquo;t get hit and the service running inside the cluster will get invoked instead.\n"},{"url":"https://istio.tetratelabs.io/blog/istio-security/","title":"How does Istio service mesh deal with security?","description":"In this blog, I will explain how Istio can help to solve issues such as encrypting traffic, provide flexible service access control, configure mutual TLS and fine-grained access policies and auditing. ","content":"In this blog, I will explain how Istio can help to solve issues such as encrypting traffic, provide flexible service access control, configure mutual TLS and fine-grained access policies and auditing.\nIstio Security Architecture The following Istio components are involved in providing security features in Istio:\n Certificate authority (CA) for managing keys and certificates Sidecar and perimeter proxies: implement secure communication between clients and servers (they work as Policy Enforcement Points (PEPs) Envoy proxy extensions: manage telemetry and auditing Configuration API server: distributes authentication, authorization policies and secure naming information   Policy Enforcement Point (PEP) is a component that serves as a gatekeeper to a resource.\n Let\u0026rsquo;s look at the architecture diagram in the figure below for different components and their responsibilities.\nAuthentication Based on the definition, authentication is a process or action of verifying the identity of a user or a process. This means Istio needs to extract credentials from requests and prove they are authentic. Envoy proxies in Istio are using a certificate for their credentials when communicating with each other. These certificates are tied to service accounts in Kubernetes.\nWhen two services start communicating, they need to exchange the credentials with identity information to mutually authenticate themselves. The client checks the server\u0026rsquo;s identity against the secure naming information to see if it is an authorized runner of the service. On the server-side, the server determines what information the client can access based on the authorization policies. Additionally, the server can audit who accessed what at what time, and make decisions whether to approve or reject clients from making calls to the server. The secure naming information contains mappings from service identities to the service names. The server identities are encoded in certificates, and the service names are names used by the discovery service or DNS. A single mapping from an identity A to a service name B means that \u0026ldquo;A is allowed and authorized service B\u0026rdquo;. Secure naming information gets generated by the Pilot and then distributed to all sidecar Envoys.\nIdentity For issuing identities, Istio uses Secure Production Identity Framework for Everyone, or SPIFFE (pronounced spiffy). SPIFFE is a specification for a framework that can bootstrap and issue identities. Citadel implements the SPIFFE spec; another implementation of SPIFFE is called SPIRE (SPIFFE Runtime Framework).\nThere are three concepts to the SPIFFE standard:\n SPIFFE ID: identity namespace that defines how service identify themselves SPIFFE Verifiable Identity Document (SVID): dictates how an issued identity is presented and verified. It encodes the SPIFFE ID. Workload API: specifies an API for a workload issuing and/or retrieving antoher workload\u0026rsquo;s SVID  In Kubernetes, service accounts are used for service identity. The URI that represents the SPIFFE ID is formatted like this: spiffe://cluster-name/ns/namespace/sa/service-account-name. By default, any pods that don\u0026rsquo;t set a service account explicitly will use the default service account that\u0026rsquo;s deployed in a namespace.\nYou can take look at the service account and the corresponding secret like this:\n$ kubectl describe sa default Name: default Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: default-token-pjqr9 Tokens: default-token-pjqr9 Events: \u0026lt;none\u0026gt; The mountable secret/token name is the name of the secret in the same namespace that contains the certificate and token.\n$ kubectl describe secret default-token-pjqr9 Name: default-token-pjqr9 Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: fe107ed9-8707-11e9-9803-025000000001 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 7 bytes token: ey.... The SPIFFE ID for the default service account would therefore be encoded like this: spiffe://cluster.local/ns/default/sa/default. The specification also describes how to encode this identity into a certificate that can be used to prove the identity. The SPIFFE says that the identity (the URI) needs to be encoded in the certificate\u0026rsquo;s subject alternative name (SAN).\nFinally, the workload API for issuing and retrieving SVIDs in Istio is implemented using ACME (Automatic Certificate Management Environment) protocol.\nThe Citadel component automatically creates the certificate for existing and new service accounts, then stores them as Kubernetes secrets. If you create a deployment and look at the pod spec, you will notice something like this:\n... volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-pjqr9 readOnly: true ... Using this snippet, Kubernetes mounts the certificate and other information from the service account to the pod. Because issued certificates are short-lived for security purposes (i.e. even if the attacker can get the SVID, they can only use it for a short time), Citadel ensures that certificates get rotated automatically.\nMutual TLS authentication Transport authentication, also known as service-to-service authentication is one of the authentication types supported by Istio. Istio implements mutual TLS as a solution for transport authentication.\nTLS stands for Transport Layer Security. TLS is used each time you try to access a secure endpoint. For example, visiting https://learnistio.com over HTTPS leverages TLS to secure the communication between the server where the website is running, and your browser. It doesn\u0026rsquo;t even matter if sensitive or private information is being transferred - the connection is secured regardless.\nUsing TLS requires a certificate authority (CA) to issue a digital certificate to the server, and this server then hands it over to the browser for validation with the CA.\nmTLS takes the same idea but applies it to applications or services. This means that instead of the client only verifying the servers' certificate, the server also verifies the clients certificate.\nAn example of TLS would be crossing a border where you need to present your passport (a certificate) to the customs officer. Customs officer ensures your passport is valid, hasn\u0026rsquo;t expired, etc. In the mTLS case, you would also ask for a passport from the customs officer, and you would validate it.\nOnce both parties have validated the certificates with their respective CAs, the communication between parties can happen securely.\nIn the case of Istio, all communication between services goes through the Envoy proxies. Here are the steps that happen when the call gets made from service A to service B:\n Traffic gets routed from service A to the Envoy proxy in the same pod Service A proxy starts an mTLS handshake with the Service B proxy (secure naming check happens as well) mTLS connection gets established Traffic gets forwarded to the Service B proxy Service B proxy forwards traffic to the service B in the same pod  Mutual TLS in Istio supports a permissive mode. This mode allows a service to accept both plain text traffic and mTLS traffic at the same time. This can help you gradually migrate your services to mTLS, without breaking existing plain text traffic. Once all services have the proxy, you can configure mTLS only mode instead.\nTo configure mTLS between services, the traffic policy field in the destination rule is used. For example, to require a client to use mTLS when communcating with the service-b, you\u0026rsquo;d use the ISTIO_MUTUAL mode:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: service-b-istio-mtls spec: host: service-b.default.svc.cluster.local trafficPolicy: tls: mode: ISTIO_MUTUAL You could also provide your own certificates and set mode to MUTUAL like this:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: service-b-mtls spec: host: service-b.default.svc.cluster.local trafficPolicy: tls: mode: MUTUAL clientCertificate: /etc/certs/cert.pem privateKey: /etc/certs/pkey.pem caCertificates: /etc/certs/cacerts.pem Finally, you can set the mode field to SIMPLE to configure the client to use TLS:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: service-b-tls spec: host: service-b.default.svc.cluster.local trafficPolicy: tls: mode: SIMPLE Origin authentication Origin authentication, known as end-user authentication, is used for verifying original clients requesting as an end-user or device. Istio enables original authentication with JSON Web Token (JWT) validation and open-source OpenID connect providers (e.g. Googe Auth, Auth0 or Firebase Auth).\nIn the case of origin authentication (JWT), the application itself is responsible for acquiring and attaching the JWT token to the request.\nAuthentication policies Authentication policies are used to specify authentication requirements for services within the mesh. Similarly, as with traffic routing, Pilot watches for changes in the policy resources and then translates and pushes the configuration to the Envoy proxies.\nThese policies define what authentication methods can be accepted (i.e. requests being received). While for the outgoing requests, you would use the destination rule as explained earlier in this blog. The figure below illustrates this:\nAuthentication policies can be defined in two scopes that are explained next.\nNamespace-scoped policy\nThe policies in the namespace scope can only affect services running in the same namespace. Additionally, you need to specify the namespace name, otherwise, the default namespace is used. Here\u0026rsquo;s an example of a namespace policy for the prod namespace:\napiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default spec: mtls: mode: STRICT Mesh-scoped policy\nMesh-scoped policies can apply to all services in the mesh. You can only define one mesh-scope policy with the name default and an empty targets section. One difference from the namespace-scoped policy is the resource name. While namespace-scope policy resource is called \u0026ldquo;Policy\u0026rdquo; the mesh-scoped policy resource is called \u0026ldquo;MeshPolicy\u0026rdquo;.\nTarget selectors To define which services are affected by the policies, target selectors are used. Target selectors are a list of rules to selected services that the policy should be applied. If a target selector is not provided, the policy is used on all services in the same namespace.\nFor example, a namespace-scope policy below would apply for the service-a (regardless of the ports) and service-b on port 8080:\napiVersion: authentication.istio.io/v1alpha1 kind: PeerAuthentication metadata: name: sample-policy namespace: prod spec: target: - name: service-a - name: service-b ports: - number: 8080 In the case of multiple policies, they get evaluated from the narrowest matching policy (e.g. service-specific), to namespace and the mesh wide. If more than one policies apply to a service, one is randomly chosen.\nTransport authentication The field called peers defines the authentication methods and any parameters for the method. At the time of writing this, the only supported authentication method is mTLS. To enable it, use the mtls key like this (using the previous example):\napiVersion: authentication.istio.io/v1alpha1 kind: PeerAuthentication metadata: name: sample-policy namespace: prod spec: target: - name: service-a - name: service-b ports: - number: 8080 peers: - mtls: ... Origin authentication The only origin authentication currently supported by Istio is JWT. Using the origins field, you can define the method and parameters, such as allowed JWT issuers and enable or disable JWT authentication for a specific path. Here\u0026rsquo;s a sample snippet that shows how to define origin authentication that accepts JWTs issued by Google. Additionally, we are excluding the /health path from JWT authentication:\norigins: - jwt: issuer: https://accounts.google.com jwksUri: https://www.googleapis.com/oauth2/v3/certs trigger_rules: - excluded_paths: - exact: /health Authorization Authorization feature can be used to enable access control on workloads in the mesh. The policy supports both ALLOW and DENY policies. In case when you\u0026rsquo;re using both allow and deny policies at the same time, the deny policies get evaluated first. Each Envoy proxy uses an authorization engine that decides at runtime if requests should be allowed or denied.\nWhen requests reach the proxy, the authorization engine evaluates the request and returns the authorization result - either ALLOW or DENY. The policies are evaluated in the following order:\n If any DENY policy matches the request → deny the request If there no ALLOW policies for the workload → allow the request If any of the ALLOW policies match the request → allow the request Deny the request  There is no need to separately enable any authorization features. It\u0026rsquo;s enough to create and apply an authorization policy for your workloads. By default, if there are no authorization policies defined, no access control is enforced and all requests are allowed.\nAuthorization policy is configured using the AuthorizationPolicy resource. This resource includes a selector (target workloads), action (allow or deny) and the list of rules that specify when to trigger the action.\nFor example, with the snippet below you can apply an authorization policy to any workloads with labels app=greeter-service and version=v2 set. Once the request comes to the Envoy proxy of the workload, the authorization engine checks if the traffic is coming from the principal with the provided service account (helloweb) and if the operation is a GET and the x-user header is set to user-1 - if all these are satisfied, the request is allowed, otherwise, the request gets denied.\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: greeter-service namespace: default spec: action: ALLOW selector: matchLabels: app: greeter-service version: v2 rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/helloweb\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.headers[x-user] values: [\u0026#34;user-1\u0026#34;] We are specifically applying the authorization policy to workloads labelled with app: greeter-service and version: v2. If we wanted to apply the policy to all workloads in the default namespace, we could simply omit the selector field like this:\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: greeter-service namespace: default spec: rules: - from: - source: principals: [\u0026#34;cluster.local/ns/default/sa/helloweb\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.headers[x-user] values: [\u0026#34;user-1\u0026#34;] You can also define an authorization policy that applies to all workloads in your service mesh, regardless of the namespace. To do that, you need to create an AuthorizationPolicy in the root namespace. By default, the root namespace is istio-system. If you need to change it, you will have to update the rootNamespace field in the MeshConfig.\nValue matching You can use the following matching schemes for most fields in the authorization policy:\n Exact: matches an exact string Prefix: matches strings that start with the specified value ([prefix]*). For example: \u0026ldquo;hello.world\u0026rdquo; matches \u0026ldquo;hello.world.blah\u0026rdquo;, but not \u0026ldquo;blah.hello.world\u0026rdquo; Suffix: matches strings that end with the specified value (*[suffix]). For example: \u0026ldquo;hello.world\u0026rdquo; matches \u0026ldquo;blah.hello.world\u0026rdquo;, but not \u0026ldquo;hell.world.blah\u0026rdquo; Presence: matches any value, except empty (i.e. value must be provided, but we don\u0026rsquo;t care what it is as long as it\u0026rsquo;s not empty)  A couple of fields are exempted and only support exact matching:\n key field under the when section ipBlocks field under the source section ports field under the to section  Here\u0026rsquo;s an example of how to allow access to any path under /api as long as it\u0026rsquo;s a GET operation:\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: greeter-service namespace: default spec: selector: matchLabels: app: greeter-service action: ALLOW rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] paths: [\u0026#34;/api/*\u0026#34;] Exclusions In addition to inclusion matching, Istio also support matching exclusions. This means you can match negative conditions like notValues, notPorts or notIpBlocks. The following snippet allows requests that are not under the /private path:\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: greeter-service namespace: default spec: selector: matchLabels: app: greeter-service action: ALLOW rules: - to: - operation: notPaths: [\u0026#34;/private\u0026#34;] Deny all and allow all To create an allow all authorization policy that allows full access to all workloads in the specified namespace, you can create a policy with an empty rules section like this:\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-all namespace: default spec: action: ALLOW rules: - {} Similarly, you can deny access to all workloads by using an empty spec field:\napiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: deny-all namespace: default spec: {} Examples To demonstrate the security features, we will deploy the Hello Web, Greeter service, and corresponding virtual service.\nStart with the greeter deployment and service:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: greeter-service-v1 labels: app: greeter-service version: v1 spec: replicas: 1 selector: matchLabels: app: greeter-service version: v1 template: metadata: labels: app: greeter-service version: v1 spec: containers: - image: learnistio/greeter-service:1.0.0 imagePullPolicy: Always name: svc ports: - containerPort: 3000 --- kind: Service apiVersion: v1 metadata: name: greeter-service labels: app: greeter-service spec: selector: app: greeter-service ports: - port: 3000 name: http EOF Then create the Hello web deployment and service:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: helloweb labels: app: helloweb version: v1 spec: replicas: 1 selector: matchLabels: app: helloweb version: v1 template: metadata: labels: app: helloweb version: v1 spec: containers: - image: learnistio/hello-web:1.0.0 imagePullPolicy: Always name: web ports: - containerPort: 3000 env: - name: GREETER_SERVICE_URL value: \u0026#39;http://greeter-service.default.svc.cluster.local:3000\u0026#39; --- kind: Service apiVersion: v1 metadata: name: helloweb labels: app: helloweb spec: selector: app: helloweb ports: - port: 3000 name: http EOF Finally, create the Virtual service for the Hello web, so that we can expose it through the gateway. Don\u0026rsquo;t forget to deploy the gateway as well - check blog 3 for the snippet.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloweb spec: hosts: - \u0026#39;*\u0026#39; gateways: - gateway http: - route: - destination: host: helloweb.default.svc.cluster.local port: number: 3000 EOF If you open http://$GATEWAY you should see the familiar Hello web with the response from the greeter service.\nLet\u0026rsquo;s use an authorization policy that denies access to all workloads:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: deny-all namespace: default spec: {} EOF With this config, we are denying access to all workloads in the default namespace.\nTry refreshing the http://$GATEWAY or running curl http://$GATEWAY. This time, it won\u0026rsquo;t work and you will see the following error:\nRBAC: access denied To allow Hello Web service to call to the Greeter service we can update the authorization policy that explicitly allows Hello Web making requests to the Greeter service.\nLet\u0026rsquo;s delete the previous policy first by running:\nkubectl delete authorizationpolicy deny-all Now we can create a new policy:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: greeter-service namespace: default spec: selector: matchLabels: app: greeter-service rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] EOF If you try to reaccess the site, you should be able to see the responses again. Note that there might be some delays due to caching.\nLet\u0026rsquo;s tighten up the service role a bit more and update the authorization policy, so we can only call the /hello endpoint on it:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: greeter-service namespace: default spec: selector: matchLabels: app: greeter-service rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] paths: [\u0026#34;*/hello\u0026#34;] EOF To test this, we will get a shell inside the Hello web container and use curl to make requests to the greeter service. First, let\u0026rsquo;s figure out the Hello web pod name by running kubectl get pod and then run the exec command:\nkubectl exec -it [podname] /bin/sh With the shell inside the container, let\u0026rsquo;s install curl first:\napk add curl We can test out the service role now. If you run curl against the /hello endpoint, everything works as expected:\ncurl greeter-service.default.svc.cluster.local:3000/hello {\u0026#34;message\u0026#34;:\u0026#34;hello 👋 \u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;1.0.0\u0026#34;} However, if you make a request against the /version endpoint, you will see the familiar error message:\ncurl greeter-service.default.svc.cluster.local:3000/version RBAC: access denied To clean everything, simply delete the authorization policy.\nConclusion In this blog, you learned about how Istio service mesh deals with the security and different features you can use to define authorization policies through a couple of examples.\n"},{"url":"https://istio.tetratelabs.io/blog/testing-services-with-istio/","title":"How to use Istio to test service resiliency?","description":"In this blog, we will talk about how to use Istio service mesh features that can help you to tests the resiliency of the services. ","content":"In this blog, we will talk about how to use Istio service mesh features that can help you to test the resiliency of the services.\nMaking Services Fail In the name of testing the service resiliency, you need a way to make your services fail and behave the way you want them to behave. Having the ability to make services fail is a great way to test them, discover how they behave in certain conditions and find any potential issues.\nI\u0026rsquo;ve been writing all kinds of tests for a long time, and I have run into problems trying to make a service/component/application fail or behave in a certain way. Yes, there are mocks and fakes and other tools and practices available to help you with that, but it can still get complicated, especially if there are other dependencies in play. For example, if I want to make service A respond with a specific error, then I also need to make sure the database read fails and the call to service B fails, etc. If you\u0026rsquo;ve never run into this - consider yourself lucky.\nWith Istio, you can inject certain types of failures into the requests. Faults that get defined in the VirtualService are injected, while the HTTP request is being forwarded to the destination route. Two different types of faults that can be injected. HTTP delays inject a user-specified delay before forwarding the request to the destination route. This type of failure is used to simulate different network issues, such as service taking too long to respond due to load. The second service resiliency testing feature is the ability to inject HTTP aborts. This allows you to specify a specific HTTP code that you return to calling service. Using the aborts, you can simulate a broken service or a service returning different responses. For example, you can test how the calling service behaves if the underlying call returns an HTTP 404 or HTTP 500 code.\nInjecting HTTP Delays Let\u0026rsquo;s start with an example of an HTTP delay. In the VirtualService below, we are injecting a two seconds delay for 50% of all requests that are being sent to the greeter service:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 fault: delay: percent: 50 fixedDelay: 2s EOF Once you deploy this, you can start making calls to the Hello web and observe the dashboards in Grafana. To quickly generate some traffic, you can use the snippet below - this command runs curl every 1 second:\nwhile true; do sleep 1; curl -H \u0026#34;Host: helloweb.dev\u0026#34; http://$GATEWAY;done Observing Delays in Grafana With the above command running, open the Istio Service Dashboard in Grafana, and from the Service drop-down, select greeter-service.default.svc.cluster.local as this is the service we injected delays in. You should already see the graphs getting populated with data. Here\u0026rsquo;s how the Istio Service Dashboard looks like after a couple of minutes:\nYou can also observe the injected delay from other graphs listed below.\nClient Request Duration\nThis graph shows the client request duration in p50, p90 and p99. Looking at the p99 it\u0026rsquo;s telling us that 99% of the calls took less than ~2.4 seconds and 1% of calls took longer than 2.4 seconds. Similarly, if you look at the p50, you can see that at certain times 50% of the calls took less than ~1.3 seconds and the other 50% of the calls took longer.\nIncoming Request Duration by Source\nJust like the Client Request Duration graph, you can see the latency coming from the upstream service (helloweb). If there were multiple versions of the upstream service, it would be very clear from this graph which version is slower and which one is faster.\nOther graphs on this page are telling you that there were no failures - the client success rate graph is at 100%. Similarly, the Incoming requests by source and response code graph only show HTTP 200 responses.\nIf you switch the service selection to the helloweb.default.svc.cluster.local from the service drop-down on the dashboard, you should see similar graphs, and it is reasonably evident that there is a two-second delay happening.\nObserving Delays in Jaeger How about traces in Jaeger? Let\u0026rsquo;s open the Jaeger dashboard and do a quick search for all traces that have a minimum duration of 2 seconds:\n Open the Jaeger dashboard using the istioctl dashboard command:  istioctl dashboard jaeger  From the Service drop-down, select the helloweb.default service Type 2s in the Min Duration text box Click the Find Traces button  Depending on how long you\u0026rsquo;ve been making requests, you should see a bunch of traces (note that these are only the traces that took at least 2 seconds). Let\u0026rsquo;s click on one of them and take a closer look.\nIt\u0026rsquo;s very clear from these traces how long the call took - the total request duration from helloweb to greeter-service took 2.03 seconds. The greeter-service took only 5.9 ms to respond.\nDiscovering issues The combination of Grafana dashboards, Jaeger traces, and HTTP delays is a powerful tool that can help you discover potential issues with your services. Let\u0026rsquo;s show an example of how you could find a potential problem.\nWe are going to cheat a bit though - in the implementation of the Hello web we set the timeout when making calls to the downstream service to 5 seconds. So, if we\u0026rsquo;d inject an HTTP delay of 6 seconds we could see how the timeouts are manifested in the graphs. Start by deploying the following VirtualService:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 fault: delay: percent: 50 fixedDelay: 6s EOF If you are following along and looking at the Grafana dashboard (you can the dashboard with istioctl dash grafana), you\u0026rsquo;ll quickly see that success rate starts to drop and the client request duration starts increasing as shown in the figure below:\nLikewise, you can see that in addition to the HTTP 200 responses, the incoming requests graph shows a bunch of HTTP 500 responses as well. This is a clear indication that something is going wrong. Let\u0026rsquo;s peek at the Jaeger dashboard and look at all traces for the helloweb service.\nNow you can see errors in some of the traces (notice the 1 Error label on the left side). Another interesting thing to note is the duration - since you set a fixed delay to 6 seconds, one would expect to see duration around that number. However, notice how the duration is only slightly above 5 seconds, but not even close to 6 seconds. What that means is that request was failing even before the 6-second delay passed, hence the error in the trace. Remember the \u0026ldquo;cheat\u0026rdquo; we mentioned - we have a 5-second timeout set; since service is taking 6 seconds to respond (due to the injected delay), the whole request times-out before that.\nInjecting HTTP Aborts As mentioned earlier, you can also randomly inject HTTP aborts into your services, to help you tease out any issues. The syntax for injecting aborts is very similar to the one you\u0026rsquo;ve seen earleier. Instead of using the delay key, you are going to use abort key and define the HTTP status you want to return from the service:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 fault: abort: percentage: value: 50 httpStatus: 404 EOF The above snippet applied to the greeter-service is going to make the service return an HTTP 404, for 50% of all the incoming requests. The effect of the injected aborts can be observed in the same graphs as the delays. Here\u0026rsquo;s how the Incoming Requests by Source and Response Code graph looks like:\nIt is clear from the above graph how we went from having some HTTP 500 responses (because of the timeout that was injected earlier) to having zero HTTP 500 response. On the right side of the graph, you can see an increasing number of HTTP 404 responses (yellow line), which is due to the abort we injected.\nLooking at Jaeger, you can search by setting the tag text box value to http.status_code=404. This returns all traces that contain the HTTP 404 status. Note how these traces only contain helloweb and no traces from the greeter-service - this is expected because the requests never reach the greeter-service as the sidecar proxy intercepts the call and returns an HTTP 404.\nAdvanced Scenarios Now that you understand how basic fault injection works, you can get fancier and start combining both HTTP delays and HTTP faults. You could easily deploy something like this:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 fault: abort: percentage: value: 10 httpStatus: 404 delay: percentage: value: 10 fixedDelay: 6s EOF With the above VirtualService, you are injecting HTTP 404 aborts for 10% of the incoming requests in addition to a 6-second delay for 10% of the incoming requests. If you go even further and combine what you\u0026rsquo;ve learned in the traffic management blog, you could write a VirtualService that looks like this:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - fault: abort: percentage: value: 10 httpStatus: 404 match: - headers: user-agent: regex: \u0026#39;.*Firefox.*\u0026#39; route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 EOF This virtual service injects HTTP 404 responses for 10% of the incoming requests that are coming from the Firefox browser. Similarly, you could insert a delay if you wanted to, or get even more advanced with using different service versions and injecting failures to only specific versions of the service.\nAs you see, there\u0026rsquo;s a lot of possibilities here. For example, you could write your chaos monkey tool that runs inside your cluster and randomly injects faults and delays in your services. Doing this is a great way to make sure your services won\u0026rsquo;t fall over and are resilient as well as ensuring you have all monitoring and observability in place that will allow you to notice these things once you\u0026rsquo;re in production.\nConclusion This blog demonstrated how you could use Istio service mesh features to test how resilient your services are. Additionally, in combination with Grafana and Jaeger, you can quickly spot when or if something goes wrong with your services.\n"},{"url":"https://istio.tetratelabs.io/blog/istio-resiliency-patterns/","title":"What are the patterns of resilience in Istio?","description":"This blog talks about Istio service mesh features that can help you with service resiliency. ","content":"This blog talks about Istio service mesh features that can help you with service resiliency.\nWhat is Resiliency? The first thing you are going to notice once you start breaking down your monolithic application to smaller, more manageable chunks (microservices) or if you are planning to start implementing microservices architecture from scratch, is that you will have more things to manage and think about. Previously, you had a single code base, one deployment. With the move to microservices, you will be doing multiple deployments, and your code may be in numerous repositories.\nIn addition to all this, all these different components you ended up with have to work together. Not only that but you also probably have multiple instances of each component running.\nFrom the deployment standpoint, you are covered to an extent if you\u0026rsquo;re using the Kubernetes platform. Kubernetes ensures services are evenly distributed across virtual machines; it does liveness and readiness checks, and even restarts the failing services for you.\n Liveness vs. readiness probes?\nLiveness and readiness probes get defined on containers running inside Kubernetes. Liveness probe tells Kubernetes when to restart the container. Similarly, the readiness probe tells Kubernetes when your container is ready to accept traffic.\n However, this alone is not enough. We need to know when something goes wrong with the services - we need to make the services resilient and observable.\nResiliency is the ability to provide and maintain an acceptable level of service in the face of faults and challenges to regular operation. It\u0026rsquo;s not about avoiding failures. It\u0026rsquo;s responding to them in such a way that there\u0026rsquo;s no downtime or data loss. The goal for resiliency is to return the service to a fully functioning state after a failure occurs.\nThere are two aspects to the service resiliency:\n  High availability: service is running in a healthy state, without significant downtime. It is responsive and it\u0026rsquo;s meeting its SLAs (Service Level Agreements)\n  Disaster recovery: it\u0026rsquo;s all about how to recover from rare, but significant incidents. It involves data backup and archiving.\n  We could also say that disaster recovery starts when the high availability design can\u0026rsquo;t handle the impact of faults anymore.\nHow to Achieve Resiliency? There are a couple of points you need to keep in mind while developing your services to get to more resilient services.\nAs a first point, you need to understand the requirements and define what service availability means for your services. You should ask yourself questions such as:\n How much downtime can I handle? How much downtime is acceptable? What does it mean for a service to be available?  In general, more downtime usually means you are losing money - either because your customers can\u0026rsquo;t interact with the services or because you are breaking the SLA. Another point to think about it here is the graceful degradation of the service - even if service is not working 100%, can you still provide a good enough experience? For example: if Amazon\u0026rsquo;s recommendation service goes down, you might still see the list of recommendations - they are probably caching the recommendations, and if the service goes down, they can again use the cached values. As an end-user, you probably don\u0026rsquo;t even notice when a service like that is not working 100%.\nNext, you can pinpoint the failure points in your system and think about what can go wrong and how can something go wrong. With the failure points determined, you need to think about how are you going to detect the failures and recover from failures. Ask yourself questions such as:\n How will service detect the failure? How will service respond to failure? How are we going to log and monitor for this failure?  For example: if service A is unavailable, we will return a 5xx status code.\nWith the above defined and implemented, you need a way to simulate different failure conditions to ensure you can detect and recover from failure - we will talk more about how to use Istio service mesh when testing for service resiliency in the upcoming blog.\nLastly, you need monitoring in place to be able to know what is happening with your services as well as with your test results. This is a crucial component, as otherwise you will be flying blind and don\u0026rsquo;t know what is happening with your services.\nService Resiliency Strategies When thinking about service resiliency, there are a couple of strategies you can utilize. As you are deploying your service to Kubernetes, the platform is ensuring that your service can scale and provides load balancing between the service instances for you. Having more than one instance of your service running and a load balancer that intelligently balances between instances will improve the resiliency. If one of your services goes down or starts crashing, the load balancer will ensure to remove that instance from the pool and won\u0026rsquo;t be sending any traffic to it until it recovers.\nA crucial element in making services available is the use of timeouts and retries when making service requests. The network crucial in microservices architecture, and there is a lot of chatter that happens over it. The network also introduces unpredictable behavior, one being latency. So, how do you know what the appropriate time to wait for a service response is? You probably don\u0026rsquo;t want to wait indefinitely as that will uses resources, might cause other (waiting) services to fail and lead to potential cascading failures. A primary solution to this problem is always to define timeouts in your clients - that way service won\u0026rsquo;t wait indefinitely for responses.\nEven with the timeouts in place, the network could be experiencing transient failures, so it might make sense to retry the requests at least a couple of times. The number of retries and the time between them depends on multiple factors. Instead of blindly retrying requests, you should be using the errors and error codes returned from the service as a guideline that helps you decide if the request should be retried or not. For example, if the error returned from the service is not transient (e.g. item doesn\u0026rsquo;t exist when trying to update it, operation XYZ is not supported, and similar) it does not make sense to retry it as it will never succeed.\nAn appropriate retry count and the interval between retries depend on the type of the request and underlying operation that is being performed. For example, you don\u0026rsquo;t want to use exponential backoff if you know a user is waiting for a response as that would make for bad user experience. The exponential backoff strategy or strategy using incremental intervals might be better suited for background operations, while interactive operations (user interface) should be handled by an immediate or constant interval retries might work better. It is also essential to test and optimize your service retry strategy to come up with the solution that works best.\nAnother important strategy that helps to prevent additional strain to the system and cascading failures is the use of circuit breaker pattern. With the help of a circuit breaker, you can limit the impact of failures on your services. Circuit breaker needs a defined threshold that will make it trip - for example, 10 consecutive failures in 5 seconds, or more than two connections, etc. Once the threshold values are exceeded, the circuit breaker trips and then it removes the failing service instance from the load balancing pool for a predefined amount of time. After the preset amount of time passes, the instances get included in the load balancing pool again.\nIstio Monitoring and Tracing Tools Istio comes with a couple of tools pre-installed that can help you with monitoring your service and tracing requests. In the next pages, we will introduce Grafana and Jaeger - two of the tools installed with Istio. Throughout the rest of the blog and book, we will be using Grafana and Jaeger to monitor the services and try to explain their behavior.\nMetrics with Prometheus and Grafana Grafana is \u0026ldquo;the open platform for beautiful analytics and monitoring\u0026rdquo; as described on their website. Grafana uses different data sources and visualizes it, using appealing graphs, tables, heat maps, etc. It features a powerful query language you can use to created other advanced and customized charts in various ways.\nGrafana instance installed with Istio is pre-configured with dashboards and set up, so it automatically uses Prometheus (running in the cluster as well) as the data source. Prometheus' job is to scrape metrics.\nIn addition to scraping and collecting the metrics from these endpoints, Prometheus acts as a server and a datastore which Grafana uses to visualize the collected metrics.\nBy default Istio does not install any of the services automatically. You have to manually install Prometheus, Grafana, and Jaeger.\nLet\u0026rsquo;s start by installing the Prometheus addon first. Open the folder where you downloaded Istio and run:\n$ kubectl apply -f samples/addons/prometheus.yaml serviceaccount/prometheus created configmap/prometheus created Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole clusterrole.rbac.authorization.k8s.io/prometheus created Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created Wait for the Prometheus Pod to start (kubectl get po -n istio-system to check the status) and then you can open the Prometheus dashboard using this command:\nistioctl dashboard prometheus From the UI, you can execute a Prometheus query as shown in the figure below:\nYou can press CTRL+C in the terminal where you executed the dashboard prometheus command to close the tunnel. Just like you installed Prometheus, you also need to install Grafana with a similar command:\n$ kubectl apply -f samples/addons/grafana.yaml To open the Grafana dashboard use the command below:\nistioctl dashboard grafana The browser will automatically open at the Grafana\u0026rsquo;s home page that looks similar to the figure below.\nIstio Dashboards There are a couple of dashboards that Istio creates automatically on installation. The table below lists all the dashboards with their short descriptions.\n   Dashboard Name Description     Istio Service Dashboard Shows detailed graphs and telemetry for each service running inside the mesh. You can use the drop-down at the top of the dashboard to select the service from the mesh.   Istio Workload Dashboard Shows detailed graphs and telemetry for service workloads.   Istio Mesh Dashboard Shows an overview of the services inside the mesh, their latency, success rate, overall request volume and number of non-5xx, 4xx, 5xx HTTP responses.   Istio Performance Dashboard Shows the performance-related graphs for different Istio components (proxies, gateways, etc.)   Istio Control Plane Dashboard Shows resource usage graphs for Istio control plane.   Istio Wasm Extension Dashboard Shows the Wasm extensions.    Most of our time in this module will be spent in the Service and Workload dashboards to monitor the behavior of our services. Performance and per-component dashboards are more useful when you\u0026rsquo;re running Istio in production and want to monitor and potentially alert on the components, their performance, and resource consumption.\nService Mesh Observability with Kiali Kiali is a project that tries to answer what services are part of your service mesh and how are they connected. Since Istio features a configured using custom resources in Kubernetes, you can quickly see how hard and complicated it is to get an overview of your mesh, to see virtual services and routes, see the service graph, configuration, etc. It also integrates with Grafana and Jaeger, so you can use it as your starting dashboard for everything service mesh-related.\nFrom the folder where you downloaded Istio, run the following command to install Kiali:\n$ kubectl apply -f samples/addons/kiali.yaml To open the Kiali dashboard, run istioctl dashboard kiali) command.\nThe browser will open automatically at the overview page, as shown in figure below.\nThe overview page of Kiali shows you a list of namespaces and several applications running in each namespace. Using the links at the bottom of each namespace box or the sidebar on the left, you can look navigate through different views.\nThe first tab in the sidebar is called Graph that shows you all service within your mesh. Figure 4.4 shows the app graph - you can quickly see how different services are related to each other and how and where the calls are being made. The colors in the chart represent the health of the services. You will notice the lines between the ingress, helloweb and, greeter service are all green, which means that they are healthy. The gray color means no requests are being made to that particular service, while the nodes in red or orange may need attention.\nUnder the display dropdown at the top, you can show or hide things such as virtual services, node names, service nodes, etc. Or turn on traffic animation - this will show how the requests are flowing through your services. Also, if you want to only focus on a specific component, you can double click on a graph node to look at that specific node as shown in figure below.\nIn the right-side panel, you will also notice the incoming and outgoing traffic as well as errors and requests.\nThe Applications, Workloads and Services tabs offer different views over things that are running inside the mesh, so we won\u0026rsquo;t be digging deeper into those features.\nAn Istio specific feature is a tab called Istio Config. In this tab, you can get an overview of all Istio resources that are deployed in your mesh, such as gateways, virtual services, and destination rules. You can also get more details for each of the deployed resources by clicking on them. From the details page, you could see, for example, the traffic split or any potential configuration errors. For example, the figure below shows validation errors on the virtual service, because we forgot to deploy the destination rule, and the subsets are not recognized. If your traffic routes don\u0026rsquo;t work, this is an excellent place to start and make sure your config is valid.\nIf you select Services and then click on a specific service, you will get an overview of that service. Additionally, you can use the Kiali UI to create matching or weighted routes, suspend traffic, or delete all traffic rules as shown in the figure below. Instead of figuring it out different options in the YAML file, you can pick and choose the conditions and apply them to desired routes.\nFrom the single service view, you can also use the Inbound Metrics and Traffic tabs to look through the metrics and traffic for that specific service. Each of those tabs also has a link to Grafana and Jaeger where you can further drill down into metrics and traces.\nDistributed Tracing with Jaeger Istio also installs Jaeger. Jaeger is an open-source solution that offers end-to-end distributed tracing. We can easily monitor distributed transactions that are happening in our service mesh and discover any performance or latency issues. For our service to participate in this tracing, there are a couple of things we need to do.\nLet\u0026rsquo;s start by installing Jaeger:\nkubectl apply -f samples/addons/jaeger.yaml Once Jaeger is install, you can open the dashboard:\nistioctl dashboard jaeger Your default browser will open the homepage for Jaeger that looks like one in the figure below:\nFrom the left bar on the homepage, you can pick a couple of things to filter down the traces, and once you get the list of traces, you can drill down into each specific trace to get more details of each trace. The figure below shows the detailed view of one of the traces.\nEach trace in Jaeger has a set of spans that correspond to services that were invoked when a request has been made. To trace details, Jaeger also shows how long each span took - for example, in the above figure, the call to the greeter service took 17.05ms.\nYou will also notice a bunch of other Istio related calls in the trace.\nTo make our services traceable, we need to propagate HTTP headers each time we\u0026rsquo;re doing calls to another service. In the Hello web service, we are making calls to the greeter service, so we need to ensure the following headers are retrieved from the incoming request and added to the outgoing request:\nx-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags x-ot-span-context b3 Here\u0026rsquo;s the code snippet from the Hello web source code that deals with these headers:\nfunction getFowardHeaders(req) { var headers = {}; for (var i = 0; i \u0026lt; 7; i++) { var traceHeader = traceHeaders[i], value = req.get(traceHeader); if (value) { headers[traceHeader] = value; } } const useragent = req.get(\u0026#39;user-agent\u0026#39;); headers[\u0026#39;user-agent\u0026#39;] = useragent; return headers; } const traceHeaders = [ \u0026#39;x-request-id\u0026#39;, \u0026#39;x-b3-traceid\u0026#39;, \u0026#39;x-b3-spanid\u0026#39;, \u0026#39;x-b3-parentspanid\u0026#39;, \u0026#39;x-b3-sampled\u0026#39;, \u0026#39;x-b3-flags\u0026#39;, \u0026#39;x-ot-span-context\u0026#39;, ]; The getForwardHeaders function is called whenever we are making requests to the downstream (greeter service) service:\nconst headers = getFowardHeaders(req); ... axios.get(serviceUrl, { headers: headers, timeout: 5000 }) ... Istio Resiliency Features Istio gives you the ability to automatically set up timeouts, retries and circuit breakers, without touching or modifying your source code. This is a great feature you could use and ensure all your services are consistent.\nRequest Timeouts Timeouts in Istio can be set in the virtual service resource using the timeout key. We are going to use a v3 version of the greeter service - in this version of the service we have two endpoints: /hello and /version. The difference in this version is that the call to the /version endpoint takes an extra 2 seconds to return (we have a 2-second artificial delay in there).\nTo demonstrate the timeouts (and later retries), we will set up the timeout on the v3 greeter service to 0.5 seconds - that way, the request will timeout because of the 2-second delay we have hard-coded.\nLet\u0026rsquo;s deploy the v3 version of the greeter service first:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: greeter-service-v3 labels: app: greeter-service version: v3 spec: replicas: 3 selector: matchLabels: app: greeter-service version: v3 template: metadata: labels: app: greeter-service version: v3 spec: containers: - name: svc image: learnistio/greeter-service:3.0.0 imagePullPolicy: Always ports: - containerPort: 3000 EOF And update the destination rule to include the v3 subset, so the traffic can reach that version of the service:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: greeter-service spec: host: greeter-service.default.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 EOF Finally, let\u0026rsquo;s update the virtual service and add a 0.5-second timeout for the greeter service v3:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v3 timeout: 0.5s EOF Another small thing we need to do is update how we are calling the hello web. Note that the reason we are doing this is to be able to demonstrate different features and scenarios, without complicating or using different services. We are going to add a query ?separateRequest=true to the helloweb request we are making. This will force the service to make two separate calls to the downstream greeter service - one to /hello endpoint and another one to /version endpoint (by default, helloweb makes one request to the /hello endpoint and extracts the greeting and version from there).\nLet\u0026rsquo;s start making continuous calls to the helloweb like this:\nwhile true; do sleep 1; curl -H \u0026#34;Host: helloweb.dev\u0026#34; http://$GATEWAY?separateRequest=true;done Next, open the Grafana to see what\u0026rsquo;s happening. Open the Istio Service dashboard and in the Incoming Requests by Source and Response Code graph, you will start seeing 504 response codes as shown in figure below - this is the gateway timeout response code.\nAnother thing you can notice in the above graph is that the success rate is around 50%. This is because helloweb is making two requests each time - one is failing with HTTP 504 and the other one (/hello) is succeeding. You can read a similar thing from the client success rate graph that\u0026rsquo;s at ~50%.\nOpen the Istio Workload Dashboard and select helloweb from the Workload dropdown. If you scroll down to the Outbound Services section you\u0026rsquo;ll notice the Outgoing Requests by Destination And Response Code graphs that shows HTTP 504 and HTTP 200 responses for the greeter services. Similarly, the Outgoing Success Rate graph will show a 50% success rate as shown in figure below.\nWith all this data we know that something is wrong, but we can\u0026rsquo;t tell what\u0026rsquo;s happening. Let\u0026rsquo;s dig deeper into Jaeger and search for the recent traces. Open Jaeger with istioctl dash jaeger. Select helloweb.default from the service dropdown and click Find Traces. You will notice that all traces have errors, as shown in figure below.\nLet\u0026rsquo;s click on one of those traces that has an error and see if we can get more information on what\u0026rsquo;s going on. You can see two calls are being made to the greeter service - one has an error, and the other one doesn\u0026rsquo;t. If you click on the greeter service that has an error and expand the tags, you can see the URL that\u0026rsquo;s being called - http://greeter-service:3000/version. Another thing to note is the duration - it\u0026rsquo;s always around 500 milliseconds, and this matches the timeout we set on the service.\nRequest Retries The second option we have that falls under the service resiliency is setting the request retries. We can configure the number of attempts and timeout per attempt we want to set when making calls to the service. To demonstrate retries, we will change the timeout to 2 seconds, and add three retries with a 0.5-second interval between each retry. This way we will be able to see the three retries being made that will eventually fail anyway (due to the 2-second timeout).\nLet\u0026rsquo;s deploy the modified virtual service first:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v3 timeout: 2s retries: attempts: 3 perTryTimeout: 0.5s EOF The effect of these changes is best observed in Jaeger - assuming you generated some traffic to the service, open the Jaeger dashboard, do a search for the last traces, and open the trace details. You should see something similar, as shown in the figure below.\nFrom the trace, it\u0026rsquo;s evident that three requests were being made to the /version endpoint - you can expand the span and check the tags to see the endpoint. Jaeger also gives you the duration details, and it shows that each request was retried at a 0.5-second interval. The other thing you can find out from the trace is the timeout itself - if you look at the first helloweb span, it shows that the duration was 2 seconds - this is the same as the timeout you have set.\nCircuit Breakers The last concept we will discuss is circuit breaking. Circuit breaking is an important pattern that can help with service resiliency. This pattern is used to prevent additional failures by controlling and managing access to the failing services.\nThe easiest way to explain it is with a simple example. Let\u0026rsquo;s say our greeter service starts failing and instead of call it continuously, we could detect the failures and either stop or reduce the number of calls being made to the service. If we added a database to this example, you could quickly imagine how calling the service could put more stress on different parts of the system and potentially make everything even worse. This is where a circuit breaker comes into play. We define the conditions when we want the circuit breaker to trip (for example, if we get more than 10 failures within a 5 second period), once circuit breaker trips, we won\u0026rsquo;t be making calls to the underlying service anymore, instead, we will just directly return the error from the circuit breaker. This way, we are preventing additional strain and damage to the system.\nIn Istio, circuit breakers get defined in the destination rule. Circuit breaker tracks the status of each host, and if any of those hosts start to fail, it will eject it from the pool. Practically speaking, if we have five instances of our pod running, the circuit breaker will eject any of the instances that misbehave, so that the calls won\u0026rsquo;t be made to those hosts anymore. Outlier detection can be configured by setting the following properties:\n number of consecutive errors scan interval base ejection time  In addition to the outliers, we can also set the connection pool properties - such as the number of connections and requests per connection being made to the service.\nLet\u0026rsquo;s look at an example for the greeter service:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: greeter-service spec: host: greeter-service.default.svc.cluster.local trafficPolicy: connectionPool: http: http2MaxRequests: 10 maxRequestsPerConnection: 5 outlierDetection: consecutiveErrors: 3 interval: 10s baseEjectionTime: 10m maxEjectionPercent: 10 subsets: - labels: version: v1 name: v1 - labels: version: v2 name: v2 The above rule sets the connection pool size to a maximum of 10 concurrent HTTP requests that have no more than five requests per connection to the greeter service. With the outlier detection properties, we are scanning the hosts every 10 seconds (default value), and if any of the hosts fails three consecutive times (default value is 5) with the 5xx error, we will remove the 10% of the failing hosts for 10 minutes.\nLet\u0026rsquo;s deploy the destination rule that configures a simple circuit breaker:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: greeter-service spec: host: greeter-service trafficPolicy: connectionPool: http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: consecutiveErrors: 1 interval: 1s baseEjectionTime: 2m maxEjectionPercent: 100 EOF Let\u0026rsquo;s also update the VirtualService and to remove the subsets and just route the traffic to the greeter service destination (no subsets):\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service.default.svc.cluster.local http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 EOF To demonstrate the circuit breaking, we will use the load-testing library called Fortio. With Fortio we can easily control the number of connection, concurrency, and delays of the outgoing HTTP calls. Let\u0026rsquo;s deploy Fortio:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: fortio-deploy spec: replicas: 1 selector: matchLabels: app: fortio template: metadata: labels: app: fortio spec: containers: - name: fortio image: istio/fortio:latest_release imagePullPolicy: Always ports: - containerPort: 8080 name: http-fortio - containerPort: 8079 name: grpc-ping EOF To make a simple call from the Fortio pod to the greeter service, run the following commands:\nexport FORTIO_POD=$(kubectl get pod | grep fortio | awk \u0026#39;{ print $1 }\u0026#39;) kubectl exec -it $FORTIO_POD -c fortio -- fortio load -curl http://greeter-service:3000/hello HTTP/1.1 200 OK x-powered-by: Express content-type: application/json; charset=utf-8 content-length: 43 etag: W/\u0026#34;2b-DdO+hdtaORahq7JZ8niOkjoR0XQ\u0026#34; date: Fri, 04 Jan 2019 00:53:19 GMT x-envoy-upstream-service-time: 7 server: envoy {\u0026#34;message\u0026#34;:\u0026#34;hello 👋 \u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;1.0.0\u0026#34;} With the above command, we are just making one call to the greeter service, and it all works, we get the response back. Let\u0026rsquo;s try to trip the circuit breaker now. To make the circuit breaker trip faster, we will decrease the number of replicas in our greeter service deployments from 3 to 1.\nkubectl scale deploy greeter-service-{v1,v2,v3} --replicas=1 Now we can use Fortio and make 20 requests with 2 concurrent connections:\nkubectl exec -it $FORTIO_POD -c fortio -- fortio load -c 2 -qps 0 -n 20 -loglevel Warning http://greeter-service:3000/hello In the output, you will notice something similar to this:\nCode 200 : 11 (55.0 %) Code 503 : 9 (45.0 %) This is telling us that 11 requests succeeded and 45% of them failed. Let\u0026rsquo;s increase the number of concurrent connections to 3:\nkubectl exec -it $FORTIO_POD -c fortio -- fortio load -c 3 -qps 0 -n 50 -loglevel Warning http://greeter-service:3000/hello Now are getting more failures:\nCode 200 : 19 (38.0 %) Code 503 : 31 (62.0 %) This is telling us that 38% of requests succeded, and the rest was caught by the circuit breaker. Another way to see the circuit breaking stats is in Prometheus. Run istioctl dash prometheus and execute the following query:\nsum(istio_requests_total{destination_app=\u0026#34;greeter-service\u0026#34;}) by (response_code, response_flags) The results include all requests made to the greeter-service and they are grouped by the response code and response flags. The UO response flag next to the 503 responses stands for upstream overflow or circuit breaking.\nConclusion In this blog, you have learned about service resiliency and different strategies and patterns you can use to make your services more reliable. We talked a bit about Grafana and Jaeger and how you can use the graphs and traces to observe your services and see if and when something is going wrong. We have also explained different resiliency concepts in Istio (timeouts and retries) you can use to make your service more resilient. Finally, we demonstrated how to create a simple circuit breaker to prevent unnecessary strain on the system in case of a failing service.\n"},{"url":"https://istio.tetratelabs.io/blog/istio-traffic-management-walkthrough-p2/","title":"Hands-on walkthrough of traffic management in Istio Part 2","description":"This blog is the second part of Istio&#39;s hands-on traffic management practice. ","content":"This blog is the second part of Istio\u0026rsquo;s hands-on traffic management practice. See the Hands-on walkthrough of traffic management in Istio Part 1. I will show you how to do more advanced with match conditions on request parameters (e.g. URL, headers). Finally, I will talk about mirroring production traffic to a newly deployed service without impacting end-users.\nAdvanced Traffic Splitting Sometimes splitting requests by weight might not be enough, and luckily Istio supports doing more advanced request matching. You can match the requests based on the URL, headers, or method before you decide where the requests will get routed.\nFor example, you are splitting traffic between two versions, but in addition to that, you only want traffic from the Firefox browsers to go to v2, while requests from other browsers go to v2 version. Or, you can route all GET requests to one version, and route other requests to another version. If you combine this functionality with the ability to rewrite the URLs or do HTTP redirects, you can cover a lot of different scenarios.\nIn this section, we will show examples of how to match requests with Istio and how to redirect and rewrite them. All this gets defined as part of the VirtualService resource.\nRequest Matching Istio allows you to use certain parts of the incoming request and match them to the values you define. If the value matches, the request gets routed to the destination specified in the virtual service. The tables below show all request properties that can get matched to user-provided values and the different ways they can get compared:\n   Property Description     uri Match the request URI to specified value   schema Match the request schema (HTTP, HTTPS, \u0026hellip;)   method Match the request method (GET, POST, \u0026hellip;)   authority Match the request authority header   headers Match the request headers. Headers need to be provided in lower-case and separated by hyphens (e.g. x-my-request-id). Note: if headers get used for matching, other properties (uri, schema, method, authority) will be ignored       Match type Description     exact Property needs to be an exact match to the provided value. For example: exact: Hello will only match if the property value is Hello - it\u0026rsquo;s not going to match if the value is HELLO or hello)   prefix Only prefix of the property will get matched. For example: prefix: Hello will match if the value is HelloWorld, Hello.   regex Value will get matched based on the regex. For example: regex: '.*Firefox.*' will match if the value is Hello Firefox World.    In addition to the matching properties, one can also define sourceLabels to further constrain the rules to services with specified labels only (e.g. we could specify app: myapp to only apply the matching to requests coming from services that have those labels specified)\nLet\u0026rsquo;s see how the matching works in practice. We will be using the same Hello Web and both versions of the Greeter service.\nAs a first example, we are deploying an updated virtual service, that routes all requests coming from the Firefox browser to the v2 version of greeter service.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service.default.svc.cluster.local http: - match: - headers: user-agent: regex: \u0026#39;.*Firefox.*\u0026#39; route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v2 - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v1 EOF The above YAML defines two routes, but only the first one starts with a match condition. This condition does a check against the incoming request and tries to match the User-Agent header against a defined Regex .*Firefox.*.\n This is straightforward matching for the browsers' user agent. We look for the Firefox string in the header. However, there is a variety of other things we could be matching on.\n If the regex gets matched, the requests get routed to the v2 subset. If you remember from earlier, this subset defines the version: v2 label which corresponds to the v2 version of the greeter service. If we can\u0026rsquo;t match the incoming request, we end up routing to the second route, which is the v1 version of the service.\nTo try this out, you can open the gateway URL in two different browsers - Firefox and another one. In the non-Firefox browser and you\u0026rsquo;ll see only v1 responses, and from the Firefox browser, the responses will be coming from the v2 version of the service.\nIn case you don\u0026rsquo;t have the Firefox browser installed, you can replace Firefox with something else, use same ModHeader extension we mentioned earlier and add another header called User-Agent with the value Firefox. Alternatively, you can also use curl:\n$ curl -A \u0026#34;Firefox\u0026#34; -H \u0026#34;Host: helloweb.dev\u0026#34; $GATEWAY \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;css/style.css\u0026#34; /\u0026gt; \u0026lt;pre\u0026gt;frontend: 1.0.0\u0026lt;/pre\u0026gt; \u0026lt;pre\u0026gt;service: 2.0.0\u0026lt;/pre\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;🔥🔥 !!HELLO!! 🔥🔥\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; Just like you matched the User-Agent header, you could match any other headers as well. For example, you could provide an opt-in mechanism for your users that would give them access to new, unreleased features. As part of this, you\u0026rsquo;d also be setting a special header to all requests (e.g. x-beta-optin: true) and then do request routing based on that header value like this:\n... http: - match: - headers: x-beta-optin: exact: \u0026#39;true\u0026#39; route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: beta-version ... Redirects and Rewrites Matching on headers can be useful, but sometimes you might need to match the requests by the values in the request URI.\nAt the moment, greeter service uses the /hello URI to return the message, but we would like to change that be /greeting instead. That way, any service or users requesting /greeting endpoint would get the same response as if they are making requests to /hello endpoint.\nOne way we could do this is to create a second endpoint in the greeter service source code and then maintain both endpoints. However, that doesn\u0026rsquo;t sound too practical, especially since there\u0026rsquo;s a more natural way to do this using a service mesh.\nDepending on what you want to do - either an HTTP rewrite or an HTTP redirect, Istio has you covered. Here\u0026rsquo;s an example of how to do an HTTP re-write and rewrite all requests that have /greeting in the URI to go to the /hello endpoint on the v2 version and any other requests go to the v1 subset:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service.default.svc.cluster.local http: - match: - uri: prefix: /greeting rewrite: uri: /hello route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v2 - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v1 EOF Don\u0026rsquo;t forget to include the other route destination at the end, so any request that doesn\u0026rsquo;t match the /greeting will be routed correctly as well. If you forget to add that last route, Istio won\u0026rsquo;t know where to route the traffic if the match condition evaluates to false.\nTry deploying the above YAML and then open the $GATEWAY/greeting URL. When the request gets matched, the /greeting prefix is detected, the URL gets rewritten to /hello and it gets routed to the defined destination.\nSimilarly, you could use an HTTP redirect as well:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service.default.svc.cluster.local http: - match: - uri: prefix: /greeting redirect: uri: /hello authority: greeter-service.default.svc.cluster.local:3000 - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 EOF Usually, you\u0026rsquo;d use the HTTP redirect if you want to redirect the call to a completely different service. Let\u0026rsquo;s say we had another service called better-greeter-service and we tried to redirect traffic to that service instead. In that case, you can update the authority to better-greeter-service and the traffic will get redirected to that service.\nJust like you\u0026rsquo;ve defined a single match condition for one destination, we could also define multiple match conditions for each route OR add more conditions to a single match. If we build on the previous example, you can add additional match that would check if there\u0026rsquo;s a specific value in the headers:\n... http: - match: - uri: prefix: /greeting headers: x-my-header: exact: something redirect: uri: /hello authority: greeter-service.default.svc.cluster.local:3000 ... In the above case we redirect requests to /hello endpoint if both match conditions are satisfied (AND semantics). Similarly, you could add an additional match entry:\n... http: - match: - uri: prefix: /greeting redirect: uri: /hello authority: greeter-service.default.svc.cluster.local:3000 - match: - uri: headers: x-my-header: exact: something redirect: uri: /blah authority: greeter-service.default.svc.cluster.local:3000 ... Dark Traffic (Mirroring) Most of this blog talked about routing requests and traffic between different versions based on some requirements and in all cases, the end-users would have a different experience, depending on how the requirements get met. However, sometimes we don\u0026rsquo;t want to release the new version and expose users to it, but we\u0026rsquo;d still like to deploy it and observe how the new service works, get telemetry and compare the performance and functionality of the existing service with the new service.\n Difference between deployment and release?\nDeploying a service to production is merely moving the code to live in production, but not sending any production traffic to it. Releasing a service involves taking a deployed service and routing production traffic to it.\n In one of the previous sections, we deployed a v2 service and then we were gradually releasing it by sending a higher percentage of traffic to it. Doing this involves risks as the v2 service might not behave correctly and end-users will be impacted by it.\nThe idea behind traffic mirroring is to minimize the risk of exposing users to potentially buggy service. Instead of releasing the service as per the definition above, we deploy the new service and then mirror the traffic that gets sent to the released version of the service. Service receiving mirrored traffic can then get observed for errors without impacting any production traffic. In addition to running a variety of tests on the deployed version of the service, you can now also use actual production traffic and increase the testing coverage which could give you more confidence and minimize the risk of releasing a buggy service.\nHere\u0026rsquo;s how to turn on the service mirroring:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service.default.svc.cluster.local http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v1 weight: 100 mirror: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v2 mirror_percent: 100 EOF The virtual service above is routing 100% of the traffic to the greeter service v1, while also mirroring 100% of the traffic to the v2 version of the service. You can control the percentage of traffic mirrored to the service by setting the mirror_percent value.\nThe easiest way to see this in action is to watch the logs from the v2 service and then open the gateway URL and reload the page a couple of times. The responses you will see on the web page will be coming from the v1 version of the service; however, you\u0026rsquo;ll also see the request being sent to the v2 version:\n$ kubectl logs greeter-service-v2-78fc64b995-krzf7 -c svc -f \u0026gt; greeter-service@2.0.0 start /app \u0026gt; node server.js Listening on port 3000 GET /hello 200 9.303 ms - 59 GET /hello 200 0.811 ms - 59 GET /hello 200 0.254 ms - 59 GET /hello 200 3.563 ms - 59 ... Sidecar Proxy An Envoy sidecar proxy that gets injected to every Kubernetes deployment is configured in such a way that it accepts traffic on all ports and can reach any service in the mesh when it forwards traffic. In some cases, you might want to change this configuration and configure the proxy, so it can only use specific ports and access certain services. To do that, you can use a sidecar resource in Istio. A sidecar resource can be deployed to one or more namespaces inside the Kubernetes cluster, but there can only be one sidecar resource per namespace if there\u0026rsquo;s not workload selector defined.\nThree parts make up the sidecar resource, a workload selector, an ingress listener, and an egress listener.\nWorkload selector The sidecar resource uses a workload selector to determine which workloads will be affected by the sidecar configuration. You can decide to control all sidecars within a namespace, regardless of the workload, or provide a workload selector to apply the configuration only to specific services.\nJust like other resources in Istio and Kubernetes, a set of labels is used to select the workloads. For example, the snippet below will apply to all proxies that live inside the default namespace:\napiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default-sidecar namespace: default spec: egress: - hosts: - \u0026#34;default/*\u0026#34; - \u0026#34;istio-system/*\u0026#34; - \u0026#34;staging/*\u0026#34; Additionally, with the egress section, you are specifying that the proxies will have access to services running in the default, istio-system and staging namespaces. To only select certain workloads, you can add the workloadSelector key. The snippet below only applies to the workloads that have the label version set to v1:\napiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default-sidecar namespace: default spec: workloadSelector: labels: version: v1 egress: - hosts: - \u0026#34;default/*\u0026#34; - \u0026#34;istio-system/*\u0026#34; - \u0026#34;staging/*\u0026#34; Ingress listener With an ingress listener, you can define which inbound traffic will be accepted. Each ingress listener needs to have a port set - this is the port where the traffic will be received (e.g. 5000 example below), and a default endpoint. The default endpoint can either be a loopback IP endpoint or a Unix domain socket - this is where the traffic will be forwarded to, for example 127.0.0.1:8080. The snippet below shows an example of an ingress listener that listens on port 5000 and forwards traffic to the loopback IP on port 8080 - this is where your service would be listening to.\n... ingress: - port: number: 5000 protocol: HTTP name: somename defaultEndpoint: 127.0.0.1:8080 ... You can also use a field called captureMode to configure how traffic will be captured (or not). By default, the capture mode defined by the environment will be used; you can also use IPTABLES as a valid setting to specify that the traffic will be captured using IPtables redirection. The last option is to use NONE - this means that is no traffic capture.\nWith the bind field, you can specify an IP address or domain socket for incoming traffic. You\u0026rsquo;d set this if you only want to listen on a specific address for the incoming traffic.\nSimilarly, instead of using an IP for the default endpoint, you could forward the traffic to Unix domain socket - e.g. unix://some/path, and have your service listen for connection on that socket. If you are using a Unix domain socket, use 0 as the port number. Also, the capture mode needs to be either set to DEFAULT or NONE.\nEgress listener An egress listener is used to define the properties for outbound traffic on the sidecar proxy. It has similar fields as the ingress listener, with the addition of the hosts field. With the hosts field you can specify service hosts in the namespace/dnsName (e.g. myservice.default or default/*). Services in the hosts field can either be actual services from the registry (all services registered in the mesh) or services defined with a ServiceEntry (external services) or with virtual services.\negress: - port: number: 8080 protocol: HTTP bind: 127.0.0.1 hosts: - \u0026#34;*/my-api.example.com\u0026#34; The snippet above allows your application to communicate with an API that\u0026rsquo;s listening on 127.0.0.1:8080 with the additional service entry, you can then proxy everything from 127.0.0.1:8080 to an external API.\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: external-api-example-com spec: hosts: - my-api.example.com ports: - number: 3000 protocol: HTTP location: MESH_EXTERNAL resolution: DNS Conclusion This blog gave you an overview of request/traffic routing features that Istio provides. You familiarized yourself with the five essential resources in Istio: Gateway, VirtualService, DestinationRule, ServiceEntry, and Sidecar.\nUsing the Hello Web and Greeter service examples, you learned how to set up a gateway and allow traffic to enter the cluster and route it to Hello web. Similarly, you went through an exercise to enable a service to reach outside of the cluster to call an existing public API. You\u0026rsquo;ve learned how to split the traffic based on different conditions and rules as well as redirect and rewrite the URL requests. Finally, we showed how to deploy a service to production and mirror traffic to it to minimize the risk to users.\n"},{"url":"https://istio.tetratelabs.io/blog/istio-traffic-management-walkthrough-p1/","title":"Hands-on walkthrough of traffic management in Istio Part 1","description":"In this blog, we will talk about how to get started with routing traffic between your services using service mesh. ","content":"In this blog, I will talk about how to get started with routing traffic between your services using service mesh. You will learn how to set up an Ingress resource to allow traffic into your cluster as well as an Egress resource to enable traffic to exit your cluster. I will explain how to deploy a new version of the service and run it alongside with the released production version of the service without disrupting production traffic. With both service versions deployed we will gradually release the new version and start routing a percentage of incoming requests to the latest version. With this basic set up in place, we will show how to do more advanced with match conditions on request parameters (e.g. URL, headers). Finally, I will talk about mirroring production traffic to a newly deployed service without impacting end-users.\nDeploying sample services In this blog, you will be using two different services. The first one, called Hello Web, is a simple frontend web application that makes calls to another service, called the Greeter Service. We will be using multiple versions of the Greeter Service to demonstrate the use of traffic routing. The diagram below shows the two services.\nBut, to get started, we will deploy the Hello Web and Greeter Service V1. Let\u0026rsquo;s start with the Kubernetes deployment and service for the Greeter Service V1 and deploy the resources to the default namespace on your Kubernetes cluster.\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: greeter-service-v1 labels: app: greeter-service version: v1 spec: replicas: 3 selector: matchLabels: app: greeter-service version: v1 template: metadata: labels: app: greeter-service version: v1 spec: containers: - image: learnistio/greeter-service:1.0.0 imagePullPolicy: Always name: svc ports: - containerPort: 3000 --- kind: Service apiVersion: v1 metadata: name: greeter-service labels: app: greeter-service spec: selector: app: greeter-service ports: - port: 3000 name: http EOF If all goes well, you will see an output like this:\ndeployment.apps/greeter-service-v1 created service/greeter-service created Similarly, let\u0026rsquo;s create the Kubernetes deployment and service for the Hello Web frontend service. Note that in this deployment we are declaring an environment variable called GREETER_SERVICE_URL. This environment variable tells our web frontend the address of the Greeter Service.\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: helloweb labels: app: helloweb version: v1 spec: replicas: 3 selector: matchLabels: app: helloweb version: v1 template: metadata: labels: app: helloweb version: v1 spec: containers: - image: learnistio/hello-web:1.0.0 imagePullPolicy: Always name: web ports: - containerPort: 3000 env: - name: GREETER_SERVICE_URL value: \u0026#39;http://greeter-service.default.svc.cluster.local:3000\u0026#39; --- kind: Service apiVersion: v1 metadata: name: helloweb labels: app: helloweb spec: selector: app: helloweb ports: - port: 3000 name: http EOF To watch the deployment, you can run kubectl get pods --watch - this shows you the status changes of each pod. You can also just run the kubectl get pods command to shows all pods and their current status:\nNAME READY STATUS RESTARTS AGE greeter-service-v1-c4f8d55cb-2gbrr 2/2 Running 0 4m48s greeter-service-v1-c4f8d55cb-h7ssg 2/2 Running 0 4m48s greeter-service-v1-c4f8d55cb-pjxcc 2/2 Running 0 4m48s helloweb-8567cfc9f8-5ctxm 2/2 Running 0 4m35s helloweb-8567cfc9f8-gcwcc 2/2 Running 0 4m35s helloweb-8567cfc9f8-p9krk 2/2 Running 0 4m35s Notice we have three replicas of each service and each pod has two containers - one is the service container and the second one is the Envoy proxy that was automatically injected.\nAccessing Deployed Services By default, any service running inside the service mesh is not automatically exposed outside of the cluster, which means that we can\u0026rsquo;t get to it from the public Internet. Up until Istio version 1.X.X, services within the mesh didn\u0026rsquo;t have access to anything running outside of the cluster either. However, that has changed with the Istio version used in this book, and you can access any services, APIs, and endpoints outside of your cluster, without explicitly allowing access to them. Note that you should still review all your services and ensure they are only accessing what they need to access and prevent them from accessing any other (unknown) external resources.\nTo allow incoming traffic to reach a service running inside the cluster, you need to create an external load balancer first. As part of the installation, Istio deploys an istio-ingressgateway service that is of type LoadBalancer. Together with this service and an Istio Gateway resource, you can configure access to services running inside the cluster.\nIf you run kubectl get svc istio-ingressgateway -n istio-system, you will get an output similar to this one:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S)... istio-ingressgateway LoadBalancer 10.107.249.46 \u0026lt;pending\u0026gt; 80:31380/TCP... The above output shows the Istio ingress gateway of type LoadBalancer. If you\u0026rsquo;re using a Minikube cluster you will notice how the external IP column shows text \u0026lt;pending\u0026gt; - that is because we don\u0026rsquo;t have a real external load balancer as everything runs locally. With a cluster running in the cloud from any cloud provider, we would see an actual IP address there - that IP address is where the incoming traffic enters the cluster. If you are using Docker for Mac/Windows, you will see localhost under the EXTERNAL-IP column.\nWe will be accessing the service in the cluster frequently, so we need to know which address to use. The address we are going to use depends on where the Kubernetes cluster is running.\nIf using Minikube Minikube has a command called minikube tunnel. This command creates networking routes from your machine into the Kubernetes cluster as well as allocates IPs to services marked with LoadBalancer. What this means is that you can access your exposed service using an external IP address, just like you would when you\u0026rsquo;re running Kubernetes in the cloud.\nTo use the tunnel command, open a new terminal window and run minikube tunnel. You should see an output similar to this one:\n$ minikube tunnel Status: machine: minikube pid: 43606 route: 10.96.0.0/12 -\u0026gt; 192.168.99.104 minikube: Running services: [istio-ingressgateway] errors: minikube: no errors router: no errors loadbalancer emulator: no errors If you run the kubectl get svc istio-ingressgateway -n istio-system command to get the ingress gateway service, you will notice an actual IP address in the EXTERNAL-IP column. It should look something like this:\n$ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) istio-ingressgateway LoadBalancer 10.107.235.182 **10.99.132.130** ....... Now you can use the external IP address (e.g. 10.99.132.130) as the public entry point to your cluster. Set this value to the GATEWAY variable like this:\nexport GATEWAY=[EXTERNAL-IP] If using Docker for Mac/Windows When using Docker for Mac/Windows, the Istio ingress gateway is exposed on http://localhost:80, so you can set the GATEWAY variable like this:\nexport GATEWAY=http://localhost If using hosted Kubernetes If you\u0026rsquo;re using hosted Kubernetes, run the kubectl get svc istio- ingressgateway -n istio-system command and use the external IP value.\nFor the rest of the book, we will be referring to the GATEWAY environment variable in all examples when accessing the services.\nUsing Istio Gateway Now that you have the GATEWAY variable set up, you can try and access it. Unfortunately, the connection will be refused:\n$ curl -v $GATEWAY * Trying 10.99.132.130... * TCP_NODELAY set * Connection failed * connect to 10.99.132.130 port 80 failed: Connection refused * Failed to connect to 10.99.132.130 port 80: Connection refused * Closing connection 0 curl: (7) Failed to connect to 10.99.132.130 port 80: Connection refused You need a Gateway resource for the ingress gateway to know where to route the requests when they hit the cluster. The ingress and the gateway resource operate at the edge of the service mesh and are used to enable incoming traffic to the cluster. Here\u0026rsquo;s how a minimal Istio Gateway resource looks like:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;*\u0026#39; The snippet above creates an Istio Gateway resource with the istio: ingressgateway selector. An istio-ingressgateway pod with the label istio=ingressgateway got created as part of the Istio installation, and the gateway resource with the matching selector istio=ingressgateway adds the routing config to the istio-ingressgateway pod.\nIf you run kubectl get pod --selector=\u0026quot;istio=ingressgateway\u0026quot; --all-namespaces you will get pods that are labeled with ingressgateway. The command returns an ingress gateway pod that\u0026rsquo;s running in the istio-system namespace and is installed as part of the Istio installation. The ingress gateway receives all incoming traffic to the cluster and will ensure the requests get redirected to services in the cluster, based on the settings defined in the Gateway resource.\nUnder servers you define which hosts will this gateway proxy. We are using * which means we want to proxy all requests, regardless of the hostname.\n Using Real Domains\nIn the real world, the host gets set to the actual domain name (e.g. www.example.com) where cluster services get accessed. The * should be only used for testing and in local scenarios and not in production.\n With the host and port combination above, we are allowing incoming HTTP traffic to port 80 for any host (*). Let\u0026rsquo;s deploy our own Gateway resource to the default namespace:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#39;*\u0026#39; EOF Let\u0026rsquo;s try and make a request to the $GATEWAY endpoint now.\n$ curl -v $GATEWAY * Trying 10.99.132.130... * TCP_NODELAY set * Connected to 10.99.132.130 (10.99.132.130) port 80 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: 10.99.132.130 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 404 Not Found \u0026lt; date: Fri, 09 Oct 2020 21:50:08 GMT \u0026lt; server: istio-envoy \u0026lt; content-length: 0 \u0026lt; * Connection #0 to host 10.99.132.130 left intact * Closing connection 0 Notice this time we get back an HTTP 404 response. The request is getting to the ingress gateway, but there\u0026rsquo;s nothing behind it and the gateway doesn\u0026rsquo;t know where or how to route the request.\nLet\u0026rsquo;s also look at the logs from the Istio ingress gateway pod to see the requests coming in:\n$ kubectl get pod --selector=\u0026#34;istio=ingressgateway\u0026#34; --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE istio-system istio-ingressgateway-7bd5586b79-nzbm8 1/1 Running 0 11m $ kubectl logs istio-ingressgateway-7bd5586b79-nzbm8 -n istio-system ... [2020-10-09T21:50:08.971Z] \u0026#34;GET / HTTP/1.1\u0026#34; 404 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 0 0 - \u0026#34;192.168.99.1\u0026#34; \u0026#34;curl/7.64.1\u0026#34; \u0026#34;697452d4-08c9-9370-a3c2-2cbca13b34c5\u0026#34; \u0026#34;10.99.132.130\u0026#34; \u0026#34;-\u0026#34; - - 172.17.0.5:8080 192.168.99.1:58015 - default .... This is telling us that the incoming traffic did reach the ingress gateway. If you think about it, that response makes sense as we only defined the port and hosts with the Gateway resource, but haven\u0026rsquo;t specified an actual destination for the traffic. To do that, we use another Istio resource called virtual service (VirtualService).\nVirtual Services Since we will be talking a lot about services throughout the book, let\u0026rsquo;s make a quick note on the terminology. The word service is overloaded and can mean a bunch of different things, depending on the context. Whenever I mention the word service, I will make sure to use it in a context and say for example a \u0026ldquo;Kubernetes service\u0026rdquo; when I am talking about the Kubernetes Service resource or \u0026ldquo;Virtual service\u0026rdquo; when I talk about the Istio virtual service resource. In addition to this, I will also refer to the sample apps service we will use throughout the book - Hello Web and Greeter Service - either as services or apps.\nIstio\u0026rsquo;s Virtual Service resource is used to configure how the requests get routed within the mesh. Virtual service is one of the resources you will use heavily throughout the book. Let\u0026rsquo;s look at an example of the Istio virtual service resource:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloweb spec: hosts: - \u0026#39;helloweb.dev\u0026#39; gateways: - gateway http: - route: - destination: host: helloweb.default.svc.cluster.local port: number: 3000 The host in the virtual service is the address used by clients when attempting to connect to the service. In our case, you will use the helloweb.dev as the host. Using an actual host instead of an asterisk (e.g. *) will allow us to have multiple services deployed and accessed through the gateway at the same time. A potential downside is that you will have to remember to include a Host: [hostname] header with each curl request you make. Besides, if you want to access services through the browser, you will need to install an extension that allows you to modify the headers. If using Chrome, you can try the ModHeader extension.\nIn addition to the hosts setting, we are also specifying a name of the gateway resource (gateway) you create before, and with this setting, you are allowing traffic to come through the gateway and hit the destination host defined in the virtual service (helloweb.default.svc.cluster.local).\nThe route and destination portion are used to define the host and the port number of the Kubernetes service you are trying to reach. The host in our case is the Kubernetes service DNS name - helloweb.default.svc.cluster.local, just like the hostname.\n The reason for using fully qualified service names is because of the way Istio interprets the short names (e.g. helloweb). Istio expands the short name to helloweb.[namespace].svc.cluster.local. However, the namespace gets replaced with the namespace that contains the virtual service and not with the namespace the helloweb service runs in. Always make sure to use fully qualified names, as to avoid confusion.\n With the help of a virtual service resource, one can also configure multiple destinations, and this is something that\u0026rsquo;s explained later in the book where we talk about traffic splitting.\nLet\u0026rsquo;s go ahead and deploy the virtual service:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloweb spec: hosts: - \u0026#39;helloweb.dev\u0026#39; gateways: - gateway http: - route: - destination: host: helloweb.default.svc.cluster.local port: number: 3000 EOF With the virtual service deploy, you can try the same curl command:\n$ curl -H \u0026#34;Host: helloweb.dev\u0026#34; $GATEWAY \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;css/style.css\u0026#34; /\u0026gt; \u0026lt;pre\u0026gt;frontend: 1.0.0\u0026lt;/pre\u0026gt; \u0026lt;pre\u0026gt;service: 1.0.0\u0026lt;/pre\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;hello 👋 \u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt;  You can also open the $GATEWAY address in your browser for the UI experience.\n The traffic is now flowing through the Istio ingress gateway, the virtual service and finally, it hits the Hello Web frontend, which in turn calls the Greeter service to get the greeting message.\nService Entries Now that we know how to allow traffic inside the cluster let\u0026rsquo;s figure out how can we enable services to make calls outside of the cluster as well.\nTo demonstrate the use of the service entry resource we are going to deploy a service called Movie web. This simple website makes an API call to themoviedb.org to retrieve a list of currently popular movies and displays them.\nBefore you can deploy the Movie web, go to http://themoviedb.org and obtain an API key. Once you get the API key, replace the \u0026lt;API_KEY_HERE\u0026gt; in the YAML file with the actual API key. Now you can deploy the Movie web:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: movieweb labels: app: movieweb version: v1 spec: replicas: 3 selector: matchLabels: app: movieweb version: v1 template: metadata: labels: app: movieweb version: v1 spec: containers: - name: movieweb image: learnistio/movie-web:1.0.0 imagePullPolicy: Always ports: - containerPort: 3000 env: - name: THEMOVIEDB_API_KEY value: \u0026#39;\u0026lt;API_KEY_HERE\u0026gt;\u0026#39; --- kind: Service apiVersion: v1 metadata: name: movieweb labels: app: movieweb spec: selector: app: movieweb ports: - port: 3000 name: http EOF With the Kubernetes service and deployment created, you also need to deploy a virtual service:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: movieweb spec: hosts: - \u0026#39;*\u0026#39; gateways: - gateway http: - route: - destination: host: movieweb.default.svc.cluster.local port: number: 3000 EOF Note that we are setting the hosts to *. This means that we don\u0026rsquo;t need to specify a Host header to access the movieweb application.\nIn the earlier versions of Istio, any requests made to the outside of the cluster were automatically blocked and you would get an error like this:\n$ curl -H \u0026#34;Host: movieweb.dev\u0026#34; $GATEWAY {\u0026#34;error\u0026#34;:\u0026#34;Error accessing: api.themoviedb.org\u0026#34;} This setting has been changed in the recent versions of Istio and traffic going outside of the cluster is now enabled by default. To control the traffic exiting the service mesh, you can use another Istio resource called ServiceEntry. With this resource, you can make any service (external to the mesh or internal services, not part of the service registry) become part of the service registry.\nWhen you use the service entry resource you are essentially making an external service part of the service mesh and \u0026ldquo;pulling\u0026rdquo; it in. Once you have an external service as part of the service mesh, it allows you to apply any other mesh patterns to it as well - for example, you can do traffic splitting, failure injection and more, and it doesn\u0026rsquo;t matter that the services are external. Just like you have a Gateway resource defined for incoming requests, you could also set another gateway resource for all egress traffic (traffic exiting the cluster).\nHere\u0026rsquo;s how a service entry resource looks like for the api.themoviedb.org:\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: movieweb spec: hosts: - api.themoviedb.org ports: - number: 443 name: https protocol: HTTPS resolution: DNS location: MESH_EXTERNAL The main parts that are interesting here are the entries under hosts - this is the actual host we call from the Movie web service. The port number (443) and protocol (https) are self-explanatory and should match the port/protocol used when accessing the external service.\nThe resolution: DNS setting defines how to resolve the IPs. There are three options you can pick from: NONE, STATIC and DNS - we are using the DNS setting because we want the proxy to attempt to resolve the IP address of the host during the requests. You would use the STATIC parameter if you had specified the static IP addresses in the service entry, and NONE if you don\u0026rsquo;t want to do any resolution - this assumes the destination is already resolved and forwards the connection to the destination IP address.\nLastly, with the location set to MESH_EXTERNAL you are saying that this is an external service that is consumed through the APIs. The other possible option for the location setting is MESH_INTERNAL. Typically, this option is used for services that should be part of the mesh, but are perhaps running on different infrastructure and not necessarily in the same cluster.\nLet\u0026rsquo;s deploy the service entry resource for the Movie web and allow the Movie web to access the API:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: movieweb spec: hosts: - api.themoviedb.org ports: - number: 443 name: https protocol: HTTPS resolution: DNS location: MESH_EXTERNAL EOF If you open the http://$GATEWAY in your browser or use curl you will get a web site with a list of movies, together with posters and descriptions, just like in it\u0026rsquo;s shown below.\nLet\u0026rsquo;s delete the MovieWeb resources before you continue:\nkubectl delete deploy movieweb kubectl delete svc movieweb kubectl delete vs movieweb Basic Traffic Splitting Up until now, we were dealing with only one version of the service, and we explained how to allow incoming requests to the cluster, and use a service entry resource to give the mesh service access to external services and APIs. Sometimes having a single frontend that calls one API is enough, however in most cases, one has multiple services running within the cluster and talking to each other. All these services can evolve at their own pace, so you can quickly end up with numerous different versions. How do you go about releasing new service versions and how to ensure and get confidence that the new version will work as good as the existing one?\nThis is where the service mesh can help out. This section will show you how to take a new service version, deploy it, and then gradually release it to production, while at the same time minimizing the risk to end-users.\nTo demonstrate traffic splitting, we will use the Hello Web and two different versions of the Greeter service we introduced at the beginning of this blog.\nIf you open the GATEWAY URL, you should see something similar to the figure below. Hello web is calling the V1 version of the Greeter service and displaying the message it gets back.\nThe V1 version of the Greeter service is excellent and helpful, but we\u0026rsquo;d like to release a v2 of the Greeter service. Because we want to minimize the risks of exposing users to a broken version of the service, we want to do gradual roll-out, while still keeping the existing service up and running. Once we\u0026rsquo;ve deployed the new version, we can start releasing it gradually by routing 10% of all incoming requests to the latest version, while the rest of the requests (90%) still goes to the existing version.\nSplitting Traffic By Weight With Istio, you can route requests by assigning weights to each version of the service - weights translate to the percentage of the incoming requests to route to that version of the service. At the moment, the traffic through the services looks like shown below.\nTraffic comes from the Internet and hits the load balancer and gateway and then goes to the helloweb Kubernetes service. After that helloweb Kubernetes services load balances the traffic between all pods that have the label app: helloweb set. If you look at the deployment YAML for the Hello web, you will notice we have the following labels defined:\nlabels: app: helloweb version: v1 From the Hello web pod, we make a call to the Greeter service using it\u0026rsquo;s DNS name (http://greeter-service.default.svc.cluster.local:3000) - this DNS name gets automatically created when Kubernetes service is deployed. The Kubernetes service will then in turn load balance between all pods labeled with the app: greeter-service. Similarly, as in the Hello web deployment, we have these labels defined in the Greeter service deployment v1:\nlabels: app: greeter-service version: v1 How would we go about deploying a second version of the greeter service?\n  We create an entirely separate Kubernetes deployment that has the label version: v2, in addition to the app: greeter-service label. This deployment also uses a different Docker image with the v2 version of our service, but everything else in the YAML file is identical to the V1 version of the service.\n  We don\u0026rsquo;t need to deploy another Kubernetes service as we already have the Kubernetes greeter-service deployed. We also shouldn\u0026rsquo;t rename the service or deploy a different version of it, because the Hello Web relies on the DNS name (e.g. http://greeter-service:3000) to make calls to it.\n  Notice how the selector is defined in the greeter service Kubernetes service:\nselector: app: greeter-service This means that it doesn\u0026rsquo;t have a clue about the greeter service versions, which is ok. However, if we would deploy the v2 of the greeter service and then reload the hello web, we would randomly get responses back from either v1 or v2 pods. This is shown blow.\nIf you remember when we talked about Istio\u0026rsquo;s VirtualService resource, we briefly mentioned that one could define multiple destinations where requests can be routed to. This is exactly what we need to do. You can define a second destination like this:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 However, this alone is not enough as we are not differentiating between service versions in any way - in both cases, we have the same host. We need to specify that we want only 10% of the requests going to V1 of the service and the rest of the traffic to go to the V2 version of the service.\nFor this purpose, Istio has a resource named DestinationRule. With this rule and the concept of subsets, you can specify how to distinguish between different versions of the service using labels on the Kubernetes pods. Any rules defined in the destination rule are applied to the request after virtual service routing occurs. Since we have two versions of the greeter service, we can create two subsets named v1 and v2 and define the labels that are used to distinguish between them (e.g. version: v1 and version: v2). These labels get then applied to the Kubernetes service selectors. So the Greeter Kubernetes service is generic, and load balances between all pods with the app: greeter-service, but this destination rule and the VirtualService will additionally apply either version: v1 or version: v2 at the time of routing.\nHere\u0026rsquo;s how a destination rule would look like for the greeter service:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: greeter-service spec: host: greeter-service.default.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 With the destination rule in place, you will also need to update the virtual service to use these subsets. Here\u0026rsquo;s a snippet that shows how to define the subsets and weights:\n... - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v1 weight: 90 - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v2 weight: 10 ... We have added the subset to each of the destinations, as well as the weight that routes 10% of the requests to the new version (v2) and 90% of the requests to the existing, v1 version.\nNow that we clarified how this works let\u0026rsquo;s come up with a better plan of deploying the v2 version of the service, without disrupting any existing traffic.\nAs a first step, you need to deploy the destination rule - in this rule, you define the versions by creating a subset for each version, and we specify the traffic policy to use mutual TLS for connections.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: greeter-service spec: host: greeter-service.default.svc.cluster.local subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: tls: mode: ISTIO_MUTUAL EOF Next, you can deploy the updated virtual service that defines how the traffic should be routed - all traffic to the v1 version of the greeter service.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service.default.svc.cluster.local http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v1 weight: 100 - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v2 weight: 0 EOF Remember that during this process, the service is \u0026ldquo;live\u0026rdquo;, and there is no downtime at all. Finally, you can deploy the v2 version of the greeter service.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: greeter-service-v2 labels: app: greeter-service version: v2 spec: replicas: 3 selector: matchLabels: app: greeter-service version: v2 template: metadata: labels: app: greeter-service version: v2 spec: containers: - name: svc image: learnistio/greeter-service:2.0.0 imagePullPolicy: Always ports: - containerPort: 3000 EOF Now you have v2 version deployed in the \u0026ldquo;dark mode\u0026rdquo; - no traffic is getting routed to that version at the moment. If you try and open the gateway URL now (don\u0026rsquo;t forget to set the helloweb.dev Host header), you will get the same response back as earlier - all responses from v1 version only. Here\u0026rsquo;s the output of the kubectl get pods command that shows that there are six greeter service pods deployed and running - three pods with the v1 version and three pods with the v2 version:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE greeter-service-v1-c4f8d55cb-2gbrr 2/2 Running 0 24h greeter-service-v1-c4f8d55cb-h7ssg 2/2 Running 0 24h greeter-service-v1-c4f8d55cb-pjxcc 2/2 Running 0 24h greeter-service-v2-9974dc6-hdcz6 2/2 Running 0 2m42s greeter-service-v2-9974dc6-qtk9q 2/2 Running 0 2m42s greeter-service-v2-9974dc6-w78bl 2/2 Running 0 2m42s helloweb-75cdb96474-c24hw 2/2 Running 0 24h helloweb-75cdb96474-m6jcg 2/2 Running 0 24h helloweb-75cdb96474-w4crk 2/2 Running 0 24h Let\u0026rsquo;s start redirecting 10% of the incoming traffic to the v2 version. To do that, you need to update the weights in the virtual service and re-deploy it.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: greeter-service spec: hosts: - greeter-service.default.svc.cluster.local http: - route: - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v1 weight: 90 - destination: host: greeter-service.default.svc.cluster.local port: number: 3000 subset: v2 weight: 10 EOF If you open the gateway URL now and refresh the page a couple of times, you will eventually see the response from the v2 version of the greeter service as shown in figure below.\nNow that you have the v2 deployed and partially released, the next thing that usually happens is that you monitor and observe your service for any issues and, if everything looks ok, you gradually route more and more traffic to the v2 version, while decreasing the percentage of requests going to the v1 version. This is accomplished by merely updating the weights in virtual service.\nIn case you discover any issues with the v2 version, you can easily roll back the percentage by either re-routing a portion or all traffic back to the v1 version of the service. It\u0026rsquo;s never fun to discover issues; however, with this approach, you are only affecting a smaller portion of the users, instead of everyone. Note that the version is not the only thing to use for request routing. In the next section, we will talk about how to match the requests in a more detailed way. You could refine the traffic that gets routed to the new version even more precisely - by browsers, location, or anything else that can be figured out from the incoming requests.\nAssuming you don\u0026rsquo;t find any issues with the new version of the service, you will eventually end up with 100% of requests routed to the v2 version of the service and 0% routed to the v1 version. Once that happens, you can remove the v1 deployment without impacting anything (assuming no other services are using it).\nA crucial thing to note here is the ability to monitor and observe your services from all angles. If you don\u0026rsquo;t have any service monitoring in place, it\u0026rsquo;s going to be hard to tell with a high degree of confidence when your service is behaving well or if it\u0026rsquo;s broken. We will talk more about observability in a later blog.\nFor more on Istio traffic management check out the Hands-on walkthrough of traffic management in Istio Part 2 of this blog.\n"},{"url":"https://istio.tetratelabs.io/blog/ebay-istio-practice/","title":"An Istio-based Traffic Management Use Case of eBay","description":"eBay shares its success story on the implementation of Istio.","content":"As a centralized cloud platform, Kubernetes manages multiple heterogeneous applications, including online services, big data, and backend searches. The number of clusters reaches up to the hundreds. In large clusters, thousands of microservices and hundreds of thousands of pods are run in a single cluster. Needless to say, different types of applications have different traffic management needs. The question then arises: how do we address these different needs with a centralized model? In fact, this is the biggest challenge that eBay has been seeking to tackle for years.\nTaking cloud applications as an example, to meet the high availability requirements across data centers, the network topology for production-level applications can be summarized as follows:\n eBay adopts an active-active data center network topology. Therefore, all production applications must deploy across three data centers. To meet the high availability requirements of a single cluster, all applications must deploy with multiple copies, and set load balancer configurations. Microservices are implemented across the entire site. To ensure there is high availability, service-to-service communications are primarily based on north-south traffic. For core applications, in addition to the local load balancing configuration of the cluster, it is also necessary to configure the cross-data center load balancer, and use weight control to transfer 99% of requests to the local data center, and 1% of traffic to cross-regional data centers. When all local service instances of an application fail, the operations and maintenance personnel can immediately restrict members of the cross-data center load balancer that point to 99% of the local traffic, and the traffic can be diverted to other data centers in seconds. In this way, business operations can run as usual. There are various causes for the failure of local service instances, including releases, hardware failures, firewalls, routers, and other network equipment changes.  Deployment Model eBay has multiple data centers. Each data center contains multiple availability zones with independent power supply and cooling systems. Each zone deploys multiple Kubernetes clusters. As the network latency of each zone is relatively small, the Istio control plane is built using the availability zone as the smallest management domain. In each availability zone, we select a Kubernetes cluster as the gateway cluster, deploy Istio Primary, and install Istio Remote in other clusters in the same availability zone. In this configuration, the service-to-service communications in multiple clusters in the same availability zone are all converted into east-west traffic, and cross-availability zone communications must pass through the Istio gateway.\nThis Istio deployment model is primarily based on the Kubernetes operating model. Different Kubernetes clusters are built with computing nodes in different availability zones. Such configuration makes possible the low latency of service access in the same mesh and the high availability of the scale of the service mesh, allowing for efficient management and control of the fault domain.\nAccess Gateway As an Internet company, the most important issue in traffic management is how to receive user requests from the public network. Transferring requests initiated by clients from outside to within the cluster is the primary concern to be addressed in traffic management.\nUnder a software-only architecture, the load balancing components consist of a Layer 4 load balancer and a Layer 7 API gateway. The Layer 4 load balancer is responsible for providing the virtual IP address of the service, receiving client requests, and forwarding client requests to the upstream instance. The Layer 7 API gateway is responsible for advanced routing functions based on application layer protocols, such as TLS offloading, access path redirections, and modification of HTTP protocol headers.\nLayer 4 Gateway The main technical approach of the Layer 4 gateway is to forward user requests from external clients to virtual IP addresses, and then to the cluster with the use of technologies such as network address translation (NAT) or tunnel. NAT is the most commonly used method, as it is easy to use and troubleshoot. Although IP tunnel is slightly more complicated to configure than NAT, it is often used to implement DSR. Not only does it retain the original IP address, but it also has a higher data forwarding efficiency, making it a better solution than NAT.\nUnder the Kubernetes architecture, the control panel of the Layer 4 load balancers can be defined as Kubernetes pods to implement advanced functions, such as failover, expansion, and contraction. Further, with the use of Linux\u0026rsquo;s own components like IPVS, Layer 4 data packet forwarding can be implemented.\nThe Layer 4 gateway uses the Kubernetes Service Controller to complete the following configurations:\n Virtual IP address assignment. After the user creates a service object, the Service Controller must select an available virtual IP address from the pre-configured public network IP to assign to the service. Configure the IPVS interface to complete the configuration of load balancing rules. In routing announcements, the virtual IP address is different from the IP of the physical device. It is not tied to any physical device. Rather, it is based on the BGP protocol and ECMP, which allows multiple devices to configure routing information with the same IP address. After the Service Controller assigns a virtual IP address to the service and configures the IPVS rules, the controller must also announce the virtual IP address with reference to the configured nodes. Based on the BGP protocol, other routers in the data center can learn how to forward the virtual IP address to the gateway node. With the support of ECMP, multiple Layer 4 load balancing components share the same behavior. In other words, they configure based on the same rules and announce the same IP address. Further, each instance carries the traffic of the virtual IP address. Compared with a hardware load balancer in active-standby mode, in this use case, all load balancing nodes are in the active mode, and there is no additional hardware overhead of standby devices.  The core function of each node is the configuration of load balancing rules, which encompass these features:\nN-tuple Hashing: Based on the source IP, source port, destination IP, destination port, and the N-tuple hash of the protocol, it is guaranteed that the same upstream instance is always selected for the same connection. The ECMP hash algorithm of the router is related to the device being used. The same N-tuple may be forwarded to multiple destinations. Here, a Layer 4 load balancer is used. As long as re-hashing is performed on the IPVS host according to the N-tuple, regardless of which IPVS instance the request is forwarded to, it will be forwarded to the same upstream server. The hash results calculated by all instances are consistent, so there is no need to synchronize the states between multiple IPVS instances. When a node fails, other nodes can forward the request to the same upstream.\nConsistent Hashing: The N-tuple-based hash algorithm will try to split the request evenly among multiple upstream servers. In Kubernetes, upstream servers exist in the form of pods, and scaling and failover commonly occur. In the ordinary hash algorithm, changing the target entails a lot of rehash. After adopting the consistent hash algorithm, only the modified part needs to be re-hashed, which reduces the CPU overhead needed for a large number of hash calculations.\nConnection Tracking: The Connection Tracking table is used to record the results of recently connected backend selections. When the IPVS module processes data for load balancing, it first queries the connection table. If it is found that the N-tuple already has a corresponding target instance and the instance is healthy, the result will be directly reused. However, if it does not exist, or the corresponding instance has an irregular status, the result will be recalculated based on the consistent hash, and the calculation result will be saved in the connection table for request data reuse at a later stage.\nData Packets: After selecting the corresponding upstream server, the IPVS module starts to process data packets. The kernel protocol stack processes data packets based on NAT or tunnel. The problem with NAT is that the user\u0026rsquo;s original IP will be lost. Here, an enhanced version of the tunnel mode is selected. The IPVS module will keep the original data packets. A layer of IP header will be encapsulated outside the original data packet. The source of the inner header of the data packet is the client IP and the destination is the virtual IP address of the service. The source of the outer header is IPVS PodIP, and the destination is the upstream server PodIP. Then, based on the IP-over-IP protocol, data will be sent to the upstream.\nHealth Check: Health check is a basic function in the load balancer. It is also required in our software load balancer. The Seasaw repository has APIs that support multiple health check modes. Simply call the interface in the controller to perform health checks on all upstream targets. If an upstream server fails in the health check, the relevant forwarding rules in IPVS must be deleted.\nThere are multiple ways to implement software-based Layer 4 load balancing. The IPVS module based on the operating system is the most direct and cost-effective solution. Needless to say, IPVS needs to rely on the operating system protocol stack during the processing of data, and the forwarding efficiency is not very high. If greater traffic processing capacity is required, there are many data plane acceleration technologies available, such as DPDK and XDP.\nLayer 7 Application Gateway Using the Layer 4 load balancer in conjunction with the ingress gateway powered by Istio, a highly available access solution for inbound traffic can be implemented throughout the site.\nAs the forwarding target of the Layer 4 load balancer, the Layer 7 API gateway needs to cooperate to complete the tunneling configurations. Since the Layer 4 load balancer is based on the IP tunnel configuration forwarding rules, when it forwards data, it uses the data packet sent by the IP-over-IP protocol as the target. The Envoy pod must disassemble the IPIP packet after receiving the request. This requires creating a device of type IPIP in the Enovy pod and binding a virtual IP address.\nThe main site of a website built with a microservice architecture usually consists of an aggregate of dozens to hundreds or even thousands of microservices. Different microservices are registered under the same main domain name with different access paths. At the same time, the API gateway will have some general access control policies. For example, the external IP cannot access the path ending with /admin. These access controls are well supported in Istio.\nAfter Envoy receives a request, it will forward the request to the relevant target according to the established Layer 7 forwarding rules. For the edge gateway, these targets are usually the virtual IP addresses of the services in the cloud. After Envoy receives the result of cloud processing, it has to forward the request back to the client, since the request is an IPIP packet when it arrives at the Envoy pod. After the operating system uninstalls the outer packet header, the inner data packet header is the client IP and service virtual IP address. When Envoy returns a packet, it only needs to rotate the source and destination addresses before sending data. The response packet can be sent directly to the client via the default gateway, bypassing IPVS, which is the well-known DSR mode.\nTraffic Management Protocol Upgrade For applications that access Istio, mTLS is enabled by default. For east-west traffic, the service-to-service communications are naturally encrypted, and the security level of intra-site traffic has been improved.\nThe Challenges of Scaling Up The power of Istio lies in the centralized management and control of east-west and north-south traffic through a set of models. For east-west traffic, there are not too many functions that need to be customized. The focus of our work in the production process is the construction of pipelines that support continuous integration and releases, as well as conducting a large number of performance tests. Based on the results of scale tests and performance tests, we define the Istio operation and maintenance model. For very large clusters, Istio faces many challenges.\nIstiod discovers all services in the cluster by default, and builds an Envoy cluster for the port of each service. If the sni-dnat mode is enabled, Istiod builds an Envoy cluster that conforms to the domain name specification. For super clusters with a large number of services, the Envoy configuration would become enormous. In the earlier version, we experienced a lot of failures that caused the access gateway to become completely unavailable when the configuration generated by Istiod exceeded the threshold that Envoy can withstand with more than 8000 services.\nIn subsequent versions of Istio, the ExportTo property is added to Istio objects to achieve visibility control. By exporting the accessible services only to the required microservices, it allows us to control the configuration scale of Envoy, reduce Envoy\u0026rsquo;s footprint, and improve push efficiency.\nAlthough the configurations of Envoy can be streamlined through ExportTo, the Istio control panel will proceed to discover all services in the cluster. For super clusters, Istiod still faces the problem of excessive resource usage and low processing efficiency due to the large number of objects that require monitoring and processing. The community version 1.9 improves Istiod\u0026rsquo;s ability to filter monitored objects based on namespace labels. This feature helps solve the scale and performance problems of the Istiod control panel under a super cluster scale.\nAutomating Access Gateway Certificates Based on Envoy\u0026rsquo;s discovery mechanism SDS, Istio can implement certificate management for website domain names. However, it requires administrators to put the certificate in the Istio root namespace in advance, and, when creating an Istio gateway object, to reference the predefined certificate information by defining credentialName. This applies to scenarios where the same Kubernetes cluster only provides one domain name. But where domain names are dynamically created, such a semi-automatic operation and maintenance mode is clearly not possible. A custom SDS must be implemented to connect with the enterprise certificate issuance center. The solution can be understood as an automatic issuance of the certificate through the custom controller to monitor the hosts attribute in the Istio gateway object, and the certificate is pushed to Envoy through SDS .\nAutomating Access Domain Names The same application gateway can implement the access of multiple applications with different domain names. For applications with different domain names, the domain name configurations must be automated. In order to do this, a NameService object is defined, allowing users to define FQDN, TTL, different DNS Providers, target services, and so on. After the target service is configured, the domain name configuration can be automatically completed.\nManagement Model Abstraction and Advanced Flow Control Istio\u0026rsquo;s model abstraction is highly flexible, and different network topologies can be defined for applications through Istio objects. However, it also faces many challenges:\n Faced with dozens of availability zones and hundreds of Kubernetes clusters, it is simply not practicable for users to configure each cluster. Doing so would result in management chaos. Customers would be forced to work with excessive infrastructure details. The computing resources are difficult to control, and the configurations lack uniformity. These factors increase the likelihood of system failure caused by any changes to be made. Istio objects have no state properties, and it is difficult to visually obtain information, such as whether the configuration is correct, and whether a push has been completed. The health check mechanism of the multipath software access gateway. Kubernetes implements multi-cluster management through cluster federation. We have implemented a set of Federated AccessPoint based on cluster federation. At its core, it defines the service object that describes load balancing in Kubernetes and the Gateway, VirtualService, DestinationRule, ServiceEntry, WorkloadEntry, and other objects that describe network traffic in Istio as templates, with AvailabilityZone or Kubernetes Cluster as the deployment target, and support cluster federation objects with the override property.  This traffic model comes with wide-ranging policy controls, including:\n PlacementPolicy control: Users can select the target cluster to complete the traffic configuration, and even select the associated FederatedDeployment object, so that AccessPoint can automatically discover the target cluster and complete the configuration. Status reports: Status reports include the gateway virtual IP address, gateway FQDN, certificate installation status and version information, and whether the routing strategy is configured. This remedies Istio\u0026rsquo;s shortcomings and makes the network configuration status of any application deployed on Istio easy to see. Release strategy control: A single-cluster gray release can be implemented for multi-cluster configurations. The release can be automatically suspended, after which the administrator would verify that the changes of a single cluster are correct, and then proceed with the release. Through this mechanism, failures caused by global traffic changes are effectively prevented. AccessPoints with different domain names may have different Layer 4 gateway virtual IP addresses to achieve Layer 4 network isolation based on IP addresses. Based on cross-regional traffic control, Istio implements workload locality-based failover strategy and weighting strategy management. Traffic management can be achieved across different regions while also maintaining a high level of availability.  Opportunities and Challenges Faced by Istio Istio has some clear advantages. It tries to uniformly manage north-south traffic and east-west traffic. This makes it possible to evolve the microservice architecture from an API gateway to a service mesh based on the same technology stack. These distinctive features make Istio stand out in the community as the most active open source service mesh project.\n Portability: It supports Kubernetes as well as virtual platforms OpenStack and Consul. A cross-language service mesh platform: It brings together Java, Scala, Nodejs, and various other languages. Centralized management of north-south and east-west traffic: Unifying the service mesh and API gateway user experience, and reduction of operating costs. Inherent security, automated certificate management, as well as seamless integration of authentication and authorization. Full functional support: All visible functions of the API gateway are supported. Strong community support: Google uses Istio as the next-generation microservice management platform, and cloud computing giants such as IBM, Microsoft, Huawei, and Alibaba are actively participating in the promotion and production of the Istio project.  Nonetheless, as an emerging project that manages highly complex traffic in distributed systems, Istio also faces many challenges:\n Scale and efficiency: The scale of clusters supported by Kubernetes is getting larger and larger. Production clusters with thousands of computing nodes, hundreds of thousands of pods, and thousands of services are becoming more and more common. Istio still faces many challenges in supporting large-scale cluster scenarios and the optimization of codes. Istio\u0026rsquo;s multi-cluster deployment will even double the magnitude. A major issue that Istio now faces is how to support super clusters. Complexity: Istio is a complex system in terms of both its control plane and its model abstraction. More functional modules means higher complexity in operation and maintenance. Integration with the existing services of the enterprise: Istio production must be integrated with the existing services of the enterprise, such as the CA, tracing system, and monitoring platform. Migration of existing businesses: Many companies already have microservice systems based on open source frameworks such as SpringCloud. This system has already supported many functions such as circuit breaker current limiting and API gateways, which are the same as those provided by Istio. The key questions are whether to migrate these existing businesses to Istio, and how to migrate.  Istio enjoys strong support from the community and has been endorsed by many industry giants and well-known projects. It can remedy Kubernetes' functional deficiencies at the traffic management level, making it an all-round microservice management platform. We are confident that Istio has a promising future.\nFor more Kubernetes- and Istio-based production practices, please see \u0026ldquo;The Road to Kubernetes Production Practice” (Chinese only). The book is currently on sale at JD.com for up to a 50% off.\nFanjie Meng is a senior architect at eBay, responsible for architecture and development of Kubernetes in enterprise implementation, focusing on networking, multi-clustering, service governance and service mesh, etc. A Kubernetes contributor, Fanjie participated in the development of community cluster federation and service controller refactoring.\n"},{"url":"https://istio.tetratelabs.io/blog/coohom-istio-practice/","title":"How Coohom Uses Istio to Integrate a New Serverless System into its Existing Self-developed Java System","description":"Istio practice in Coohom.","content":"Coohom successfully launched its home cloud design platform based on the core technology of distributed parallel computing and multimedia data mining. The platform is committed to the research and development of cloud rendering, cloud design, BIM, VR, AR, AI, and other technologies, in order to achieve the experience of “What You See Is What You Get”. It is an SAAS cloud software service platform that generates a design plan in 5 minutes, a rendering in 10 seconds, and a VR plan with a single click.\nCore Issue The fast-growing business needs of the company have led to the launch of serverless facilities supporting various languages in six domestic and overseas Kubernetes clusters. However, the company already has a mature Java service management system of its own that contains thousands of services deployed on the Kubernetes / KVM platform. Given that their service management systems are starkly different, the core issue is how to successfully enable service calls between them, such that the serverless platform can help make product research and development more efficient.\nStrategies The Istio-based Knative is the mature open-source serverless facility used by Coohom. In the Kubernetes cluster, Istio provides flexible and powerful dynamic routing/traffic management functions, coupled with some related gateway facilities. This has skillfully resolved the difficulties in enabling calls between services under different systems, while also maintaining the high flexibility and availability under Coohom’s current system.\nOutcomes With the help of Istio, serverless platform services are seamlessly integrated into the company’s self-developed service management system in the testing, pre-release, and production environments. Mature businesses using Java services do not need to undergo architectural changes. In respect of new businesses on the serverless platform, through languages NodeJS, Python, C++, and Golang, hundreds of services equipped with features such as rapid release and high flexibility can provide efficient production capacities to dozens of business processes.\nIntroducing new technologies on mature and complex service management platforms With many years of experience in back-end services, Coohom has a mature Dubbo-based Java microservice management framework that has been significantly modified according to the company\u0026rsquo;s business needs. The framework is compatible with the hybrid deployment of Kubernetes and KVM. However, as a medium-large innovative start-up company, Coohom has a large number of new business and new products in various departments, and the technical framework and language requirements of businesses (such as the NodeJS service that integrates product design and display requirements with the browser, and the Python and C++ languages that are required by cloud rendering and scientific research departments) constantly evolve. As a result, the language-based approach of embedding JAR packages for service management is no longer capable of meeting the growing business needs.\nSince 2017, Coohom has started to use self-developed or third-party hosted Kubernetes clusters as production-level container facilities, which encompass strong upgrade, maintenance, and problem-solving capabilities. On the Kubernetes platform, Istio uses a sidecar to inject services that are completely decoupled from languages, which is currently the best heterogeneous management plan that uses a multi-development language framework. Next, we will discuss how Istio VirtualService can be used as a bridge of communication between hundreds of serverless services and existing Java microservices.\nThe KFaas Solution – Using Istio VirtualService to Bridge Communications with Existing Microservices In Coohom, all traffic from the main website www.coohom.com must pass through the Java gateway service site-gateway under the existing service management system for JWT authentication, filtering, and forwarding to the services in the corresponding cluster according to the rules. Under the current management system, the only services that can accept traffic forwarding are Java services. Under this environment, serverless services are only deployed in the same Kubernetes, and HTTP requests matching the /faas/api/ rule are subscribed to using the Java services. Then, use SpringCloud zuul to forward the traffic directly to the gateway service in the Istio cluster. (Note that the configuration for cluster-local-gateway is fundamentally the same as that for istio-ingress-gateway. The only difference is that it only accepts traffic from the intranet and can be dynamically scaled according to load.) Lastly, deploy in the cluster used by the serverless service. In this way, the serverless service can be called by the existing management system.\nThe following is a business use case of a display platform. The service is deployed in a cluster and consists of two parts.\nPart 1: The Knative service deployed in the faas-alpha namespace of the Kubernetes cluster has the same access mode in the cluster as the ordinary Kubernetes service, namely http://saas-showroom.faas-alpha.svc.cluster.local\nPart 2: The routing rules of the Istio VirtualService deployed in the Kubernetes cluster kfaas-system namespace are:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: faas-alpha-ns-saas-showroom-ksvc-virtualservice namespace: kfaas-system spec: gateways: - knative-serving/cluster-local-gateway - knative-serving/knative-ingress-gateway hosts: - faas-alpha-ksvc-gateway.kfaas-system.svc.cluster.local http: - match: - uri: prefix: /faas/api/saas/showroom/ rewrite: authority: saas-showroom.faas-alpha.svc.cluster.local route: - destination: host: cluster-local-gateway.istio-system.svc.cluster.local weight: 100  The HTTP path prefix matches /faas/api/p/saas/virtual-showroom/ (At present, we use the methods of prefix and exact, and the regex matching function will be provided in the future to avoid routing conflicts.) The HTTP host must be faas-alpha-ksvc-gateway.kfaas-system.svc.cluster.local (The Kubernetes service is the external-service of cluster-local-gateway. In a non-production environment, use multiple external-services and multiple test environments to share a cluster-local-gateway service under istio-system.) Forward the traffic to the target service saas-showroom.faas-alpha.svc.cluster.local. Retry, delay, and other rules can be added to improve the robustness of request forwarding in the mesh. The percentage set by weight will be used in the future to enhance the traffic management capabilities of blue-green releases.  Calling Existing Java Microservices Using the Serverless Solution Another Java export gateway service is used here. Since the forwarding and subscription rules are already found in the existing service management system, they will not be detailed here. The serverless approach to access another microservice management system through the entry and exit gateways is known as the “east-west gateway”. Although the rules, usage and formats differ across various management systems, they can be integrated through gateway services compatible with both sides. Calling across systems is commonly seen in the industry, and is also used by some of Coohom’s new subsidiaries or departments to promote compatibility.\nThe KFaas Solution and Version Decoupling between Istio / Knative In 2018, before the release of Istio’s official version 1.0, we had already started conducting trial runs in the test environment. Since version 1.0, the releases evolved in the following sequence in the production environment: 1.0.6 -\u0026gt; 1.1.7 -\u0026gt; 1.4.6 -\u0026gt; 1.5.4 (partial clusters). As for Knative, its development is as follows: 0.8 -\u0026gt; 0.9 -\u0026gt; 0.14 -\u0026gt; 0.15. New versions of Istio / Knative continue to evolve. However, some of our initial practices, such as writing business-related configurations into the istio-system namespace, have posed enormous challenges during the updates of incompatible versions. With this in mind, we have specially designed a KFaas solution that not only relies on Istio / Knative facilities, but also decouples from its fixed version.\nThe kfaas-system namespace includes:\n East-west gateway service for the serverless entry and exit, which can be elastically scaled according to load VirtualService configured by serverless dynamic routing Ingress configuration of domain name certificate for specific business-related use or intranet use  Business Level of Istio VirtualService in Coohom’s Services Under the current business level of the testing environment, the number of VirtualServices has reached over 700 by the end of 2020.\nThe Future of Service Mesh with Coohom Coohom uses the Istio service mesh to integrate service clusters under different service management systems, facilitating the implementation of serverless frameworks. With this powerful set of productivity tools, dozens of business lines of the company, including the products and R\u0026amp;D teams, can now work more efficiently than ever before. Coohom’s success story presents an illustrative use case of the application of cloud-native technologies in an SaaS company.\nAs Coohom’s diversified business lines continue to grow in 2021, the demands for heterogeneous services with multilingual frameworks, fine-grained service management systems, and better business infrastructure are expected to rise. These cloud-native technologies powered by Kubernetes / Istio are bound to create lasting value for Coohom.\nAuthor: Luo Ning (罗宁), Senior Development Engineer, Coohom Advanced Technology Engineering Team\n"},{"url":"https://istio.tetratelabs.io/tags/announcements/","title":"announcements","description":"","content":""},{"url":"https://istio.tetratelabs.io/blog/whats-new-istio-1-9/","title":"What&#39;s new in Istio 1.9?","description":"On February 9, Istio announced the release of Istio 1.9. In this release, we can see the wider adoption of VMs into the service mesh, and even better VM support, cert issuance to VMs, and health checking for the workload entry. Istio’s latest releases, 1.7 and 1.8, made a lot of progress toward making VMs first-class workloads in the mesh, and cert issuance has been the final gap to close.","content":"On February 9, Istio announced the release of Istio 1.9. In this release, we can see the wider adoption of VMs into the service mesh, and even better VM support, cert issuance to VMs, and health checking for the workload entry. Istio’s latest releases, 1.7 and 1.8, made a lot of progress toward making VMs first-class workloads in the mesh, and cert issuance has been the final gap to close.\nOnboarding VMs in the Service Mesh Virtual machine integration is one of the core features of Istio, and in this release it is upgraded to beta, which means it is available for production and is no longer a toy.\nRunning Kubernetes workloads in an Istio mesh has been a given for quite some time and so has been running VM workloads for the past couple of Istio releases. This latest release of Istio makes it even easier to mix Kubernetes and VM workloads in a service mesh.\nCommon use cases include running applications on VMs in a datacenter or VMs in a cloud environment. These VMs either run legacy or third party applications/services. Some of these applications/services won’t be going away any time soon\u0026ndash; or in some cases, ever! Some of these VM workloads are part of an application modernization journey including the move to microservices or RESTful services deployed as distributed services, some of them running in containers. In the course of this application modernization journey, some of these VMs run monolithic workloads until they are broken down into microservices: running these applications in VMs provides a path to the target RESTful services or APIs and makes for a smoother transition.\nWith such an incremental approach you can start onboarding existing applications running in VMs to the service mesh. Then, as you build out your service mesh practices, you can gradually decompose those monolithic applications to services and more easily deploy them across multiple clusters, clouds, and hybrid environments. Istio can help with this using WorkloadEntry, WorkloadSelector and WorkloadGroup to manage VMs in the mesh to facilitate a more assured transition in your application modernization journey.\nAligning with the Kubernetes Service API With the Kubernetes Service API, infrastructure providers and platform operators can set up multiple Controllers for different purposes. Thus it decouples Gateway from Envoy, which facilitates the use of different reverse-proxy backends in Istio.\nIstio has been actively working with Kubernetes SIG-NETWORK group since version 1.6, using the Kubernetes Service API to replace the existing Gateway declaration and expose services in the mesh to the outside. Previously, you needed to create a VirtualService to bind to the Gateway in order to expose the service outside of the mesh. Now you can use GatewayClass, Gateway, and Route. GatewayClass defines a set of Gateways that share a common configuration and behavior. This is similar to IngressClass for Ingress and StorageClass for PersistentVolumes. Route is similar to the Route configuration in VirtualService. You can try this feature by referring to the Istio documentation, but note that this feature is still in the experimental stage.\nSummary Istio 1.9 makes the status of each feature clearer, which also helps to increase user confidence in using them. After the last few major changes, I believe that Istio\u0026rsquo;s API will become more stable in further development.\nExtending the service mesh to VMs has been an important part of Tetrate’s founding mission. Tetrate offers Istio support and a premium, Istio-based service mesh management platform built for multi-cluster, multitenancy, and multi-cloud.\nResources:  Istio 1.9 Release: https://istio.io/latest/news/releases/1.9.x/announcing-1.9/ Tetrate Service Bridge: www.tetrate.io/tetrate-service-bridge GetIstio, Tetrate’s tested, open source Istio distro: www.getistio.io Contact Tetrate for more information.  "},{"url":"https://istio.tetratelabs.io/tags/getistio/","title":"getistio","description":"","content":""},{"url":"https://istio.tetratelabs.io/blog/launching-getistio/","title":"Introducing GetIstio: the easiest way to get Istio","description":"GetIstio is an integration and lifecycle management CLI tool that ensures the use of supported and vetted versions of Istio.","content":"Istio is one of the most popular and fast growing open-source projects in the cloud-native world; while this growth speaks volumes about the value users get from Istio, its rapid release cadence can also be a challenge for users who may be managing several different versions of Istio clusters at the same time and manually configuring CA certificates for cloud platforms.\nOverview We launched a new open-source project called GetIstio today, offering users the easiest way to install and upgrade Istio. GetIstio provides a vetted, upstream distribution of Istio \u0026ndash; a hardened image of Istio with continued support that is simpler to install, manage, and upgrade. It will have integrations with cloud native and popular on-prem certificate managers (e.g., AWS ACM, Venafi, etc). This launch includes:\n GetIstio CLI, the easiest way to install, operate, and upgrade Istio. GetIstio provides a safe, vetted, upstream Istio distro, tested against AKS, EKS, and GKE. A free, online course on Istio Fundamentals, that is available now at Tetrate Academy. A new community to bring together Istio and Envoy users and technology partners.  GetIstio CLI GetIstio is an integration and lifecycle management CLI tool that ensures the use of supported and vetted versions of Istio. Enterprises require the ability to control Istio versioning, support multiple versions of Istio, easily move between the versions, integrate with cloud providers’ certification systems, and centralize config management and validation. The GetIstio CLI tool supports these enterprise-level requirements as it:\n enforces fetching certified versions of Istio and enables only compatible versions of Istio installation allows seamlessly switching between multiple istioctl versions Includes a FIPS-compliant flavor delivers Istio configuration validations platform based by integrating validation libraries from multiple sources uses a number of cloud provider certificate management systems to create Istio CA certs that are used for signing Service-Mesh managed workloads, and provides multiple additional integration points with cloud providers  Quick start The command below obtains a shell script that downloads and installs the GetIstio binary that corresponds to the OS distribution detected by the script (currently macOS and Linux are supported). Additionally, the most recent supported version of Istio is downloaded. Also, the script adds the GetIstio location to the PATH variable (re-login is required to get PATH populated)\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash The following video shows the basic use of the GetIstio command line tool.\n Get involved As part of GetIstio we are also launching a new community for developers, end users and technology partners of Istio, Envoy, and service mesh. Community is open to all. The GetIstio.io website also includes practical tutorials for using Istio.\nIf you want to take your learning to the next level, we have also prepared a free Learn Istio Fundamentals course as part of Tetrate Academy. It is a self-paced course that has 8 modules with theoretical lessons where we explain the theory and concepts, practical lessons which consist of labs, and quizzes so you can check your knowledge. Join our weekly meetings, file issues, or ask questions in Slack. No contribution is too small and your opinions and contributions matter!\nGetIstio Subscription Tetrate provides commercial support for GetIstio for direct access to Istio experts, priority bug fixes and 24/7 support. More details here.\nRelated links:\n func-e: func-e.io GitHub: github.com/tetratelabs/getistio Join the Istio Slack and search for the GetIstio channel to contact us Get certified on the \u0026ldquo;Fundamentals of Istio\u0026rdquo;: academy.tetrate.io GetIstio Subscription: tetrate.io/getistio  "},{"url":"https://istio.tetratelabs.io/blog/external-ca-integration-with-cert-manager-using-istio-csr/","title":"External CA integration with cert-manager using istio-csr","description":"This blog will show how to integrate cert-manager as an external CA to your Istio service mesh using istio-csr extension. ","content":"In this article we at Tetrate will show how to integrate cert-manager as an external CA to your Istio service mesh using istio-csr extension.\nBackground For key and certificate management, Istio is using its own Certificate Authority (CA) inside istiod control plane.\nHere, we would use the cert-manager provisioned Issuer as the external CA to sign the workload certificates using Istio CSR API with the CSR request directly going from the workloads to the external CA.\nSetting up the Environment We performed this demo using Kubernetes 1.18 and Istio 1.8:\n  Deploy Kubernetes cluster(\u0026gt;=1.16) with minikube.\nminikube start --memory=10000 --cpus=4 --kubernetes-version=1.18.6   Install helm3 for cert-manager CRD installation.\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | sh   Install GetMesh and fetch Istio.\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | sh getmesh fetch   Deploy cert-manager Install cert-manager in cert-manager namespace.\nkubectl create namespace cert-manager helm repo add jetstack https://charts.jetstack.io helm repo update helm install \\  cert-manager jetstack/cert-manager \\  --namespace cert-manager \\  --version v1.1.0 \\  --set installCRDs=true Configure cert-manager Issuer While we could use cert-manager to connect with several Issuers, in this example we would configure cert-manager to create a self-signed CA to issue workload certificates. The istio-ca secret would hold the CA key/cert pair.\napiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: istio-ca spec: isCA: true duration: 2160h # 90d secretName: istio-ca commonName: istio-ca subject: organizations: - cluster.local - cert-manager issuerRef: name: selfsigned kind: Issuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: istio-ca spec: ca: secretName: istio-ca kubectl create ns istio-system kubectl apply -f https://raw.githubusercontent.com/cert-manager/istio-csr/master/hack/demo/cert-manager-bootstrap-resources.yaml -n istio-system Deploy istio-csr agent for cert-manager Deploy istio-csr through official helm chart. It will use a certificate named istio-ca to generate istio-ca secret for workload certificate and verification.\nhelm repo add https://chart.jetstack.io helm repo update helm install -n cert-manager cert-manager-istio-csr jetstack/cert-manager-istio-csr This agent also creates a secret istod-tls which holds the tls cert/key for istiod to serve XDS. This newly created cert is signed by istio-ca.\nInstall and Configure Istio Initialize Istio operator.\ngetmesh istioctl operator init Configure on the operator yaml.\n Configure External CA address for workloads Disable istiod as the CA Server provide TLS certs for istiod from cert-manager  apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: istio-operator-csr spec: profile: \u0026#34;demo\u0026#34; hub: containers.istio.tetratelabs.com values: global: # Change certificate provider to cert-manager istio agent for istio agent caAddress: cert-manager-istio-csr.cert-manager.svc:443 components: pilot: k8s: env: # Disable istiod CA Sever functionality - name: ENABLE_CA_SERVER value: \u0026#34;false\u0026#34; overlays: - apiVersion: apps/v1 kind: Deployment name: istiod patches: # Mount istiod serving and webhook certificate from Secret mount - path: spec.template.spec.containers.[name:discovery].args[7] value: \u0026#34;--tlsCertFile=/etc/cert-manager/tls/tls.crt\u0026#34; - path: spec.template.spec.containers.[name:discovery].args[8] value: \u0026#34;--tlsKeyFile=/etc/cert-manager/tls/tls.key\u0026#34; - path: spec.template.spec.containers.[name:discovery].args[9] value: \u0026#34;--caCertFile=/etc/cert-manager/ca/root-cert.pem\u0026#34; - path: spec.template.spec.containers.[name:discovery].volumeMounts[6] value: name: cert-manager mountPath: \u0026#34;/etc/cert-manager/tls\u0026#34; readOnly: true - path: spec.template.spec.containers.[name:discovery].volumeMounts[7] value: name: ca-root-cert mountPath: \u0026#34;/etc/cert-manager/ca\u0026#34; readOnly: true - path: spec.template.spec.volumes[6] value: name: cert-manager secret: secretName: istiod-tls - path: spec.template.spec.volumes[7] value: name: ca-root-cert configMap: secretName: istiod-tls defaultMode: 420 name: istio-ca-root-cert You could validate on all cert-manager certs in istio-system namespace through the following command.\nkubectl get certificates -n istio-system NAME READY SECRET AGE istio-ca True istio-ca 16m istiod True istiod-tls 16m Deploy application Here we deploy a simple workload for certificate verification.\nkubectl create ns foo kubectl label ns foo istio-injection=enabled kubectl apply -f samples/sleep/sleep.yaml -n foo kubectl get pods -n foo NAME READY STATUS RESTARTS AGE sleep-8f795f47d-vv6hx 2/2 Running 0 42s Validate mTLS certs issued for workloads After the workload is running, we could check on the secret in SDS using the istioctl proxy-config command.\ngetmesh istioctl pc secret sleep-8f795f47d-vv6hx.foo -o json \u0026gt; proxy_secret Check on the proxy_secret. There should be a field named ROOTCA and in ROOTCA there should be a field trustedCA which is signed by cert-manager istio-ca.\nCompare the base64 value in trustedCA with the istio-ca secret ca.crt located in istio-system. Those two should be the same.\nkubectl get secrets -n istio-system istio-ca -o json \u0026gt; istio-ca Summary This article demonstrates how to integrate cert-manager as an external CA for Istio service mesh. The cert-manager/service mesh integration can be successfully implemented in different clouds with brand-new or existing deployments of cert-manager, when the service mesh is introduced during Day 2 implementations stage. In the end, you\u0026rsquo;re getting the best of two worlds: Istio integrating with an external CA of your choice!\n"},{"url":"https://istio.tetratelabs.io/tags/security/","title":"security","description":"","content":""},{"url":"https://istio.tetratelabs.io/tags/gke/","title":"GKE","description":"","content":""},{"url":"https://istio.tetratelabs.io/blog/how-to-set-up-istio-with-getistio-on-gke/","title":"How to set up Istio with GetMesh on GKE","description":"This article will start by taking you through a hands-on installation and use of GetMesh on GKE.","content":"GetMesh is a CLI for Istio distributions open sourced by Tetrate. It mainly solves the following problems of Istio.\n Istio lifecycle management Tested and safe Istio configurations Native integrations into the computing environment Learning curve and ongoing support  To learn more about GetMesh, please visit https://istio.tetratelabs.io\nThis article will start by taking you through a hands-on installation and use of GetMesh on GKE, including:\n Installing Istio 1.7 on GKE Adding a virtual machine to the mesh Deploying the Bookinfo example Integrating a virtual machine for testing Upgrading Istio to 1.8  From the above, you will learn about Istio\u0026rsquo;s deployment architecture, basic functionality, and operation.\nThis blog is only for Istio 1.7, for 1.8 the steps to integrate a virtual machine have changed, please refer to the Istio documentation.\nPrerequisite In order to complete the do-it-yourself process, you will need to prepare the following environment.\n A Google Cloud account with a sufficient balance in it A local installation of the gcloud tool  Setup Istio Let’s set up Istio 1.7.5 with GetMesh.\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash getmesh fetch 1.7.5 Setup the demo profile（including Ingress gateway, egress gateway and Istiod）:\ngetmesh istioctl install --set profile=demo --set values.global.meshExpansion.enabled=true Detected that your cluster does not support third party JWT authentication. Falling back to less secure first party JWT. See https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens for details. ✔ Istio core installed ✔ Istiod installed ✔ Ingress gateways installed ✔ Egress gateways installed ✔ Installation complete To see the pods under istio-system namespace:\nkubectl get pod -n=istio-system NAME READY STATUS RESTARTS AGE istio-egressgateway-695f5944d8-wdk6s 1/1 Running 0 67s istio-ingressgateway-5c697d4cd7-4cgq7 1/1 Running 0 67s istiod-59747cbfdd-sbffx 1/1 Running 0 106s Enable sidecar auto injection:\nkubectl label namespace bookinfo istio-injection=enabled DeployBookinfo sample:\nkubectl create ns bookinfo kubectl apply -n bookinfo -f samples/bookinfo/platform/kube/bookinfo.yaml service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created Verify the deployment complete.\nkubectl exec \u0026#34;$(kubectl get pod -n bookinfo -l app=ratings -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; -n bookinfo -c ratings -- curl -s productpage:9080/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; Normally you will see a display that looks like this:\n\u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; Enable the access from outside of the mesh:\nkubectl -n bookinfo apply -f samples/bookinfo/networking/bookinfo-gateway.yaml export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].nodePort}\u0026#39;) export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;https\u0026#34;)].nodePort}\u0026#39;) echo \u0026#34;$INGRESS_PORT\u0026#34; echo \u0026#34;$SECURE_INGRESS_PORT\u0026#34; export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) echo \u0026#34;$INGRESS_HOST\u0026#34; export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT echo \u0026#34;$GATEWAY_URL\u0026#34; Apply the default DestinationRule.\nkubectl apply -n bookinfo -f samples/bookinfo/networking/destination-rule-all.yaml Adding a virtual machine to Istio mesh We will install MySQL on the VM, and configure it as a backend for the ratings service. Eventually, the service topology diagram for the bookinfo example will look something like this.\nWe will create a virtual machine in Google cloud and add it to the Istio Mesh. Assuming the name of the VM instance is instance-1, first create a certificate for the VM.\nkubectl create secret generic cacerts -n istio-system \\  --from-file=samples/certs/ca-cert.pem \\  --from-file=samples/certs/ca-key.pem \\  --from-file=samples/certs/root-cert.pem \\  --from-file=samples/certs/cert-chain.pem Setup Istio mesh expansion.\ngetmesh istioctl install \\  -f manifests/examples/vm/values-istio-meshexpansion.yaml Execute the following command in the Cloud shell to generate the configuration file for the virtual machine expansion.\nVM_NAME=instance-1 VM_NAMESPACE=vm WORK_DIR=vm SERVICE_ACCOUNT=instance-1 cat \u0026lt;\u0026lt;EOF\u0026gt; \u0026#34;${WORK_DIR}\u0026#34;/vmintegration.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: meshExpansion: enabled: true EOF getmesh istioctl install -f \u0026#34;${WORK_DIR}\u0026#34;/vmintegration.yaml kubectl create namespace \u0026#34;${VM_NAMESPACE}\u0026#34; kubectl create serviceaccount \u0026#34;${SERVICE_ACCOUNT}\u0026#34; -n \u0026#34;${VM_NAMESPACE}\u0026#34; # 1 hour. Note the expiration time, this token is only used during authentication and is not needed later. tokenexpiretime=3600 echo \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;TokenRequest\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;authentication.k8s.io/v1\u0026#34;,\u0026#34;spec\u0026#34;:{\u0026#34;audiences\u0026#34;:[\u0026#34;istio-ca\u0026#34;],\u0026#34;expirationSeconds\u0026#34;:\u0026#39;$tokenexpiretime\u0026#39;}}\u0026#39; | kubectl create --raw /api/v1/namespaces/$VM_NAMESPACE/serviceaccounts/$SERVICE_ACCOUNT/token -f - | jq -j \u0026#39;.status.token\u0026#39; \u0026gt; \u0026#34;${WORK_DIR}\u0026#34;/istio-token kubectl -n \u0026#34;${VM_NAMESPACE}\u0026#34; get configmaps istio-ca-root-cert -o json | jq -j \u0026#39;.\u0026#34;data\u0026#34;.\u0026#34;root-cert.pem\u0026#34;\u0026#39; \u0026gt; \u0026#34;${WORK_DIR}\u0026#34;/root-cert ISTIO_SERVICE_CIDR=$(echo \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;tst\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;clusterIP\u0026#34;:\u0026#34;1.1.1.1\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:443}]}}\u0026#39; | kubectl apply -f - 2\u0026gt;\u0026amp;1 | sed \u0026#39;s/.*valid IPs is //\u0026#39;) touch \u0026#34;${WORK_DIR}\u0026#34;/cluster.env echo ISTIO_SERVICE_CIDR=$ISTIO_SERVICE_CIDR \u0026gt; \u0026#34;${WORK_DIR}\u0026#34;/cluster.env echo \u0026#34;ISTIO_INBOUND_PORTS=3306,8080\u0026#34; \u0026gt;\u0026gt; \u0026#34;${WORK_DIR}\u0026#34;/cluster.env touch \u0026#34;${WORK_DIR}\u0026#34;/hosts-addendum echo \u0026#34;${INGRESS_HOST}istiod.istio-system.svc\u0026#34; \u0026gt; \u0026#34;${WORK_DIR}\u0026#34;/hosts-addendum touch \u0026#34;${WORK_DIR}\u0026#34;/sidecar.env echo \u0026#34;PROV_CERT=/var/run/secrets/istio\u0026#34; \u0026gt;\u0026gt;\u0026#34;${WORK_DIR}\u0026#34;/sidecar.env echo \u0026#34;OUTPUT_CERTS=/var/run/secrets/istio\u0026#34; \u0026gt;\u0026gt; \u0026#34;${WORK_DIR}\u0026#34;/sidecar.env Import the virtual machine configuration files into the virtual machine instance instance-1.\nUsing SSH to login to the virtual machine instance-1 and copy the files generated for the Kubernetes cluster and Istio to the $HOME directory of the virtual machine.\nsudo apt -y update sudo apt -y upgrade sudo mkdir -p /var/run/secrets/istio sudo cp \u0026#34;${HOME}\u0026#34;/root-cert.pem /var/run/secrets/istio/root-cert.pem sudo mkdir -p /var/run/secrets/tokens sudo cp \u0026#34;${HOME}\u0026#34;/istio-token /var/run/secrets/tokens/istio-token # Setup Istio on the VM curl -LO https://storage.googleapis.com/istio-release/releases/1.7.1/deb/istio-sidecar.deb sudo dpkg -i istio-sidecar.deb sudo cp \u0026#34;${HOME}\u0026#34;/cluster.env /var/lib/istio/envoy/cluster.env sudo cp \u0026#34;${HOME}\u0026#34;/sidecar.env /var/lib/istio/envoy/sidecar.env sudo sh -c \u0026#39;cat $(eval echo ~$SUDO_USER)/hosts-addendum \u0026gt;\u0026gt; /etc/hosts\u0026#39; sudo cp \u0026#34;${HOME}\u0026#34;/root-cert.pem /var/run/secrets/istio/root-cert.pem sudo mkdir -p /etc/istio/proxy sudo chown -R istio-proxy /var/lib/istio /etc/certs /etc/istio/proxy /var/run/secrets Note: By default the virtual machine\u0026rsquo;s firewall will deny inbound requests to port 3306, we need to configure the firewall rules in the VPC to allow services in mesh to access port 3306 of the virtual machine.\nNow you can start Istio on the virtual machine.\nsudo systemctl start istio Virtual Machine integrating test Install MySQL in a virtual machine and use it as the backend of the service.\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y mariadb-server sudo mysql GRANT ALL PRIVILEGES ON *.* TO \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39; WITH GRANT OPTION; quit; sudo systemctl restart mysql curl -q https://raw.githubusercontent.com/istio/istio/release-1.7/samples/bookinfo/src/mysql/mysqldb-init.sql | mysql -u root -ppassword mysql -u root -ppassword test -e \u0026#34;select * from ratings;\u0026#34; mysql -u root -ppassword test -e \u0026#34;update ratings set rating=5 where reviewid=1;select * from ratings;\u0026#34; hostname -I # If the IP address obtained here is \u0026lt;virtual_machine_ip\u0026gt; Register the services in the virtual machine to mesh.\ngetmesh istioctl experimental add-to-mesh -n vm mysqldb \u0026lt;virtual_machine_ip\u0026gt; mysql:3306 You will see the following output:\n2020-09-17T11:34:37.740252Z warn Got \u0026#39;services \u0026#34;mysqldb\u0026#34; not found\u0026#39; looking up svc \u0026#39;mysqldb\u0026#39; in namespace \u0026#39;vm\u0026#39;, attempting to create it 2020-09-17T11:34:38.111244Z warn Got \u0026#39;endpoints \u0026#34;mysqldb\u0026#34; not found\u0026#39; looking up endpoints for \u0026#39;mysqldb\u0026#39; in namespace \u0026#39;vm\u0026#39;, attempting to create them Deploy the v2 version of the ratings service, using MySQL as the storage backend.\nkubectl apply -n bookinfo -f samples/bookinfo/platform/kube/bookinfo-ratings-v2-mysql-vm.yaml kubectl apply -n bookinfo -f samples/bookinfo/networking/virtual-service-ratings-mysql-vm.yaml Add configurations for DestinationRule, ServiceEntry, and WorkloadEntry for MySQL services deployed on the virtual machine.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: mtls-mysqldb-vm spec: host: mysqldb.vm.svc.cluster.local trafficPolicy: tls: mode: ISTIO_MUTUAL --- apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: mysqldb-vm spec: hosts: - mysqldb.vm.svc.cluster.local location: MESH_INTERNAL ports: - number: 3306 name: mysql protocol: mysql resolution: STATIC workloadSelector: labels: app: mysqldb-vm --- apiVersion: networking.istio.io/v1alpha3 kind: WorkloadEntry metadata: name: mysqldb-vm spec: address: \u0026lt;virtual_machine_ip\u0026gt; #change to the VM’s IP labels: app: mysqldb-vm instance-id: ubuntu-vm-mariadb Upgrade to Isito 1.8 On November 19, 2020, Istio 1.8 was released with support for canary and in-place upgrades. Here we will upgrade Istio using the in-place method.\ngetmesh fetch --version 1.8 getmesh istioctl upgrade Confirm to proceed [y/N]? Enter y to confirm the upgrade, and after confirming that the control plane upgrade is complete, let\u0026rsquo;s upgrade the data plane next.\nkubectl rollout restart deployment --namespace bookinfo Use the command getmesh istioctl proxy-status -n bookinfo to check the version of the proxy and you can see that it has been updated to 1.8.0.\nFrequently used command The following are the common commands used in the above operations.\nUse the gcloud command to log in to the Google cloud virtual machine\ngcloud compute ssh jimmy@instance-1 --zone=us-west2-a Cleanup bookinfo sample\nIn the root directory of the unpacked istio installation package, execute:\nsamples/bookinfo/platform/kube/cleanup.sh "},{"url":"https://istio.tetratelabs.io/tags/installation/","title":"installation","description":"","content":""},{"url":"https://istio.tetratelabs.io/blog/","title":"Blog","description":"Community blogs","content":""},{"url":"https://istio.tetratelabs.io/tags/case-study/","title":"case study","description":"","content":""},{"url":"https://istio.tetratelabs.io/blog/netease-qingzhou-istio-practice/","title":"How NetEase Qingzhou Enables Microservices Architecture Evolution with Istio","description":"The practice of using Istio in NetEase Qingzhou.","content":"Author Fei Pei, he is a senior architect at NetEase Qingzhou.\nIn a company with diverse internet businesses like NetEase, each business has its own unique choice of microservices tech stack and system architecture based on their business nature and team composition. While this might seem harmless at the initial stage of business development, as the businesses grow, the business scale, level of complexity and team composition will vary. These problems may stand in the way of the microservices architecture:\n High R\u0026amp;D costs due to uncoordinated R\u0026amp;D efforts by different businesses. Failure to make full use of the technological assets of the NetEase Group. High level of intrusion into businesses. Business personnel must understand, learn about, monitor, and maintain the microservices framework Long upgrade cycle. A minor framework upgrade may take more than a month. Language limitations. The microservices system adopted by most core services is built from Java with little support for other languages.  Strategies The service mesh is a key microservices technology under the cloud-native system. It can effectively solve problems arising out of the microservices architecture adopted by a wide variety of internet businesses at NetEase. The decision by NetEase to use Istio, the classic service mesh open-source framework, is the product of careful deliberations:\n Strong background in cloud-native solutions. Endorsed by tech giants. Envoy, a key data plane component of Istio, is a de facto standard component of the cloud-native data plane. As the most popular choice within the service mesh landscape, Istio has a vibrant community and an outstanding architecture. As an enterprise-friendly solution, Istio strives to optimize its services for improved user experience and enhanced support in service implementation.  Having decided on Istio as the service mesh platform of choice, the NetEase Shufan Qingzhou team built the Qingzhou Service Mesh platform. This addresses the various problems of the microservices architecture faced by the diverse internet businesses of the NetEase Group and consolidates the existing microservices management framework. The establishment of this common, distributed microservices platform enables more enterprises to implement the evolution and upgrade of their microservices architecture. Further, NetEase Qingzhou has released an upgrade of the API gateway system built upon the Istio tech stack. The Envoy and Istio-based Qingzhou API gateway has enhanced capabilities and better performance. It has since become the standard component of the NetEase API gateway. Despite the comprehensive service mesh solutions provided by Istio, enterprises still need to set clear goals when constructing their service mesh systems.\nArchitectural Design An overview of the architecture of the NetEase Qingzhou Service Mesh is shown as follows:\nKey Points on Architectural Design:  The overall architectural design is implemented by extending Envoy and customizing Istio. The requisite feasibility checks have been carried out. Envoy-based data planes support various ways of interception. In addition to the complete interception of TCP traffic natively powered by Istio, other traffic interception methods that facilitate business integration, such as IP pointing and a dynamic interception, have been added. Components in the Istio Pilot-based control plane are pluggable. Before Istio 1.5, the control plane used to have more components, leading to higher maintenance costs. In designating Pilot as a key component of the control plane and making other components pluggable, the risks of production and implementation, as well as the maintenance costs, are significantly reduced. It also allows developers and maintainers to focus on building a better system. Multiple extension methods allowing easy integration with existing business platforms. Numerous open platforms, including the Istio CRD, MCP and API plane, can fully preserve their native capabilities. Simply with the use of a Restful API, they significantly reduce the costs incurred by businesses to build a new platform or to integrate with an existing platform. Performance improvements: Improvements are made on both components and networks. For components, as an example, the patterns of Envoy filter are now responsible for the back-end capabilities of the centralized mixer found in earlier versions of Istio, allowing configuration trimming for call tracing. On the other hand, the networks are faster than ever to keep up with the pace of container networks. This is a major step towards reducing latency.  Development Plan The microservices architecture development plan is founded upon the overall architectural design and reference to the architecture and tech stack of services. Let’s take the NetEase e-commerce business as an example. Its business development plan is shown below:\nKey Points on Development:  Overall migration architecture is specifically designed for the existing technologies and architectural pattern of the business. Business integration: Adaptation at the infrastructure level to ensure imperceptible migration for the business. Cross-cloud accesses: Ensuring seamless mutual accesses before and after migration with the use of the mixed cloud solutions of edge gateways. High usability: Ensuring that the SLAs are met during the migration process with added capabilities such as fallback routing as well as routing a certain percentage of traffic.  Platform Building During their application of Istio, businesses need a visualized map of the mesh, and the ability to efficiently manage the mesh. Because of this, the NetEase Qingzhou microservices platform has been enhanced with the addition of service mesh monitoring capabilities and supporting cross-cluster centralized monitoring between service mesh and microservices framework (Spring Cloud, Dubbo, gRPC and Thrift). With this update, users can benefit from a smoother migration of their existing microservices architecture.\nKey Points on Building: Stepping up on product enhancements to address business challenges\n Observability: Providing visualization tools and quick resolution functions. Usability: Packaging on par with cloud-native concept products. Extensibility: Open API system architecture. Rapid migration with business platforms.  System Building The Istio-based service mesh architecture requires comprehensive system protection features.\nKey Points on System Building  Architectural and deployment: Service mesh component CICD and auto-deployments. Quality: Automated functions, performance, and stability testings; overall fault diagnosis; chaos testing. Debugging: Debugging for services and components; rapid fault location and recovery in the service mesh. Operations and maintenance: All-round monitoring and alarm functions. Hot upgrade: Support for hot upgrade of sidecars. Open-source technologies: The Qingzhou team continues to contribute to the service mesh community.  Use Case Extensions: Support for the API Gateway Key Points on Design:  Extensions of the service mesh tech stack, making full use of cloud-native technologies. Overall design validation and feasibility checks. Enhancement on the extensibility of the high-performance control plane Envoy. Multiple extension plug-in methods for the primary control plane Istio Pilot. API interface design mitigates the discrepancies between different platforms, fostering rapid integration across platforms.  Results  Various services of NetEase, including NetEase Yeation, NetEase Media, NetEase Youdao, and industry platforms, have been successfully launched. NetEase effectively manages thousands of services and tens of thousands of instances. The NetEase microservices architecture is built onto a uniform infrastructure. Different lines of business can now leave the hassles of microservices management behind as well as lower R\u0026amp;D and maintenance costs for microservices. Introduction of service management capabilities, such as multi-language management, hot upgrade, fault injection, routing, and fuse downgrade. As part of the long-term plan for cloud-native infrastructure, more use cases are now supported: API gateways, DB \u0026amp; middleware mesh, failure drill, and so on. API gateway has become the standard component of the NetEase API gateway, providing support for entire site traffic access for many core services of NetEase, such as NetEaseMedia, NetEase Yeation, and NetEase Lofter. The NetEase microservices architecture and technologies are leading in the industry.  "},{"url":"https://istio.tetratelabs.io/istio-ca-certs-integrations/gcp-cas-integration/","title":"GCP CA Integration","description":"GCP CA Integration","content":"Instead of using a self-signed root certificate, here we get an intermediary Istio certificate authority (CA) from GCP CAS (Certificate Authority Service) to sign the workload certificates.\nThis approach enables the same root of trust for the root CA\u0026rsquo;s workloads in GCP CAS. As Istio signs the workload certs, the latency for getting workload certs issued is far less than directly getting the certs signed by ACM Private CA itself.\nThe getmesh gen-ca command furnishes the options to connect to ACM Private CA and get the intermediary CA cert signed. It uses the certificate details thus obtained to create the cacerts Kubernetes secret for Istio to use to sign workload certs. Istio, at startup, checks for the presence of the secret cacerts to decide if it needs to use this cert for signing workload certificates.\nPrerequisites To follow this tutorial, you will need a Google Cloud Platform account and a Kubernetes cluster with Istio installed and the following:\n A CA set up in GCP CAS GCP credentials file and the environment variable GOOGLE_APPLICATION_CREDENTIALS pointed to the crednetials to get the CSR with CA set signed by the GCP CAS. Refer to the Getting started with authentication documentation on how to setup the GCP credentials.  You can follow the prerequisites for instructions on how to install and setup Istio.\n Click here, if you need to set up GCP CA Setting up CAS The first thing we need is to set up the CAS in Google Cloud Console. Log in to your Google Cloud account and follow the steps below to create a CAS instance.\n From the navigation menu, select Security → Certificate Authority Service. Click the Create CA button. Configure the CA type:  Select Root CA. Select 365 days for validity. Select the Enterprise tier. Select the CAS\u0026rsquo;s location from the Location list (e.g. us-east1). Click Next.   Configure the CA subject name (you can use your values here):  For Organization (O), enter Istio. For Organization unit (OU), enter engineering. For Country name (C), enter US. For Locality name, enter Sunnyvale. For CA Common name (CN), enter getmesh.example.io. For Resource ID, enter getmesh-example-io. Click Next.   Configure the CA key size and algorithm:  Select RSA PKCS1 2048 (SHA 256). Click Next.   Click the Create button to create the CAS.  The figure below shows the summary page. Note that your page might look different if you configured your own CA subject name.\nConfigure GCP credentials Ensure you have GCP credentials set up (e.g.GOOGLE_APPLICATION_CREDENTIALS environment variable has to point to the credentials) on a machine you\u0026rsquo;re accessing the Kubernetes cluster from. Alternatively, if you installed Tetrate Istio Distro on Google Cloud Shell, the credentials are already set up.\n Creating CAS configuration We will use a YAML configuration to configure CAS and CSR creation. Use the YAML below as a template, and enter the CAS information from the CAS summary page on GCP:\nproviderName: \u0026#39;gcp\u0026#39; providerConfig: gcp: # This will hold the full CA name for the certificate authority you created on GCP casCAName: \u0026#39;projects/tetrate-io-istio/locations/us-west1/certificateAuthorities/getmesh-example-com\u0026#39; certificateParameters: secretOptions: istioCANamespace: \u0026#39;istio-system\u0026#39; # namespace where `cacerts` secrets live overrideExistingCACertsSecret: true # overwrites the existing `cacerts` secret and replaces it with this new one caOptions: validityDays: 365 # validity days before the CA expires keyLength: 2048 # length (bits) of Key to be created certSigningRequestParams: # x509.CertificateRequest; most fields omitted subject: commonname: \u0026#39;getmesh.example.io\u0026#39; country: - \u0026#39;US\u0026#39; locality: - \u0026#39;Sunnyvale\u0026#39; organization: - \u0026#39;Istio\u0026#39; organizationunit: - \u0026#39;engineering\u0026#39; emailaddresses: - \u0026#39;youremail@example.io\u0026#39; Save the above file to gcp-cas-config.yaml and use gen-ca command to create the cacert:\ngetmesh gen-ca --config-file gcp-cas-config.yaml The command output should look similar to this:\nKubernetes Secret YAML created successfully in /home/user/.getmesh/secret/getmesh-740905469.yaml Kubernetes Secret created successfully with name: cacerts, in namespace: istio-system Before continuing, make sure to delete the istiod Pod in the istio-system namespace to force it to use the created cacerts.\nTry it out If you\u0026rsquo;ve labeled the default namespace for automatic sidecar injection (see Prerequisites), we can then deploy a sample Hello world application:\nkubectl create deploy helloworld --image=gcr.io/tetratelabs/hello-world:1.0.0 Wait for the Pod to start and then get the certificate chain and CA root certificate proxies use for mTLS. We will save them in the proxy_secret file:\ngetmesh istioctl pc secret [pod-name] -o json \u0026gt; proxy_secret The CA root certificate is base64 encoded in the trustedCA field. For example:\n... { \u0026#34;name\u0026#34;: \u0026#34;ROOTCA\u0026#34;, \u0026#34;versionInfo\u0026#34;: \u0026#34;2021-02-09 04:11:42.776876959 +0000 UTC m=+0.407526692\u0026#34;, \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-02-09T04:11:42.783Z\u0026#34;, \u0026#34;secret\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ROOTCA\u0026#34;, \u0026#34;validationContext\u0026#34;: { \u0026#34;trustedCa\u0026#34;: { \u0026#34;inlineBytes\u0026#34;: \u0026#34;\u0026lt;base64 encoded value\u0026gt;\u0026#34;  } } } Store the decoded value to the encodedCA.crt file and then use openssl to decrypt the certificate into a more readable form:\nopenssl x509 -text -noout -in encodedCA.crt The output will include the common name, organization and other values we set in the CAS:\nCertificate: Data: Version: 3 (0x2) Serial Number: 55:7f:b3:00:f8:b2:24:50:dc:51:7c:e5:85:5a:14:7a:65:28:26:38 Signature Algorithm: sha256WithRSAEncryption Issuer: C = US, L = Sunnyvale, O = Istio, OU = engineering, CN = getmesh.example.io  Validity Not Before: Feb 10 17:24:51 2021 GMT Not After : Feb 10 17:24:51 2022 GMT Subject: C = US, L = Sunnyvale, O = Istio, OU = engineering, CN = getmesh.example.io  Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) ... Similarly, if you go to CAS instance in Google Cloud Console, you\u0026rsquo;ll notice the issued certificate under Issued Certificates tab, as shown in the figure below. Notice the ca.istio.io domain.\n"},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/install-and-update/","title":"GetMesh installation and update","description":"How to install and update Tetrate Istio Distro","content":"GetMesh CLI can be obtained by issuing the following command:\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash This, by default, downloads the GetMesh CLI with the latest version of Tetrate Istio Distro and certified Istio. To check if the download was successful, run the version command:\ngetmesh version or\ngetmesh version --remote=false #only the client version details An output of the form below suggests that Tetrate Istio Distro was installed successfully.\ngetmesh version: 0.6.0 active istioctl: 1.8.2-tetrate-v0  \nTo see the list of commands available with Tetrate Istio Distro and its supported features, run the help command:\ngetmesh --help \n"},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/","title":"Install GetMesh and Istio","description":"Installing getmesh and Istio","content":"In this section, you\u0026rsquo;ll learn how to install getmesh on your local client, and install an Istio distribution of your choice on a Kubernetes cluster:\n getmesh: Install getmesh, the TID tool that performs the management and maintenance of Istio instances. A single getmesh instance can manage multiple Istio instances across multiple Kubernetes clusters. Istio: Use getmesh to discover which Istio variants can be deployed on your target cluster, select one and perform a safe, validated installation operation. EKS Addon: On Amazon EKS, you don\u0026rsquo;t need to use getmesh to install a Tetrate Istio distribution. You can install directly from the AWS console, using the Tetrate Istio EKS Addon. Validate the Install: Once you\u0026rsquo;ve installed an Istio instance, you can use getmesh to check the install and validate the Istio configuration  "},{"url":"https://istio.tetratelabs.io/mgmt-tasks/install-istio-updates/","title":"Install Istio Updates","description":"Detailed Istio update instructions.","content":"Versions of Istio that are delivered through Tetrate Istio Distro are supported longer than upstream versions and are actively patched for key bug fixes and security updates. To check if you’re running Istio versions that are up to date or if they’re missing any key security patches, run the check-upgrade command. This command connects to the cluster (defined by the Kubernetes config of the operator workstation) to verify existing Istio installations, compares those with the latest available Tetrate Istio Distro certified versions, and recommends suggested upgrades. To check for upgrades, run:\ngetmesh check-upgrade Output would be something like\n$ getmesh check-upgrade [Summary of your Istio mesh] active istioctl version: 1.13.2-tetratefips-v0 data plane version: 1.13.2-tetratefips-v0 (9 proxies) control plane version: 1.13.2 [GetMesh Check] [WARNING] the upstream istio distributions are not supported by check-upgrade command: 1.13.2. Please install distributions with tetrate flavor listed in `getmesh list` command - There is the available patch for the minor version 1.13-tetratefips. We recommend upgrading all 1.13-tetratefips versions -\u0026gt; 1.13.3-tetratefips-v0 Fetch the latest istio version\ngetmesh fetch --version 1.13.3 --flavor tetratefips Output would be something like\n$ getmesh fetch --version 1.13.3 --flavor tetratefips fallback to the flavor 0 version which is the latest one in 1.13.3-tetratefips 1.13.3-tetratefips-v0 already fetched: download skipped For more information about 1.13.3-tetratefips-v0, please refer to the release notes: - https://istio.io/latest/news/releases/1.13.x/announcing-1.13.3/ istioctl switched to 1.13.3-tetratefips-v0 now Upgrade istio control plane and gateway proxies\ngetmesh istioctl upgrade --set profile=demo --set hub=containers.istio.tetratelabs.com --set tag=1.13.3-tetratefips-v0-distroless Output would be something like\n$ getmesh istioctl upgrade --set profile=demo --set hub=containers.istio.tetratelabs.com --set tag=1.13.3-tetratefips-v0-distroless WARNING: Istio control planes installed: 1.13.2. WARNING: A newer installed version of Istio has been detected. Running this command will overwrite it. This will install the Istio 1.13.3 demo profile with [\u0026#34;Istio core\u0026#34; \u0026#34;Istiod\u0026#34; \u0026#34;Ingress gateways\u0026#34; \u0026#34;Egress gateways\u0026#34;] components into the cluster. Proceed? (y/N) y ✔ Istio core installed ✔ Istiod installed ✔ Egress gateways installed ✔ Ingress gateways installed ✔ Installation complete Manually update all the application sidecar proxies to new release\nkubectl rollout restart deployment \u0026lt;application deployment name\u0026gt; -n \u0026lt;namespace\u0026gt; Output would be something like\n$ kubectl rollout restart deployment productpage-v1 ratings-v1 reviews-v1 reviews-v2 reviews-v3 -n bookinfo deployment.apps/productpage-v1 restarted deployment.apps/ratings-v1 restarted deployment.apps/reviews-v1 restarted deployment.apps/reviews-v2 restarted deployment.apps/reviews-v3 restarted "},{"url":"https://istio.tetratelabs.io/mgmt-tasks/install-istio-and-manage-multiple-istioctl/","title":"Manage multiple istioctl","description":"How to install Istio and manage multiple istioctl","content":"Supporting Multiple Clusters The GetMesh tool connects to the kubernetes cluster pointed to by the default kubernetes config file. Switch between these using kubectl config use-context.\nIf the KUBECONFIG environment variable is set, that then takes precedence.\nSupporting Multiple istioctl versions We recommend always using GetMesh to invoke istioctl.\nGetMesh eases switching between multiple versions of istioctl, and does version compatibility and configuration checks to ensure that only certified Istio is deployed.\nRefer to Istio documentation for the latest istioctl commands and options.\nReal-life requirements very often dictate the use of a different version of istioctl (than the latest version) or leveraging multiple versions of istioctl due custom configuration. The steps to achieve that are explained below.\nList the currently downloaded versions of Istio through Tetrate Istio Distro using the show command:\ngetmesh show Example output would be\n1.7.6-distro-v0 1.8.1-distro-v0 1.8.2-distro-v0 (Active) If the required version of Istio is not yet downloaded, the operator can first query the list of trusted Istio versions through the list command:\ngetmesh list Example output would be:\nISTIO VERSION FLAVOR FLAVOR VERSION K8S VERSIONS *1.8.2 tetrate 0 1.16,1.17,1.18 1.8.1 tetrate 1 1.16,1.17,1.18 1.7.6 tetratefips 2 1.16,1.17,1.18 1.7.5 tetratefips 3 1.16,1.17,1.18 1.7.4 tetrate 0 1.16,1.17,1.18 Below is an example of obtaining version 1.8.1 of Istio by leveraging the fetch command:\ngetmesh fetch --version 1.8.1 --flavor tetrate --flavor-version 0 In the example above, Flavor tetrate maps to upstream Istio with the addition of possible patches and Flavor tetratefips is a FIPS-compliant version of the Flavor tetrate.\nUse the show command getmesh show to cross check if the Istio version is downloaded and the output will list all versions and mark the active one:\n$ getmesh show 1.7.4-distro-v0 1.7.6-distro-v0 1.8.1-distro-v0 (Active) 1.8.2-distro-v0 To switch to a different version of istioctl, run the switch command for example:\ngetmesh switch --version 1.8.1 --flavor tetrate --flavor-version=0 Output would be something similar to:\nistioctl switched to 1.8.1-tetrate-v0 now "},{"url":"https://istio.tetratelabs.io/getmesh-cli/install/post-install/","title":"Once you&#39;ve installed Istio","description":"Post-install checks with getmesh","content":"Once you have installed Istio, you can use getmesh to verify that the installation is operating correctly.\nConfirm the getmesh and Istio versions using getmesh version:\ngetmesh version Output: $ getmesh version getmesh version: 0.6.0 active istioctl: 1.8.2-tetrate-v0 client version: 1.8.2-tetrate-v0 control plane version: 1.8.2-tetrate-v0 data plane version: 1.8.2-tetrate-v0 (2 proxies) \nVerify that the Istio configuration is correctly applied, using getmesh config-validate:\n# for all namespaces getmesh config-validate "},{"url":"https://istio.tetratelabs.io/blog/istio-18-a-virtual-machine-integration-odyssey/","title":"Istio 1.8: A Virtual Machine Integration Odyssey","description":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. ","content":"In this article, I’ll give you an overview of Istio‘s history of virtual machine integration support. In particular, the introduction of the smart DNS proxy and WorkloadGroup in Istio 1.8, which makes virtual machines and containers equivalent at the resource abstraction level.\nI will show you a tumultuous odyssey of Istio’s virtual machine integration. Tetrate, the enterprise service mesh company that made pushing Istio to run everywhere part of its founding mission, has used VM features extensively in customer deployments and has been instrumental in pushing VMs to Istio upstream.\nPreface In my previous article, I talked about how Istio 1.7 supported virtual machines. But at that time, late October, virtual machines were still not seamlessly integrated into Istio — there was still a lot of manual work required. Now, Istio 1.8 has added WorkloadGroup and smart DNS proxy, which allows non-Kubernetes workloads like VMs to become first-class citizens in Istio — just like pods.\nWith or without a sidecar installed for virtual machines, until 1.7 you could not resolve the DNS name of a Kubernetes service unless a kube-external DNS was configured — which is the last piece of virtual machine integration in Istio. This shortcoming has finally been fixed in Istio 1.8.\nWhy Is Virtual Machine Support Important? In the process of migrating our applications to cloud native architectures and continuously containerizing them, we will go through three phases as shown in the figure below.\n Stage 1: All applications are deployed on virtual machines Stage 2: Applications are deployed on both virtual machines and containers, are migrating from virtual machines to containers, and are using Kubernetes to manage containers. Stage 3: All applications are deployed in containers first, using Kubernetes to manage containers and Istio to manage service-to-service communication.  The above diagram is artificially simplified: in reality, there might be multiple hybrid clouds, multiple regions, multiple clusters, etc. Plus, at stage 3 containers and virtual machines may remain in long-term coexistence, but the trend of containerization remains unchanged.\nIstio’s History of Virtual Machine Support Istio’s support for virtual machines is a long process, an odyssey of sorts.\n0.2: Istio Mesh Expansion As of version 0.2, Istio added virtual machines to the Mesh via Istio Mesh Expansion, provided that the following prerequisites were met.\n Virtual machines must have direct access to the application’s pods via IP address, which requires a flat network between the container and the VM via VPC or VPN; and virtual machines do not need access to the Cluster IP, but rather direct access to the service’s endpoints. Virtual machines must have access to Istio’s control plane services (Pilot, Mixer, CA, now being integrated as Istiod), which can expose the control plane endpoints to virtual machines by deploying load balancers in the Istio Mesh. (optional) the virtual machine has access to the DNS server inside the Mesh (deployed in Kubernetes).  The steps to integrate a virtual machine are as follows.\n Create an internal load balancer for the Istio control plane service and the DNS service for the Kubernetes cluster. Generate a configuration file for the Istio Service CIDR, Service Account token, security certificate, and IP of the Istio Control Plane Service (the IP exposed through the Internal Load Balancer) and send it to the virtual machine. Setup the Istio component, dnsmaq (for DNS discovery), in the virtual machine; so that the virtual machine can access the services in the mesh using FQDN, to ensure that the virtual machine can correctly resolve the Cluster IP of the services in the mesh. To run the service in a virtual machine, you need to configure the sidecar, add inbound ports to be intercepted, then restart Istio and also run istioctl to register the service.  The following figure shows the detailed flow from integrating a virtual machine to accessing services in the virtual machine in a mesh.\n The DNS is hijacked by dnsmasq deployed in the virtual machine, which allows it to correctly obtain the Cluster IP of the Istio service (Kubernetes’ built-in DNS). Access to Kubernetes’ built-in DNS service (which is exposed outside the cluster via the Internal Load Balancer and can be accessed directly). Return the Cluster IP resolved by productpage.bookinfo . svc. cluster . local, noting that the IP address is not directly accessible, but failure to be DNS resolved will result in a failed VM request for the service. The virtual machine’s call to services in a mesh is hijacked by the sidecar proxy. Since the proxy is connected to the Istio control plane, the endpoints of the service can be queried via xDS, so traffic will be forwarded to one of the endpoints. To access VM services in mesh, you need to manually add VM services to mesh using the istioctl register command, which essentially registers the VM services to the service and endpoint in Kubernetes. Services in the mesh can be accessed using the VM-registered service name (FQDN, e.g. mysql.vm.svc.cluster.local).  The above Istio support for virtual machines continued with Istio 1.0, which introduced a new API ServiceEntry with Istio 1.1, that allows additional entries to be added to Istio’s internal service registry so that services in the mesh can access/route to these manually specified services. The istioctl register command is no longer needed and will be deprecated in Istio 1.9.\nThe istioctl experimental add-to-mesh command has been added to Istio 1.5 to add services from a virtual machine to a mesh, and it works just like the istioctl register.\n1.6 to 1.7: New Resource Abstractions Istio introduced a new resource type, WorkloadEntry, in traffic management from version 1.6, to abstract virtual machines so that they can be added to the mesh as equivalent loads to the pods in Kubernetes; with traffic management, security management, observability, etc. The mesh configuration process for virtual machines is simplified with WorkloadEntry, which selects multiple workload entries and Kubernetes pods based on the label selector specified in the service entry.\nIstio 1.8 adds a resource object for WorkloadGroup that provides a specification that can include both virtual machines and Kubernetes workloads, designed to mimic the existing sidecar injection and deployment specification model for Kubernetes workloads to bootstrap Istio agents on the VMs.\nBelow is a comparison of resource abstraction levels for virtual machines versus workloads in Kubernetes.\nFrom the above diagram, we can see that for virtual machine workloads there is a one-to-one correspondence with the workloads in Kubernetes.\nEverything seems perfect at this point. However, exposing the DNS server in the Kubernetes cluster directly is a big security risk, so we usually manually write the domain name and Cluster IP pair of the service the virtual machine needs to access to the local /etc/hosts — but this is not practical for a distributed cluster with a large number of nodes.\nThe process of accessing the services inside mesh by configuring the local /etc/hosts of the virtual machine is shown in the following figure.\n Registration of services in the virtual machine into the mesh. Manually write the domain name and Cluster IP pairs of the service to be accessed to the local /etc/hosts file in the virtual machine. Cluster IP where the virtual machine gets access to the service. The traffic is intercepted by the sidecar proxy and the endpoint address of the service to be accessed is resolved by Envoy. Access to designated endpoints of the service.  In Kubernetes, we generally use the Service object for service registration and discovery; each service has a separate DNS name that allows applications to call each other by using the service name. We can use ServiceEntry to register a service in a virtual machine into Istio’s service registry, but a virtual machine cannot access a DNS server in a Kubernetes cluster to get the Cluster IP if the DNS server is not exposed externally to the mesh, which causes the virtual machine to fail to access the services in the mesh. Wouldn’t the problem be solved if we could add a| Tables | Are | Cool |\n   Items Kubernetes Virtual Machine     Basic schedule unit Pod WorkloadEntry   Component Deployment WorkloadGroup   Service register and discovery Service ServiceEntry    sidecar to the virtual machine that would transparently intercept DNS requests and get the Cluster IP of all services in the mesh, similar to the role of dnsmasq in Figure 1?\nAs of Istio 1.8 — Smart DNS Proxy With the introduction of smart DNS proxy in Istio 1.8, virtual machines can access services within the mesh without the need to configure /etc/hosts, as shown in the following figure.\nThe Istio agent on the sidecar will come with a cached DNS proxy dynamically programmed by Istiod. DNS queries from the application are transparently intercepted and served by the Istio proxy in the pod or VM, with the response to DNS query requests, enabling seamless access from the virtual machine to the service mesh.\nThe WorkloadGroup and smart DNS proxy introduced in Istio 1.8 provide powerful support for virtual machine workloads, making legacy applications deployed in virtual machines fully equivalent to pods in Kubernetes.\nSummary In this odyssey of Istio’s virtual machine support, we can see the gradual realization of unified management of virtual machines and pods — starting with exposing the DNS server in the mesh and setting up dnsmasq in the virtual machine, and ending with using smart DNS proxies and abstracting resources such as WorkloadEntry, WorkloadGroup and ServiceEntry. This article only focuses on the single cluster situation, which is not enough to be used in real production. We also need to deal with security, multicluster, multitenancy, etc.\n"},{"url":"https://istio.tetratelabs.io/tags/vm/","title":"VM","description":"","content":""},{"url":"https://istio.tetratelabs.io/mgmt-tasks/config-validation/","title":"Config Validation","description":"Istio config validation","content":"Istio configuration is defined by a set of multiple objects and object types and is susceptible to operator error or architecture oversight. The GetMesh config-validate command performs validations of the cluster\u0026rsquo;s current config and yaml manifests that are not applied yet.\nThe command invokes a series of validations using external sources such as upstream Istio validations, Kiali libraries, and Tetrate Istio Distro custom configuration checks. A combined validation output is then sent to the stdout. Custom configuration validation checks are actively being added all the time.\nconfig-validate command is not limited to Istio versions installed via GetMesh CLI and works well with all Istio distros, upstream and others.\nConfig validation can be performed against two targets:\n the current cluster config, which might include multiple Istio configuration constructs pending manifest yaml files that have not yet been applied to the cluster  The command below checks if applying manifest that are defined in my-app.yaml and another-app.yaml will trigger any validation errors. The command reports the findings based on three sources (Istio upstream, Kiali and native Tetrate Istio Distro) without applying any configuration changes. It prevents unnecessary downtime or the preventable issues to affect production workloads:\n# validating a local manifest against the current cluster getmesh config-validate my-app.yaml another-app.yaml For convenience the command can use all manifests from the specified directory instead of operator using individual filenames. The example below takes all manifests from my-manifest-dir and checks if applying those manifests triggers any validation alerts:\n# validating local manifests in a directory against the current cluster in a specific namespace getmesh config-validate -n bookinfo my-manifest-dir/ The validation of the currently implemented configuration is also possible - can be done clusterwise or per namespace leveraging the commands below:\n# for all namespaces getmesh config-validate # for a specific namespace getmesh config-validate -n bookinfo The output would look similar to:\n NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE bookinfo-gateway Gateway IST0101 Error Referenced selector not found: \"app=nonexisting\" bookinfo-gateway Gateway KIA0302 Warning No matching workload found for gateway selector in this namespace The error codes of the found issues are prefixed by 'IST' or 'KIA'. For the detailed explanation, please refer to - https://istio.io/latest/docs/reference/config/analysis/ for 'IST' error codes - https://kiali.io/documentation/latest/validations/ for 'KIA' error codes  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/","title":"Reference","description":"Detailed information on GetMesh CLI commands","content":"  "},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-013/","title":"","description":"Join us to learn about the Tetrate Istio Certified Administrator exam. Vijay Nadkarni will share his tips and tricks on how he prepared and passed the TCIA exam.","content":" Sep 2, 2021 at AM 11:00 PST  Check your timezone  Join us on August 19th for another Istio weekly episode where we\u0026rsquo;ll talk about how to customize and create new service-level metrics.\nJoin here "},{"url":"https://istio.tetratelabs.io/download/downloads/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/faq/questions/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/fips-request/fips/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/helm/charts/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/introduction/intro/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/istio-cheatsheet/cheatsheet/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/login/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/quickstart/start/","title":"","description":"","content":""},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-016/","title":"Access Control for Microservices","description":"Join us on November 18th for the next Istio weekly episode where we&#39;ll talk with Tetrate&#39;s founding engineer, Ignasi Barrera.","content":" Nov 18, 2021 at AM 11:00 PST  Check your timezone  Join us on November 18th for the next Istio weekly episode where we\u0026rsquo;ll talk with Tetrate\u0026rsquo;s founding engineer, Ignasi Barrera.\nIgnasi leads the development of a modern security solution for the hybrid and multi-cloud worlds, based on NGAC - Next Generation Access Control. In this episode, we’ll talk about access control for microservices and we’ll explain what NGAC is and why it matters to you.\nJoin here "},{"url":"https://istio.tetratelabs.io/community/building-and-testing/","title":"Building and Testing","description":"","content":"Before you proceed, please make sure that you have the following dependencies available in your machine:\n https://github.com/google/addlicense https://github.com/golangci/golangci-lint a Kubernetes cluster (e.g. https://kind.sigs.k8s.io/)  Run linter Here we use golangci-lint configured in .golangci.yml for static analysis, so please make sure that you have it installed.\nTo run linter, simply execute:\nmake lint Run unittests Running unittests does not require any k8s cluster, and it can be done by\nmake test Build binary make build Run e2e tests Running end-to-end tests requires you to have a valid k8s context. Please note that e2e will use your default kubeconfig and default context.\nIn order to run e2e tests, execute:\nmake e2e-test Build auto-generated docs make doc-gen Add license headers We require every source code to have the specified license header. Adding the header can be done by\nmake license "},{"url":"https://istio.tetratelabs.io/istio-ca-certs-integrations/cert-manager-integration/","title":"cert-manager CA Integration","description":"cert-manager CA Integration","content":"This task shows how to provision Control Plane and Workload Certificates with an external Certificate Authority using cert-manager. cert-manager is a x509 certificate operator for Kubernetes that supports a number of Issuers, representing Certificate Authorities that can sign certificates. The istio-csr project installs an agent that is responsible for verifying incoming certificate signing requests from Istio mesh workloads, and signs them through cert-manager via a configured Issuer.\nInstall cert-manager First, cert-manager must be installed on the cluster using your preferred method.\nhelm repo add jetstack https://charts.jetstack.io helm repo update kubectl create namespace cert-manager helm install -n cert-manager cert-manager jetstack/cert-manager --set installCRDs=true Verify the cert-manager deployments have successfully rolled-out.\nfor i in cert-manager cert-manager-cainjector cert-manager-webhook; \\ do kubectl rollout status deploy/$i -n cert-manager; done Configure cert-manager An Issuer must be created in the istio-system namespace to sign Istiod and mesh workload certificates. We will use a SelfSigned Issuer, though any supported Issuer can be used.\n Note that publicly trusted certificates are strongly discouraged from being used, including ACME certificates.\n kubectl create namespace istio-system kubectl apply -n istio-system -f - \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: istio-ca spec: isCA: true duration: 2160h # 90d secretName: istio-ca commonName: istio-ca subject: organizations: - cert-manager issuerRef: name: selfsigned kind: Issuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: istio-ca spec: ca: secretName: istio-ca EOF Verify the \u0026ldquo;istio-ca\u0026rdquo; and \u0026ldquo;selfsigned\u0026rdquo; issuers are present and report READY=True:\nkubectl get issuers -n istio-system Once the issuers have become ready, istio-csr can be installed into the cluster.\nInstall istio-csr helm install -n cert-manager cert-manager-istio-csr jetstack/cert-manager-istio-csr Verify the istio-csr deployment has successfully rolled-out.\nkubectl rollout status deploy/cert-manager-istio-csr -n cert-manager Certificates \u0026ldquo;istio-ca\u0026rdquo; and \u0026ldquo;istiod\u0026rdquo; should be present and report READY=True.\nkubectl get certificates -n istio-system Install Istio Istio must be configured to use cert-manager as the CA server for both workload and Istio control plane components. The following configuration uses the IstioOperator resource to install Istio with cert-manager integration:\ngetmesh istioctl install -y -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system spec: values: global: # Changes the certificate provider to Cert Manager istio-csr caAddress: cert-manager-istio-csr.cert-manager.svc:443 components: pilot: k8s: env: # Disables istiod CA Sever functionality - name: ENABLE_CA_SERVER value: \u0026#34;false\u0026#34; overlays: - apiVersion: apps/v1 kind: Deployment name: istiod patches: # Mounts istiod serving and webhook certificates from Secret - path: spec.template.spec.containers.[name:discovery].args[-1] value: \u0026#34;--tlsCertFile=/etc/cert-manager/tls/tls.crt\u0026#34; - path: spec.template.spec.containers.[name:discovery].args[-1] value: \u0026#34;--tlsKeyFile=/etc/cert-manager/tls/tls.key\u0026#34; - path: spec.template.spec.containers.[name:discovery].args[-1] value: \u0026#34;--caCertFile=/etc/cert-manager/ca/root-cert.pem\u0026#34; - path: spec.template.spec.containers.[name:discovery].volumeMounts[-1] value: name: cert-manager mountPath: \u0026#34;/etc/cert-manager/tls\u0026#34; readOnly: true - path: spec.template.spec.containers.[name:discovery].volumeMounts[-1] value: name: ca-root-cert mountPath: \u0026#34;/etc/cert-manager/ca\u0026#34; readOnly: true - path: spec.template.spec.volumes[-1] value: name: cert-manager secret: secretName: istiod-tls - path: spec.template.spec.volumes[-1] value: name: ca-root-cert configMap: defaultMode: 420 name: istio-ca-root-cert EOF The installation should complete, and the Istio control plane should become in a ready state.\nkubectl get pods -n istio-system Test mTLS All workload certificates will now be requested through cert-manager using the configured Issuer. Let\u0026rsquo;s run an example workload to test mTLS. First, create a namespace and configure it for automatic sidecar injection:\nkubectl create ns foo kubectl label ns/foo istio-injection=enabled Run the sample sleep and httpbin workloads.\nISTIO_VERSION=1.11 kubectl apply -n foo -f https://raw.githubusercontent.com/istio/istio/release-$ISTIO_VERSION/samples/sleep/sleep.yaml kubectl apply -n foo -f https://raw.githubusercontent.com/istio/istio/release-$ISTIO_VERSION/samples/httpbin/httpbin.yaml Verify the sleep and httpbin deployments have successfully rolled-out.\nfor i in sleep httpbin; do kubectl rollout status -n foo deploy/$i; done Verify the sidecar proxy was injected for each workload. Each workload pod should show 2/2 containers are READY.\nfor i in sleep httpbin; do kubectl get po -n foo -l app=$i; done By default, Istio configures destination workloads using PERMISSIVE mode. This mode means a service can accept both plain text and mTLS connections. To ensure mTLS is being used between sleep and httpbin workloads, set the mode to STRICT for namespace \u0026ldquo;foo\u0026rdquo;.\nkubectl apply -n foo -f - \u0026lt;\u0026lt;EOF apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: \u0026#34;default\u0026#34; spec: mtls: mode: STRICT EOF Test mTLS from the sleep pod to the httpbin pod.\nkubectl -n foo exec -it deploy/sleep -c sleep -- curl -o /dev/null -s -w \u0026#39;%{http_code}\\n\u0026#39; http://httpbin.foo:8000/headers You should receive an 200 response code.\nVisualize mTLS Optionally, you can run Kiali to visualize the mTLS connections.\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-$ISTIO_VERSION/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-$ISTIO_VERSION/samples/addons/kiali.yaml Verify the Kiali and Prometheus deployments successfully rolled-out.\nfor i in prometheus kiali; do kubectl rollout status -n istio-system deploy/$i; done Open the Kiali dashboard.\nistioctl dashboard kiali Navigate to Workloads \u0026gt; Namespace:foo \u0026gt; httpbin. You should see the connection graph with padlocks to indicate mTLS is being used. If the graph is empty, adjust the \u0026ldquo;traffic metrics per refresh\u0026rdquo; dropdown list in the upper right-hand corner or regenerate connections.\nCongratulations, you successfully configured Istio to use cert-manager as a CA for mesh mTLS authentication.\n"},{"url":"https://istio.tetratelabs.io/changelog/","title":"Changelog posts","description":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.","content":"February Updates Feb 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nChanged\r  Better support for using applying additional filters to posts_tax_query for categories for custom WordPress syncs\n  Reporting fine-tuning for speed improvements (up to 60% improvement in latency)\n  Replaced login / registration pre-app screens with a cleaner design\n   Removed\r Removed an issue with the sync autolinker only interlinking selectively. Removed up an issue with prematurely logging out users    March Updates Mar 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nAdded\r Some scheduled changelogs, tweets, and slack messages queued up this weekend and were not published on time. We fixed the issue and all delayed publications should be out. We now prioritize keywords over title and body so customers can more effectively influence search results Support form in the Assistant is now protected with reCaptcha to reduce spam reinitializeOnUrlChange added to the JavaScript API to improve support for pages with turbolinks   Fixed\r Fixed an issue with the sync autolinker only interlinking selectively. Fixed up an issue with prematurely logging out users    Changelog label Added\r Changed\r Depricated\r Removed\r Fixed\r Security\r Unreleased\r "},{"url":"https://istio.tetratelabs.io/community-events/","title":"Community Events","description":"","content":"There are multiple ways you can contribute and get involved in the community:\n Start contributing to the GetMesh project on Github. Join and participate at one of our events. Watch Tetrate Tech Talks videos. Join the #GetMesh channel on Tetrate Community Slack. Get Istio certified at Tetrate Academy.  "},{"url":"https://istio.tetratelabs.io/community/contributing/","title":"Contributing to GetMesh","description":"","content":"We welcome contributions from the community. Please read the following guidelines carefully to maximize the chances of your PR being merged.\nCode Reviews  Indicate the priority of each comment, following this feedback ladder. If none was indicated it will be treated as [dust]. A single approval is sufficient to merge, except when the change cuts across several components; then it should be approved by at least one owner of each component. If a reviewer asks for changes in a PR they should be addressed before the PR is merged, even if another reviewer has already approved the PR. During the review, address the comments and commit the changes without squashing the commits. This facilitates incremental reviews since the reviewer does not go through all the code again to find out what has changed since the last review.  DCO All authors to the project retain copyright to their work. However, to ensure that they are only submitting work that they have rights to, we are requiring everyone to acknowledge this by signing their work.\nThe sign-off is a simple line at the end of the explanation for the patch, which certifies that you wrote it or otherwise have the right to pass it on as an open-source patch. The rules are pretty simple: if you can certify the below (from developercertificate.org):\nDeveloper Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 660 York Street, Suite 102, San Francisco, CA 94110 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. then you just add a line to every git commit message:\nSigned-off-by: Joe Smith \u0026lt;joe@gmail.com\u0026gt;  using your real name (sorry, no pseudonyms or anonymous contributions.)\nYou can add the sign off when creating the git commit via git commit -s.\nOr, you can sign off the whole PR via git rebase --signoff main.\n"},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-014/","title":"Episode 11: Enterprise service mesh and why you need it","description":"Join us to learn to manage and configure service meshes in enterprise environments. Adam will talk about scenarios that call for an enterprise service mesh such as TSB.","content":" Sep 16, 2021 at AM 11:00 PST  Check your timezone  Join us on September 16th for another Istio weekly episode where we’ll talk to Adam Zwickey, director of global solutions engineering at Tetrate. Adam will talk about scenarios that call for an enterprise service mesh, such as TSB. He’ll show us how TSB can help us manage and configure service meshes in enterprise environments.\nJoin here "},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-015/","title":"Episode 12: How I started contributing to Istio and Envoy","description":"Join us on October 1st for the next Istio weekly episode where we will talk with Istio maintainer Aditya Prerepa.","content":" Oct 1, 2021 at PM 4:00 PST  Check your timezone  Join us on October 1st for the next Istio weekly episode where we\u0026rsquo;ll talk with Istio maintainer Aditya Prerepa.\nIn addition to being a member of the Istio networking working group, Aditya is also finishing high school this year. He’s worked on Istio for more than a year with a focus on Pilot and xDS as well as VM health checking, auto registration, and multitenancy.\nWith Hacktoberfest coming up, we’ll discuss what Aditya is currently working on, how he started contributing to Istio and Envoy, his upcoming EnvoyCon talk, and any tips he has for anyone trying to start contributing to Istio/Envoy and other open-source projects.\nJoin here "},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-018/","title":"Episode 15: Setting up Istio with external control plane","description":"Join us on December 9th for the next Istio weekly episode, where we&#39;ll show you how to install Istio with an external control plane.","content":" Dec 9, 2021 at AM 11:00 PST  Check your timezone  Join us on December 9th for the next Istio weekly episode, where we\u0026rsquo;ll show you how to install Istio with an external control plane.\nThe external control plane deployment model offers a clean separation between mesh operators and mesh administrators. Mesh operators install and manage Istio’s control plane on one cluster and then connect one or more remote clusters (data planes) to form a mesh. Mesh administrators can then configure the mesh as needed.\nJoin here "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh/","title":"GetMesh","description":"","content":"GetMesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.\nOptions  -h, --help help for getmesh -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh check-upgrade\t- Check if there are patches available in the current minor version getmesh config-validate\t- Validate the current Istio configurations in your cluster getmesh default-hub\t- Set or Show the default hub passed to \u0026ldquo;getmesh istioctl install\u0026rdquo; via \u0026ldquo;\u0026ndash;set hub=\u0026rdquo; e.g. docker.io/istio getmesh fetch\t- Fetch istioctl of the specified version, flavor and flavor-version available in \u0026ldquo;getmesh list\u0026rdquo; command getmesh gen-ca\t- Generate intermediate CA getmesh istioctl\t- Execute istioctl with given arguments getmesh list\t- List available Istio distributions built by Tetrate getmesh prune\t- Remove specific istioctl installed, or all, except the active one getmesh show\t- Show fetched Istio versions getmesh switch\t- Switch the active istioctl to a specified version getmesh version\t- Show the versions of getmesh cli, running Istiod, Envoy, and the active istioctl  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_check-upgrade/","title":"getmesh check-upgrade","description":"","content":"Check if there are patches available in the current minor version, e.g. 1.7-tetrate: 1.7.4-tetrate-v1 -\u0026gt; 1.7.5-tetrate-v1\ngetmesh check-upgrade [flags] Examples # example output $ getmesh check-upgrade ... - Your data plane running in multiple minor versions: 1.7-tetrate, 1.8-tetrate - Your control plane running in multiple minor versions: 1.6-tetrate, 1.8-tetrate - The minor version 1.6-tetrate is not supported by Tetrate.io. We recommend you use the trusted minor versions in \u0026quot;getmesh list\u0026quot; - There is the available patch for the minor version 1.7-tetrate. We recommend upgrading all 1.7-tetrate versions -\u0026gt; 1.7.4-tetrate-v1 - There is the available patch for the minor version 1.8-tetrate which includes **security upgrades**. We strongly recommend upgrading all 1.8-tetrate versions -\u0026gt; 1.8.1-tetrate-v1 In the above example, we call names in the form of x.y-${flavor} \u0026quot;minor version\u0026quot;, where x.y is Istio's upstream minor and ${flavor} is the flavor of the distribution. Please refer to 'getmesh fetch --help' or 'getmesh list --help' for more information. Options  -h, --help help for check-upgrade Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_config-validate/","title":"getmesh config-validate","description":"","content":"Validate the current Istio configurations in your cluster just like \u0026lsquo;istioctl analyze\u0026rsquo;. Inspect all namespaces by default. If the \u0026lt;file/directory\u0026gt; is specified, we analyze the effect of applying these yaml files against the current cluster.\ngetmesh config-validate \u0026lt;file/directory\u0026gt;... [flags] Examples # validating a local manifest against the current cluster $ getmesh config-validate my-app.yaml another-app.yaml # validating local manifests in a directory against the current cluster in a specific namespace $ getmesh config-validate -n bookinfo my-manifest-dir/ NAME RESOURCE TYPE ERROR CODE\tSEVERITY\tMESSAGE httpbin Service IST0108 Warning [my-manifest-dir/service.yaml:1] Unknown annotation: networking.istio.io/non-exist # for all namespaces $ getmesh config-validate NAMESPACE NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE default bookinfo-gateway Gateway IST0101 Error Referenced selector not found: \u0026quot;app=nonexisting\u0026quot; bookinfo default Peerauthentication KIA0505 Error Destination Rule disabling namespace-wide mTLS is missing bookinfo bookinfo-gateway Gateway KIA0302 Warning No matching workload found for gateway selector in this namespace # for a specific namespace $ getmesh config-validate -n bookinfo NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE bookinfo-gateway Gateway IST0101 Error Referenced selector not found: \u0026quot;app=nonexisting\u0026quot; bookinfo-gateway Gateway KIA0302 Warning No matching workload found for gateway selector in this namespace # for a specific namespace with Error as threshold for validation $ getmesh config-validate -n bookinfo --output-threshold Error NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE bookinfo-gateway Gateway IST0101 Error Referenced selector not found: \u0026quot;app=nonexisting\u0026quot; The following is the explanation of each column: [NAMESPACE] namespace of the resource [NAME] name of the resource [RESOURCE TYPE] resource type, i.e. kind, of the resource [ERROR CODE] The error code of the found issue which is prefixed by 'IST' or 'KIA'. Please refer to - https://istio.io/latest/docs/reference/config/analysis/ for 'IST' error codes - https://kiali.io/documentation/latest/validations/ for 'KIA' error codes [SEVERITY] the severity of the found issue [MESSAGE] the detailed message of the found issue Options  -n, --namespace string namespace for config validation --output-threshold string severity level of analysis at which to display messages. Valid values: [Error Warning Info] (default \u0026quot;Info\u0026quot;) -h, --help help for config-validate Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_default-hub/","title":"getmesh default-hub","description":"","content":"Set or Show the default hub (root for Istio docker image paths) passed to \u0026ldquo;getmesh istioctl install\u0026rdquo; via \u0026ldquo;\u0026ndash;set hub=\u0026rdquo; e.g. docker.io/istio\ngetmesh default-hub [flags] Examples # Set the default hub to docker.io/istio $ getmesh default-hub --set docker.io/istio # Show the configured default hub $ getmesh default-hub --show # Remove the configured default hub $ getmesh default-hub --remove Options  -h, --help help for default-hub --remove remove the configured default hub --set string pass the location of hub, e.g. --set gcr.io/istio-testing --show set to show the current default hub value Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_fetch/","title":"getmesh fetch","description":"","content":"Fetch istioctl of the specified version, flavor and flavor-version available in \u0026ldquo;getmesh list\u0026rdquo; command\ngetmesh fetch [flags] Examples # Fetch the latest \u0026quot;tetrate flavored\u0026quot; istioctl of version=1.8 $ getmesh fetch --version 1.8 # Fetch the latest istioctl in version=1.7 and flavor=tetratefips $ getmesh fetch --version 1.7 --flavor tetratefips # Fetch the latest istioctl of version=1.7, flavor=tetrate and flavor-version=0 $ getmesh fetch --version 1.7 --flavor tetrate --flavor-version 0 # Fetch the istioctl of version=1.7.4 flavor=tetrate flavor-version=0 $ getmesh fetch --version 1.7.4 --flavor tetrate --flavor-version 0 # Fetch the istioctl of version=1.7.4 flavor=tetrate flavor-version=0 using name $ getmesh fetch --name 1.7.4-tetrate-v0 # Fetch the latest istioctl of version=1.7.4 and flavor=tetratefips $ getmesh fetch --version 1.7.4 --flavor tetratefips # Fetch the latest \u0026quot;tetrate flavored\u0026quot; istioctl of version=1.7.4 $ getmesh fetch --version 1.7.4 # Fetch the istioctl of version=1.8.3 flavor=istio flavor-version=0 $ getmesh fetch --version 1.8.3 --flavor istio # Fetch the latest \u0026quot;tetrate flavored\u0026quot; istioctl $ getmesh fetch As you can see the above examples: - If --flavor-versions is not given, it defaults to the latest flavor version in the list If the value does not have patch version, \u0026quot;1.7\u0026quot; or \u0026quot;1.8\u0026quot; for example, then we fallback to the latest patch version in that minor version. - If --flavor is not given, it defaults to \u0026quot;tetrate\u0026quot; flavor. - If --versions is not given, it defaults to the latest version of \u0026quot;tetrate\u0026quot; flavor. For more information, please refer to \u0026quot;getmesh list --help\u0026quot; command. Options  --name string Name of distribution, e.g. 1.9.0-istio-v0 --version string Version of istioctl e.g. \u0026quot;--version 1.7.4\u0026quot;. When --name flag is set, this will not be used. --flavor string Flavor of istioctl, e.g. \u0026quot;--flavor tetrate\u0026quot; or --flavor tetratefips\u0026quot; or --flavor istio\u0026quot;. When --name flag is set, this will not be used. --flavor-version int Version of the flavor, e.g. \u0026quot;--version 1\u0026quot;. When --name flag is set, this will not be used. (default -1) -h, --help help for fetch Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_gen-ca/","title":"getmesh gen-ca","description":"","content":"Generates intermediate CA from different managed services such as AWS ACMPCA, GCP CAS\ngetmesh gen-ca [flags] Examples - AWS: cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; aws.yaml providerName: aws disableSecretCreation: false providerConfig: aws: signingCAArn: \u0026lt;your ACM PCA CA ARN\u0026gt; templateArn: arn:aws:acm-pca:::template/SubordinateCACertificate_PathLen0/V1 signingAlgorithm: SHA256WITHRSA certificateParameters: secretOptions: istioCANamespace: istio-system secretFilePath: ~/.getmesh/secret/ overrideExistingCACertsSecret: false caOptions: certSigningRequestParams: raw: [] rawtbscertificaterequest: [] rawsubjectpublickeyinfo: [] rawsubject: [] version: 0 signature: [] signaturealgorithm: 0 publickeyalgorithm: 0 publickey: null subject: country: - US organization: - Istio organizationalunit: [] locality: - Sunnyvale province: - California streetaddress: [] postalcode: [] serialnumber: \u0026quot;\u0026quot; commonname: Istio CA names: [] extranames: [] attributes: [] extensions: [] extraextensions: [] dnsnames: - ca.istio.io emailaddresses: [] ipaddresses: [] uris: [] validityDays: 3650 keyLength: 2048 EOF getmesh gen-ca --config-file aws.yaml - GCP: cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; gcp.yaml providerName: gcp disableSecretCreation: false providerConfig: gcp: casCAName: projects/{project-id}/locations/{location}/certificateAuthorities/{YourCA} maxIssuerPathLen: 0 certificateParameters: secretOptions: istioCANamespace: istio-system secretFilePath: ~/.getmesh/secret/ overrideExistingCACertsSecret: false caOptions: certSigningRequestParams: raw: [] rawtbscertificaterequest: [] rawsubjectpublickeyinfo: [] rawsubject: [] version: 0 signature: [] signaturealgorithm: 0 publickeyalgorithm: 0 publickey: null subject: country: - US organization: - Istio organizationalunit: [] locality: - Sunnyvale province: - California streetaddress: [] postalcode: [] serialnumber: \u0026quot;\u0026quot; commonname: Istio CA names: [] extranames: [] attributes: [] extensions: [] extraextensions: [] dnsnames: - ca.istio.io emailaddresses: [] ipaddresses: [] uris: [] validityDays: 3650 keyLength: 2048 EOF getmesh gen-ca --config-file gcp.yaml Options  --config-file string path to config file --disable-secret-creation file only, doesn't create secret -p, --provider string name of the provider to be used, i.e aws, gcp --signing-ca string signing CA ARN string --template-arn string Template ARN used to be used for issuing Cert using CSR --signing-algorithm string Signing Algorithm to be used for issuing Cert using CSR for AWS --cas-ca-name string CAS CA Name string --max-issuer-path-len int32 CAS CA Max Issuer Path Length --common-name string Common name for x509 Cert request --country stringArray Country names for x509 Cert request --province stringArray Province names for x509 Cert request --locality stringArray Locality names for x509 Cert request --organization stringArray Organization names for x509 Cert request --organizational-unit stringArray OrganizationalUnit names for x509 Cert request --email stringArray Emails for x509 Cert request --istio-ca-namespace cacerts Namespace refered for creating the cacerts secrets in --secret-file-path string secret-file-path flag creates the secret YAML file --override-existing-ca-cert-secret override-existing-ca-cert-secret overrides the existing secret and creates a new one --validity-days int valid dates for subordinate CA --key-length int length of generated key in bits for CA -h, --help help for gen-ca Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_help/","title":"getmesh help","description":"","content":"Help provides help for any command in the application. Simply type getmesh help [path to command] for full details.\ngetmesh help [command] [flags] Options  -h, --help help for help Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_istioctl/","title":"getmesh istioctl","description":"","content":"Execute istioctl with given arguments where the version of istioctl is set by \u0026ldquo;getsitio fetch or switch\u0026rdquo;\ngetmesh istioctl \u0026lt;args...\u0026gt; [flags] Examples # install Istio with the default profile getmesh istioctl install --set profile=default # check versions of Istio data plane, control plane, and istioctl getmesh istioctl version Options  -h, --help help for istioctl Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_list/","title":"getmesh list","description":"","content":"List available Istio distributions built by Tetrate\ngetmesh list [flags] Examples $ getmesh list ISTIO VERSION\tFLAVOR FLAVOR VERSION\tK8S VERSIONS *1.8.2 tetrate\t0 1.16,1.17,1.18 1.8.1 tetrate\t0 1.16,1.17,1.18 1.7.6 tetrate\t0 1.16,1.17,1.18 1.7.5 tetrate\t0 1.16,1.17,1.18 1.7.4 tetrate\t0 1.16,1.17,1.18 '*' indicates the currently active istioctl version. The following is the explanation of each column: [ISTIO VERSION] The upstream tagged version of Istio on which the distribution is built. [FLAVOR] The kind of the distribution. As of now, there are three flavors \u0026quot;tetrate\u0026quot;, \u0026quot;tetratefips\u0026quot; and \u0026quot;istio\u0026quot;. - \u0026quot;tetrate\u0026quot; flavor equals the upstream Istio except it is built by Tetrate. - \u0026quot;tetratefips\u0026quot; flavor is FIPS-compliant, and can be used for installing FIPS-compliant control plain and data plain built by Tetrate. - \u0026quot;istio\u0026quot; flavor is the upstream build. Flavor version for upstream build will always be '0' [FLAVOR VERSION] The flavor's version. A flavor version 0 maps to the distribution that is built on exactly the same source code of the corresponding upstream Istio version. [K8S VERSIONS] Supported k8s versions for the distribution Options  -h, --help help for list Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_prune/","title":"getmesh prune","description":"","content":"Remove specific istioctl installed, or all, except the active one\ngetmesh prune [flags] Examples # remove all the installed $ getmesh prune # remove the specific distribution $ getmesh prune --version 1.7.4 --flavor tetrate --flavor-version 0 Options  --version string Version of istioctl e.g. 1.7.4 --flavor string Flavor of istioctl, e.g. \u0026quot;tetrate\u0026quot; or \u0026quot;tetratefips\u0026quot; or \u0026quot;istio\u0026quot; --flavor-version int Version of the flavor, e.g. 1 (default -1) -h, --help help for prune Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_show/","title":"getmesh show","description":"","content":"Show fetched Istio version\ngetmesh show [flags] Examples getmesh show Options  -h, --help help for show Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_switch/","title":"getmesh switch","description":"","content":"Switch the active istioctl to a specified version\ngetmesh switch [flags] Examples # Switch the active istioctl version to version=1.7.7, flavor=tetrate and flavor-version=0 $ getmesh switch --version 1.7.7 --flavor tetrate --flavor-version=0, # Switch to version=1.8.3, flavor=istio and flavor-version=0 using name flag $ getmesh switch --name 1.8.3-istio-v0 # Switch from active version=1.8.3 to version 1.9.0 with the same flavor and flavor-version $ getmesh switch --version 1.9.0 # Switch from active \u0026quot;tetrate flavored\u0026quot; version to \u0026quot;istio flavored\u0026quot; version with the same version and flavor-version $ getmesh switch --flavor istio # Switch from active version=1.8.3, flavor=istio and flavor-version=0 to version 1.9.0, flavor=tetrate and flavor-version=0 $ getmesh switch --version 1.9.0 --flavor=tetrate # Switch from active version=1.8.3, flavor=istio and flavor-version=0 to version=1.8.3, flavor=tetrate, flavor-version=1 $ getmesh switch --flavor tetrate --flavor-version=1 # Switch from active version=1.8.3, flavor=istio and flavor-version=0 to the latest 1.9.x version, flavor=istio and flavor-version=0 $ getmesh switch --version 1.9 Options  --name string Name of distribution, e.g. 1.9.0-istio-v0 --version string Version of istioctl, e.g. 1.7.4. When --name flag is set, this will not be used. --flavor string Flavor of istioctl, e.g. \u0026quot;tetrate\u0026quot; or \u0026quot;tetratefips\u0026quot; or \u0026quot;istio\u0026quot;. When --name flag is set, this will not be used. --flavor-version int Version of the flavor, e.g. 1. When --name flag is set, this will not be used (default -1) -h, --help help for switch Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/getmesh-cli/reference/getmesh_version/","title":"getmesh version","description":"","content":"Show the versions of getmesh cli, running Istiod, Envoy, and the active istioctl\ngetmesh version [flags] Options  -h, --help help for version --remote Use --remote=false to suppress control plane and data plane check (default true) Options inherited from parent commands  -c, --kubeconfig string Kubernetes configuration file SEE ALSO  getmesh\t- getmesh is an integration and lifecycle management CLI tool that ensures the use of supported and trusted versions of Istio.  "},{"url":"https://istio.tetratelabs.io/contact/","title":"Got Any Questions","description":"this is meta description","content":""},{"url":"https://istio.tetratelabs.io/community/event/tid-episode-017/","title":"Istio&#39;s new Wasm-based extension mechanism and ecosystem","description":"Join us on November 24th for the next Istio weekly episode where we&#39;ll talk with Tetrate software engineer.","content":" Nov 24, 2021 at PM 4:00 PST  Check your timezone  Join us on November 24th for the next Istio weekly episode where we\u0026rsquo;ll talk with Tetrate software engineer and Envoy proxy and Proxy Wasm maintainer Takeshi Yoneda.\nIn this episode, Takeshi will give us an overview of WebAssembly (Wasm) in Istio and talk about exciting new Wasm-related features he worked on for the Istio 1.12 release.\nJoin here "},{"url":"https://istio.tetratelabs.io/search/","title":"Search Result","description":"this is meta description","content":""}]